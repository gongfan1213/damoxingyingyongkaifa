#### （3）QLoRA

大模型微调是一种有效手段，能够显著提升模型性能，增加所需功能或去除不必要特性。然而，微调参数庞大的模型（如LLaMA 65B模型）成本极高，常规16bit微调需使用超过780GB的GPU内存。尽管最新量化技术能减少大模型内存占用，但多数仅适用于推理阶段。为了解决这些问题，技术人员提出QLoRA（Quantized LoRA，量化低秩适应）方案，将模型量化至4bit，并在微调过程中保持性能不受影响。

图2-25展示全量微调（Full Fine-Tuning）、LoRA、QLoRA三种不同的微调方法及其内存需求。QLoRA通过将变换器模型量化到4bit精度，并使用分页优化器来处理内存峰值，从而改进了LoRA。

![image](https://github.com/user-attachments/assets/aa3affe2-f619-4e09-9fbe-0009c1e15a45)


**图2-25 全量微调、LoRA、QLoRA的对比**

- **全量微调（无适配器）**：优化器状态（32bit）、适配器（32bit）、基础模型（16bit Transformer）。

- **LoRA**：优化器状态（无）、适配器（32bit）、基础模型（16bit Transformer）。

- **QLoRA（页面优化流）**：优化器状态（CPU处理，页面优化流）、适配器（4bit Transformer）、基础模型（4bit Transformer），涉及参数传播流、页面优化流。



QLoRA采用创新的高精度技术，将预训练模型转换为4bit格式，并引入一组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLoRA运用了两种精度的数据类型：用于存储的4bit低精度数据类型和用于计算的BFloat16高精度数据类型。这意味着在使用QLoRA权重张量时，系统会先将其从4bit格式转换回BFloat16格式，然后执行16位的矩阵乘法运算。QLoRA还引入了两项技术以实现高质量的4bit量化微调：NF4（4bit NormalFloat，4位正常浮点数）量化和双量化。NF4是一种针对正态分布权重设计的新型数据格式，理论上最优，实际应用中表现优于传统4bit整数和4bit浮点数。双量化技术在初次量化基础上对权重进行二次量化，进一步减少模型参数所需的存储空间。

此外，QLoRA嵌入分页优化器，利用NVIDIA内存存储特性，在GPU内存溢出时自动在CPU和GPU间进行内存分页传输，避免出现内存不足错误。这一过程类似传统CPU内存与硬盘间内存使用的分页机制。具体来说，为优化器状态分配可分页内存空间，当GPU内存不足时，系统自动将数据转移到CPU内存中，并在需要优化器更新时再次加载到GPU内存中。使用16bit、8bit或4bit大小的适配器方法进行微调，QLoRA均能达到与16bit全量调优相同标准的性能。这意味着即便在模型量化过程中出现性能下降，通过低秩适配器微调，这部分损失的性能是可以被完全找回的。



### 2.4 对齐优化

以往的研究表明，以大模型为代表的AI系统可能会产生一些不良行为，如生成有害内容、提供不真实的回答、表现出谄媚、欺骗以及追求权力等。这些被称为AI系统的“未对齐”（Misalignment）行为，它们与人类的期望和意图相悖，是AI系统带来风险的关键因素。值得注意的是，这些未对齐问题并非仅在大模型被人为恶意操控的情况下才会发生。更令人忧虑的是，随着AI模型规模的扩大，这些问题可能会变得更加严重。AI对齐是一个重要的研究领域，专注于确保AI系统的行为和价值观与人类社会保持一致。



#### 2.4.1 反馈

反馈是指向AI系统提供的信息，旨在确保其行为与人类意图相一致。这种反馈有助于优化系统，使其与人类的价值观保持一致。反馈可以分为显式反馈和隐式反馈两种类型。

- **显式反馈**：指用户直接、明确地对模型输出进行评价或提供指导的信息，而隐式反馈则是指用户通过其行为间接地表达反馈信息，这种反馈不是直接表达的，而是通过用户的行为模式推断出来的，例如用户在浏览网页时停留的时间长短等。

- **在AI系统的开发阶段**：反馈可以指导系统架构和内部信息的改进。一旦系统部署，它可以根据反馈动态调整其行为。



在AI系统对齐的过程中，主要采用显式反馈包括奖励、示范和比较。

1. **奖励**：基于奖励的反馈提供了对AI系统进行定量评估的依据，有助于直接指导AI系统的行为调整。这种反馈通常来源于预先设计的规则函数或程序。然而，奖励反馈的内在复杂性有时会导致在处理某些任务时遇到困难。不完善或有缺陷的奖励函数可能会引导AI系统执行与设计者意图不符的行为，甚至产生负面影响或滥用奖励机制。此外，奖励反馈可能难以排除潜在的操纵，例如通过篡改奖励信号来引导系统执行不恰当的操作。因此，在设计奖励反馈系统时，必须仔细权衡和考虑，以最大限度地减少这些潜在问题，并确保智能系统能够适当地响应奖励信号，避免误导或滥用。

2. **示范**：通过记录专家在执行特定领域任务时的行为数据来实现的，可以采用多种形式，如通过视频或可穿戴设备进行示范。如果示范者和AI学习者的行为完全一致，示范数据可以直接形成状态 - 动作对的轨迹。这些状态 - 动作对有时可能只能被部分观察到。例如，为了通过录制视频展示人类专家在机器人操作任务中的表现，并为视频的每一帧标注相关的机器人状态和动作，可以创建一个包含人类示范的状态 - 动作对数据集。该数据集可以用来训练AI代理的策略，使其模仿专家的行为。示范反馈在逆强化学习（IRL）中被广泛应用，技术人员提出了将示范、偏好和其他类型的人类反馈结合起来，用于学习奖励函数。示范反馈提供了丰富的信息，不需要系统从生成的数据中学习，但获取这些数据通常需要更大的工作量和更多的专业知识。示范反馈直接利用了专家顾问的经验和专业知识，而无须构建复杂的知识表示模型。然而，示范反馈在处理超出人类顾问专业领域的任务时可能会遇到困难，且人类顾问的示范可能存在不精确和模糊的情况，这也可能对示范反馈的有效性带来挑战。此外，现实世界中的示范数据常常包含噪声和次优行为，这可能会对训练产生不利影响。

3. **比较**：通过比较AI系统的一组输出并进行排名，从而指导系统做出更明智的决策。比较反馈的优势在于能够处理难以精确评估的任务和目标，但其限制在于可能需要大量的比较数据。目前，基于比较反馈的偏好建模已经成为大模型微调领域的一种实用方法。



#### 2.4.2 偏好模型

在复杂任务（如对话系统）中，创建精准的奖励函数是一项较为困难的任务。同时基于示范反馈又可能需要大量的专家资源，以至于成本较高。因此，技术人员目前主要利用基于比较反馈的“偏好建模”的方法，通过比较不同输出来捕捉用户的偏好，进行大模型的微调。

偏好模型有两个主要方面：偏好粒度和偏好类别。

1. **偏好粒度**

其中，偏好粒度（Granularity of Preference）主要有三种类型：动作偏好、状态偏好和轨迹偏好。

- **动作偏好**：是指在给定特定状态下的不同行动的对比，以便明确在特定条件下的首选行动。状态偏好是不同状态之间的比较。轨迹偏好则对比整个状态 - 动作序列。在将状态偏好转化为轨迹偏好的过程中，通常需要考虑可达性和独立性的假设。这一过程较为依赖评估专家的专业知识。相比之下，轨迹偏好提供了更全面的战略信息，因为它本质上是对长期效用的评估，并且较少依赖于专家的主观判断。在顺序决策背景下，三种偏好粒度比较如表2-5所示。可以看出，{a1, S1, τ1}的偏好粒度优于{a2, S2, τ2}。


![image](https://github.com/user-attachments/assets/74ca576b-d30a-4bad-8f3d-0ece82bdc403)


**表2-5 在顺序决策背景下，三种偏好粒度的比较**

|偏好粒度|定义|
| ---- | ---- |
|动作偏好|在某一状态S下比较动作a1、a2，动作a1严格优于动作a2，表示为a1>a2|
|状态偏好|比较状态S1、S2，状态S1在偏好上优于状态S2，表示为S1>S2|
|轨迹偏好|轨迹τ = {S0,a0,S1,a1, …, St - 1,at - 1,St}，轨迹τ1严格优于轨迹τ2，表示为τ1>τ2|

2. **偏好类别**

偏好类别包括绝对偏好（Absolute Preference）和相对偏好（Relative Preference）。绝对偏好独立地表达了每个选项的偏好程度，包括二元式（Binary）和递进式（Gradual）。

- **二元式**：是用户偏好的简化模型，将物品通过喜欢和不喜欢直接分类，提供了判断用户偏好的简单模型。

- **递进式**：进一步区分为数值偏好和序数偏好。数值偏好使用绝对数值，使每个项目都获得一个数值分数，反映了偏好的程度。而序数偏好则涉及对一组固定项目进行分级评估，例如将项目划分为首选、次选或中间等级，而不包括具体的数值测量。



相对偏好是描述项目间偏好关系的一种方法，主要分为两种类型。
- **完全顺序（Total Order）**：这种顺序为所有项目对建立了一个全面的偏好关系。它确定了一个明确的偏好顺序，从最受欢迎到最不受欢迎。

- **部分顺序（Partial Order）**：与完全顺序不同，部分顺序允许在某些情况下不明确表达对两个项目的偏好。也就是说，某些项目可能被认为是不可比较的，从而为用户在决策时提供了更多的灵活性。



#### 2.4.3 RLHF

RLHF是一种使机器学习模型适应复杂和不明确目标的策略，尤其在AI对齐领域，它已成为确保大模型行为与人类价值观一致的关键技术。诸如OpenAI的GPT 4、Anthropic的Claude、Google的Bard，以及Meta的Llama 2-Chat等先进的AI系统，都依赖于RLHF来实现其功能。

RLHF的过程包括三个相互关联的组成部分：反馈收集、奖励建模和策略优化，如图2-26所示。

1. **反馈收集**：这一步骤通过分析人类的反馈来评估机器学习模型的输出，从而获取对模型表现的评价。

2. **奖励建模**：在这一阶段，使用监督学习技术训练一个奖励模型，该模型能够模仿人类的偏好，预测哪些行为更可能受到积极评价。

3. **策略优化**：通过优化AI系统的策略，使该系统生成的输出更有可能获得奖励模型的积极评价。



与传统的利用演示、手动设计奖励函数或其他指定奖励的方法相比，RLHF更能有效地识别并强化那些被人类认为是“好”的行为。这种方法使得AI系统能够更好地理解和适应人类的价值观和偏好。图2-26为从人类反馈中进行强化学习的流程。


![image](https://github.com/user-attachments/assets/623ac2f1-9856-4efa-8278-764943eaa20d)


**图2-26 从人类反馈中进行强化学习**

RLHF包括收集人类反馈、奖励建模、策略优化三个部分，有监督奖励学习反馈和强化学习的奖励贯穿其中。评估示例中，圆角矩形表示模型输出（例如文本），菱形表示评估。


![image](https://github.com/user-attachments/assets/0c100a76-8e30-403f-93c9-8b41bcecf3b1)


![image](https://github.com/user-attachments/assets/9ed921b3-5549-4858-9ea0-4c7d321cbb57)


![image](https://github.com/user-attachments/assets/d9cfc181-34b2-4837-8367-152c06025d9c)


基于RLHF的大模型对齐训练框架可分为4个主要步骤。

1. **预训练**：RLHF从一个已经训练好的初始基础模型πθ开始，该模型具有参数θ，可生成一系列的示例。在对大模型进行RLHF处理时，基础模型通常是一个在网络文本或其他数据集上预训练过的语言生成器。

2. **收集人类反馈**：利用基础模型生成示例并收集人类对这些示例的反馈。考虑一个人类H，假设其偏好与某个奖励函数rH一致。从πθ中抽取一个示例数据集，其每个示例xi由一个基础模型（或多个基础模型组合）生成。让反馈函数f将示例xi和随机噪声ϵi映射到反馈yi。因此，数据收集过程通常被建模为：xi ~ πθ, yi = f(H, xi, ϵi)。

在对大模型实施RLHF时，常见的做法是利用问答对(x, y)作为训练数据。在这种训练数据中，反馈通常体现为人类在每对对话中表达的偏好(yi)。这种方法有助于模型学习如何更好地理解和响应人类的交流方式。

3. **拟合奖励模型**：拟合一个奖励模型$\hat{r}_\phi$，使用反馈来尽可能近似来自H的评价。给定一个示例和偏好数据集$\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^n$，使用估计奖励的参数$\phi$最小化损失函数$\mathcal{L}(\mathcal{D}, \phi)$与基准模型的差异，并与基准模型进行对比训练。

$\mathcal{L}(\mathcal{D}, \phi)=\sum_{i = 1}^{n}\ell(\hat{r}_\phi(x_i), y_i)+\lambda_r(\phi)$

其中$\ell$是合适的损失函数，$\lambda_r$是某种正则化项。如果反馈是成对比较，则一般使用交叉熵损失或贝叶斯个性化排序算法作为损失函数。 

4. **策略优化**：使用奖励模型$\hat{r}_\phi$通过强化学习$\mathcal{E}_{x \sim \pi_{\theta_{new}}}$来微调基础模型。新参数$\theta_{new}$被引入训练，以获得最大化的奖励$\mathcal{R}$：

$\mathcal{R}(\theta_{new})=\mathcal{E}_{x \sim \pi_{\theta_{new}}}[\hat{r}_\phi(x)+\lambda_p(\theta, \theta_{new}, x)]$

其中$\lambda_p$是某种正则化项（例如两个分布之间基于差异的惩罚）。


![image](https://github.com/user-attachments/assets/0ad2083b-9cac-4d60-86df-5fd54127dae1)


![image](https://github.com/user-attachments/assets/ccb09c42-beda-4683-8fdc-c7eb0c96daef)



在RLHF的基础上，有技术人员提出了RLxF（x代表AI或人类和AI），主要的方法包括利用AI辅助提供反馈的强化学习（Reinforcement Learning from AI Feedback，RLAIF），以及利用人类和AI反馈的强化学习（RLHAIF）。

- **RLAIF**：训练流程会使用大模型生成的反馈取代人类反馈，根据预设标准，策略模型进行自我评估并根据测试提示进行修改，然后用修订后的回应对初始策略模型进行微调。最后用微调后的策略模型评估另一种大模型的反馈。实验结果表明使用RLAIF的效果和RLHF的效果大体相同。

- **RLHAIF**：集成了AI和人类反馈以增强监督能力，人类在评估和处理复杂问题方面具有显著优势。但也受限于时间和精力，大量研究表明，AI作为辅助工具，能够有效扩展在多样化领域的监督和处理能力。



RLxF方法的核心思想是将一个大问题分解成更小的子问题，从而能够利用更高效的工具（如AI和软件）快速解决子问题。通过利用这些子问题的解决方案，可以加速解决主要问题。更进一步，在RLxF的基础上，增加IDA（持续蒸馏和放大）的迭代步骤可为AI系统提供反馈。

IDA不是让人类专家（这里记作H）单独演示或评估目标行为，而是允许他们调用智能体（这里记作X）的多个副本来帮助评估和决策。我们将由H和多个X副本共同解决问题的复合系统称为Amplify^H(X)。然后，智能体X从Amplify^H(X)中学习，就像它之前只从H学习一样。

构造Amplify^H(X)的目的是增强人类专家的能力。由人类专家选择一系列的子问题组成问题Q，这些问题由Amplify^H(X)来回答。Amplify^H(X)将每个子问题交给智能体X，并由X给出子回答，人类专家根据这些子回答来决定如何回答Q。

在每个迭代步中，都训练X用于预测Amplify^H(X)的输出，这样X便是一个自回归模型。最开始X的行为是随机的，X实际上是在从一个专家那里学习。随着时间的推移，X变得更加强大，专家的任务转变为“协调”具有多个副本的X，这样能比单个副本更好地解决问题。

具体来说：H从一开始负责为训练X而选择子问题，并给出基于X回答的人类反馈。为了减轻人类专家H的负担，训练一个“人类预测器”H^p。它的作用是生成训练数据，即训练H^p来模仿H在计算Amplify^H(X)时的角色，并使用Amplify^{H^p}(X)来训练X，而非直接使用Amplify^H(X)。由于X的变化，H^p需要预测H对X提供的子答案的反应，因此需要在整个训练过程中不断更新H^p。

在训练中，重复采样问题Q ~ D，使用Amplify^H(X)来回答该问题，并记录H在此过程中做出的每个决定。H负责找到一个有助于回答Q的子问题Q1，然后利用X计算答案A1 = X(Q1)。重复这个过程k次，这样就得到了很多子问题的答案，最后由H综合这些子问题，从而计算出答案A。之后，存储传输记录τ = (Q, Q1, A1, …, Qk, Ak, A)。

这样的结果就是：在最初阶段X随机回答问题。当人类提出子问题时，经常收到不连贯或无用的子答案。接着，X会学到人类专家在没有X的帮助下回答问题的答案。进而，一旦X能提供简单答案，人类专家就可以通过将问题分解成很多简单的部分来提供更好的答案，这样X学会提供更好的答案。在整个过程中，X扩大它能回答的查询范围并逐步改善它提供的答案。并且在训练的每个阶段，Amplify^H(X)都比单独的X稍微聪明一些。


![image](https://github.com/user-attachments/assets/9d97f75c-0f18-41d5-880f-6c40bb6a32ff)


### 2.5 提示工程

提示工程（Prompt Engineering）是一种通过设计、实验和优化输入提示来引导大模型生成高质量、
