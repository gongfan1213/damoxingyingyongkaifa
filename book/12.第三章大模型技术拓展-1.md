## 第3章 大模型技术拓展
在实际应用中，除了需要组合使用提示工程、RAG、模型微调、模型预训练等核心技术之外，为了实现大模型落地，还需要结合使用一些拓展技术，如推理优化、训练评估、部署等。

### 3.1 推理优化技术
比起常规的深度学习模型，大模型往往推理速度较慢，这限制了它在实际生活中的使用。通过使用解码加速和推理加速等技术，可以提高大模型的推理速度。

#### 3.1.1 解码优化算法
大模型的解码算法是在给定一个输入序列后，从模型的输出概率分布中选择最佳的输出序列，从而完成输入序列的解码，即通常所说的解码过程。而常用的解码优化算法主要包括基于搜索的解码算法和基于采样的解码算法。

**1. 基于搜索的解码算法**

大模型的本质是自然语言生成模型，而在NLG解码任务中，常用的基于搜索的解码算法主要有贪心搜索（Greedy Search）、集束搜索（Beam Search）和对比搜索（Contrastive Search）。

- **贪心搜索**：在计算输出序列时的每一步只选择概率最高的词作为下一个词。这种逻辑简单的方法虽然可以快速得到一个输出序列，但也可能发生局部最优问题，导致输出的质量不高。
  - 贪心搜索伪代码：
```python
# 输入: 模型(model), 输入序列(input), 最大长度(max_length)
# 输出: 输出序列(output)
output = input  # 初始化输出序列为输入序列
while output的长度 < max_length:  # 循环直到达到最大长度
    prob = model(output)  # 计算模型的输出概率分布
    next_word = argmax(prob)  # 从概率分布中选择概率最高的词
    output = output + next_word  # 将选择的词添加到输出序列
    if next_word == EOS:  # 如果选择的词是结束符, 则跳出循环
        break
return output  # 返回输出序列
```
- **集束搜索**：将基于概率进行简单选择的方法做了一定的改进，它不再是简单地选择序列，而是在每一步计算出k个候选输出序列，称为集束（Beam）。下一步，它将上一步得到的每个候选序列扩展为k个可能的下一个词，然后从候选序列列表中重新选择k个最优的序列作为新的集束。因此，这种方法可以考虑更多的可能性，进而提高输出的质量，但也增加了计算的复杂度。
  - 集束搜索伪代码：
```python
# 输入: 模型(model), 输入序列(input), 输入序列最大长度(max_length), 选出的最优序列的数量(k)
# 输出: 输出序列(output)
beam = [input]  # 初始化集束为只包含输入序列的列表
while beam中序列的长度 < max_length:  # 循环, 直到达到序列的最大长度
    candidates = []  # 初始化候选序列的列表为空
    for sequence in beam:  # 循环获取集束中的每个序列并进行计算
        prob = model(sequence)  # 计算模型的输出概率分布
        top_k = top_k(prob, k)  # 从概率分布中选出概率最高的k个词
        for word in top_k:  # 循环获取top-k序列中的每个词
            new_sequence = sequence + word  # 在原序列加上获取的每个词, 并生成一个新的序列
            candidates.append(new_sequence)  # 将新的序列添加到候选序列的列表中
    beam = top_k(candidates, k)  # 从候选序列的列表中选出k个最优的序列作为新的集束
output = best(beam)  # 从最终的集束中选出最佳的输出序列
return output  # 返回输出序列
```
在集束搜索示例（k = 2）中，在解码的每一步，分别记录当前最优的k个候选序列：A，C；AB，AE；ABC，AED 。
- **对比搜索**：是一种基于对比学习的解码算法，它在每一步不仅要考虑当前的输出概率，还要考虑与其他候选序列的差异性。对比搜索的目标是生成一个高概率且多样性的输出序列。这种方法可以避免输出过于平凡或重复，但需要额外增加对比损失函数和对比样本。
  - 对比搜索伪代码：
```python
# 输入: 模型(model), 输入序列(input), 最大长度(max_length), 参数(k), 对比损失函数(contrastive_loss)
# 输出: 输出序列(output)
beam = [input]  # 初始化集束为只包含输入序列的列表
while beam中序列的长度 < max_length:  # 循环, 直到达到最大长度
    candidates = []  # 初始化候选序列的列表为空
    for sequence in beam:  # 循环获取集束中的每个序列并进行计算
        prob = model(sequence)  # 计算模型的输出概率分布
        top_k = top_k(prob, k)  # 从概率分布中选出概率最高的k个词
        for word in top_k:  # 循环获取top_k序列中的每个词
            new_sequence = sequence + word  # 在原序列加上获取的每个词, 并生成一个新的序列
            # 计算新的序列的得分, 考虑输出概率和对比损失
            score = log(prob[word]) - contrastive_loss(new_sequence, beam)
            candidates.append((new_sequence, score))  # 将新的序列和得分添加到候选序列的列表中
    beam = top_k(candidates, k)  # 从候选序列的列表中选出k个最优的序列作为新的集束
output = best(beam)  # 从最终的集束中选出最佳的输出序列
return output  # 返回输出序列
```

![image](https://github.com/user-attachments/assets/375d524b-a239-4a7c-a41a-4d5d650d1914)


这3种基于搜索的解码算法在执行NLG任务时具有不同的特点和应用场景。贪心搜索（计算逻辑简单）可以快速地得到一个输出序列，但可能输出的质量不高；集束搜索考虑更多的可能性，提高了输出的质量，但增加了计算的复杂度；对比搜索避免输出过于平凡或重复，但需要额外增加对比损失函数和对比样本。根据具体的任务和需求，可以选择合适的解码算法来完成NLG任务。

**2. 基于采样的解码算法**

以上提到的算法都是基于输出概率进行输出词的搜索，而目前主流的大模型解码还有基于采样的算法，常见的有随机采样（Random Sampling）、Top - k采样（Top - k Sampling）和Top - p采样（Top - p Sampling）。
- **随机采样**：是一种简单的解码算法，它根据模型的输出概率分布，随机选择一个词作为下一个词。这种方法可以增加输出的多样性，但也可能产生一些不合理或不连贯的结果。
  - 随机采样伪代码：
```python
# 输入: 模型(model), 输入序列(input), 最大长度(max_length)
# 输出: 输出序列(output)
output = input  # 初始化输出序列为输入序列
while output的长度 < max_length:  # 循环, 直到达到最大长度
    prob = model(output)  # 计算模型的输出概率分布
    next_word = sample(prob)  # 从概率分布中随机采样一个词
    output = output + next_word  # 将采样的词添加到输出序列
    if next_word == EOS:  # 如果采样的词是结束符, 则跳出循环
        break
return output  # 返回输出序列
```
- **Top - k采样**：是一种改进的解码算法，它在每一步只考虑概率最高的k个词，而忽略其他的词。这种方法可以避免一些低概率的词影响输出的质量，但也可能导致输出过于平凡或重复。
  - Top - k采样伪代码：
```python
# 输入: 模型(model), 输入序列(input), 输入序列最大长度(max_length), 选出概率最高的词的数量(k)
# 输出: 输出序列(output)
output = input  # 初始化输出序列为输入序列
while output的长度 < max_length:  # 循环, 直到达到序列的最大长度
    prob = model(output)  # 计算模型的输出概率分布
    top_k = top_k(prob, k)  # 从概率分布中选出概率最高的k个词
    next_word = sample(top_k)  # 从top k中随机采样一个词
    output = output + next_word  # 将采样的词添加到输出序列
    if next_word == EOS:  # 如果采样的词是结束符, 则跳出循环
        break
return output  # 返回输出序列
```
- **Top - p采样**：是一种更灵活的解码算法，它在每一步只考虑累积概率达到p的最小集合，而忽略其他的词。这种方法可以根据不同的情况动态地调整采样的范围，从而平衡输出的多样性和质量。
  - Top - p采样伪代码：
```python
# 输入: 模型(model), 输入序列(input), 最大长度(max_length), 累积概率值参数(p)
# 输出: 输出序列(output)
output = input  # 初始化输出序列为输入序列
while output的长度 < max_length:  # 循环, 直到达到序列的最大长度
    prob = model(output)  # 计算模型的输出概率分布
    top_p = top_p(prob, p)  # 从概率分布中选出累积概率达到p的最小集合
    next_word = sample(top_p)  # 从top_p中随机采样一个词
    output = output + next_word  # 将采样的词添加到输出序列
    if next_word == EOS:  # 如果采样的词是结束符, 则跳出循环
        break
return output  # 返回输出序列
```

这3种基于采样的解码算法在生成文本时具有不同的特点和应用场景。随机采样可以产生多样化的结果，但可能缺乏准确性；Top - k采样可以平衡准确性和多样性；Top - p采样可以根据概率分布动态调整生成结果的多样性。根据具体的任务和需求，可以选择合适的解码算法来生成高质量的文本。

除了基于概率的搜索和采样之外，也可以通过调整概率分布的方式来干预解码的结果。这类方法通过一个参数来动态调整输出词的概率分布，进而控制采样的词。这个参数被命名为Temperature（温度）。如果温度小，则概率分布差距大，容易采样到概率大的词；而温度大，则概率分布差距小，进而增加低概率的词被采到的机会，这种方法也被称为Temperature采样解码方法。现在在一些公司提供的大模型接口中看到的Temperature参数就是用来控制解码过程的。在实际使用中，一般输入的提示越长，则模型生成的质量越好，即模型输出词概率的置信度越高，这时可以适当调大Temperature值；如果输入的提示很短，指令不清晰，则模型生成的词语概率很有可能不那么可靠，这时需要适当减小Temperature值，降低模型输出的不稳定性。

此外，在大模型实际应用的解码中，往往还采用一种结合了Top - k采样、Top - p采样和Temperature调整的联合采样方法，这种方法综合多个解码算法的优点，可以平衡解码的时间和效果。

#### 3.1.2 推理加速策略
大模型的推理加速可以通过减小模型大小来降低推理时的资源需求，或通过硬件加速优化推理效率。常见的大模型推理加速策略包括KV缓存、FlashAttention、投机采样等。

**1. KV缓存**

KV缓存加速机制是一种针对Transformer网络，利用空间换时间的网络推理计算策略。KV缓存策略是占用一块缓存空间来缓存Transformer的键和值矩阵，在计算每个位置的查询值时保存先前位置的键和值，并在处理新位置时重用它们，从而避免重复计算。

具体来说，当处理新的位置时，首先计算新位置的查询，然后计算新位置的注意力分数。接着，使用注意力分数对保存在缓存中的键和值进行加权求和，而不是重新计算键和值。这样可以大大减少计算量，加速模型推理过程，在不影响计算精度的前提下提高推理性能。

KV缓存加速机制包括两个阶段：

- **预填充阶段**：为输入的提示序列生成键缓存和值缓存。
- **解码阶段**：更新KV缓存和计算Transformer layer的输出。

在实际应用中，为了保持缓存的大小不变，通常会采用一些策略，比如将最早处理的位置的键和值从缓存中移除，然后将新位置的键和值添加到缓存中。这种KV缓存优化机制特别适用于处理长序列的情况，因为长序列的自注意力机制的计算成本较高，通过使用KV缓存，可以显著提高模型的推理速度。

虽然通过KV缓存技术，可以极大地提高大模型的推理速度，但是现有的KV缓存仍存在一些问题，例如缓存的显存占用过高（大约占了模型参数显存的一半）以及管理KV缓存会占用大量的内存。由此，PagedAttention策略被提出来改善KV缓存的使用。

PagedAttention将每个序列的KV缓存分成多个块，每个块包含固定数量的已标记的键和值。在注意力计算过程中，PagedAttention内核高效地识别和获取这些块，并采用并行的方式加速计算。基于PagedAttention的开源大模型高速推理框架VLLM，实现了实时场景下大模型服务的吞吐与内存使用效率的极大提升，是目前最常用的大模型推理框架之一。

**2. FlashAttention**

不同于提高计算效率时会牺牲模型质量的大多数现有近似注意力方法，FlashAttention提出从I/O感知的角度优化GPU上注意力模块的速度和内存消耗，以此加速Transformer模型的注意力计算。通常来说，计算机存储处理设备主要有主存储器和GPU。其中，GPU中的存储单元主要包括HBM（高带宽内存）和SRAM（静态随机存取存储器）。HBM具有较大的容量，但I/O访问速度较慢；SRAM容量较小，但具有更快的I/O访问速度。具有带宽和内存大小的内存层次结构 。


![image](https://github.com/user-attachments/assets/c3b0a38a-7048-4fdd-be68-8feffffad5ff)


**图3-2 内存层次结构**

展示了GPU SRAM、GPU HBM、主存储器（CPU DRAM）的带宽和容量关系 ，GPU SRAM带宽19Tbit/s(20MB) ，GPU HBM带宽1.5Tbit/s(40GB) ，主存储器（CPU DRAM）带宽12.8Gbit/s(大于1TB) 。


由于Transformer模型的计算复杂度和空间复杂度都随序列长度呈平方级增长，因此处理长序列时速度会变得更慢且内存需求更大。因此，FlashAttention利用GPU中不同级别的内存（如HBM和SRAM）优化注意力计算过程，将输入组织成块，并引入必要的重计算，以更好地利用快速内存（SRAM）。

FlashAttention采取如下策略优化注意力计算过程。

- **FlashAttention技术**：使得在不读取整个矩阵的情况下进行Softmax计算成为可能。这种方法通过将注意力分数的计算与矩阵乘法操作相结合，避免了传统注意力机制必须计算所有注意力分数的Softmax，从而显著提升了计算效率。这种方法不仅减少了计算量，还减少了对内存的需求，使得模型能够处理更大的数据集，同时保持较快的运行速度。

Softmax被用来规范注意力权重，确保权重之和为1。这样可以确保每个输入元素的重要程度（权重）以概率形式表达出来。具体到注意力机制的计算步骤中，Softmax函数通常用于处理查询与键的点积计算之后的结果，这些点积结果称为注意力得分。Softmax函数将这些得分转换为概率分布，从而确定了不同输入元素的相对重要性。 
