2）FlashAttention还优化了内存使用，它不为反向传播算法存储完整的注意力矩阵。在传统的注意力机制中，存储整个注意力矩阵会占用大量内存，这对资源有限的设备来说是一个挑战。FlashAttention通过在计算过程中即时更新并使用注意力矩阵的部分结果，避免了存储整个矩阵，从而大幅减少了内存占用。这种方法不仅提高了模型的运行效率，还使得模型能够在内存受限的环境中运行，扩展了其应用范围。

通过以上优化策略，FlashAttention实现了训练加速和空间复杂度的优化，同时保持了与标准注意力机制相同的精确性。具体的FlashAttention分块优化示意图可参考图3 - 3。它将输入的查询、键、值对应的输入向量（Q、K，V矩阵）分成多个小块，并在SRAM上进行计算，然后将归一化指数函数Softmax（对应图中的sm）结果写回HBM中。这种分块计算的方式使得FlashAttention能够处理更长的序列，实现结果显示比PyTorch更快。


![image](https://github.com/user-attachments/assets/642dbd42-7507-47ec-9df3-edebd2ea61d2)



**图3 - 3 FlashAttention分块优化示例**

展示了输入向量（Q、K、V矩阵 ）分块在SRAM上计算，结果写回HBM的过程，涉及外循环、内循环等操作 。

其中，N和d分别表示Q矩阵、K矩阵和V矩阵的行和列。

FlashAttention作为CUDA中的融合内核实现，优化方法已经被集成到了一些开源框架中，如PyTorch、DeepSpeed、Megatron - LM、Triton和XFormer等，用以加速Transformer模型的训练和推理过程。


![image](https://github.com/user-attachments/assets/1232e28b-9fb5-4bcc-95ff-250b02ce3b68)


**3. 投机采样**

除了针对大模型中具体的Transformer网络结构提出的推理加速策略外，在模型工作任务的分配层面也提出了一些优化策略。我们知道大模型的推理过程通常使用自回归采样方法，即逐个Token进行串行解码。这种推理过程非常缓慢，因为每个Token的生成都需要将所有参数从存储单元传输到计算单元，导致内存访问带宽成为瓶颈。


为了解决这个问题，研究人员提出了投机采样加速策略来直接优化这个过程。通过同时使用一个大模型和一个小模型加速大模型的推理过程，其中，大模型是指原始目标模型，小模型是指比原始模型小得多的近似模型。

投机采样的核心思想是将简单的Token生成交给近似模型处理，而困难的Token则交给大模型处理。近似模型用于进行自回归串行采样，而大模型则用于评估采样的结果。它可以在不妨碍生成效果的前提下，显著提高推理速度。

投机采样的工作步骤如下。

1）使用一个小型模型（Mq）进行自回归采样，连续生成一定数量（如γ个）的标记。

2）将生成的γ个标记与前缀拼接在一起，作为输入送入一个大模型（Mp）进行一次前向计算。

3）对大模型和小型模型的逻辑值结果进行比对，如果发现小型模型生成的某个标记x∈γ不好，即小型模型的概率q(x)大于大模型的概率p(x)，则以一定概率（1 - p(x)/q(x)）拒绝该标记的生成，并从一个新的概率分布中重新采样一个标记。

如果对小型模型生成的结果满意，则使用大模型采样下一个标记，重复步骤1。



投机采样的优势在于可以同时计算多个标记，从而提高计算访存比，加速推理过程。同时，投机采样保证了生成结果的采样分布与使用原始模型的采样分布完全相同。这是因为对任意分布p(x)和q(x)，通过投机采样从这两个分布中所得到的标记分布与仅从p(x)中采样所得到的标记分布相同。

应用投机采样可以在大模型的推理过程中获得显著的加速效果，而不会牺牲生成效果。例如，GPT - 4报告中提到的OpenAI线上模型推理就使用了投机采样。


![image](https://github.com/user-attachments/assets/257f2dfc-1d19-4e69-a48d-4704657d3c2c)


### 3.2 大模型训练技术

大模型的训练面临着庞大的训练数据量和模型参数量的挑战。由于体量庞大，因此一旦出现训练任务异常，需要承担的成本损失巨大，为了满足普通开发者对大模型训练的需求，需要针对性地使用大模型训练技术。另外，由于大模型训练动辄需要数百块显卡，需要并行训练数月的时间，因此还需要在保证训练精度的情况下尽可能提高训练的速度。

实际上，随着开源大模型训练技术在普通开发者群体的不断应用，在节省成本和保证精度的平衡上，不断有更好的新技术提出。本节主要介绍并行训练、训练容错以及混合精度训练这几种通用的大模型训练技术。

#### 3.2.1 并行训练

现如今，大模型的参数和数据量的增长速度，已经远超GPU和训练加速框架的发展速度。因此，在处理大规模的模型训练时，底层系统的计算压力也在不断增加。表3 - 1展示了在一块NVIDIA V100 GPU上训练语言模型所需的时间。可以看到，这个训练时间对技术人员来说是无法接受的。

**表3 - 1 在NVIDIA V100 GPU上训练语言模型所需的时间**

|模型|参数量|数据集大小|训练时间|
| ---- | ---- | ---- | ---- |
|GPT|110M|4GB|3天|
|BERT|340M|16GB|50天|
|GPT - 2|1.5B|40GB|200天|
|GPT - 3|175B|560GB|90年|

虽然英伟达公司在不断推出性能远超V100的各类新一代GPU，但在普通开发者面前，提升算力设备虽然简单直接有效，但不能根本性地解决问题。以GPU的单卡显存容量为例，目前较为通用的A100显存为80GB，这已经远远不足以支持大模型的单卡训练。除此之外，随着更大规模的模型的提出，即便是以A100、H100为主要GPU搭建的算力集群，也无法满足多个开发者同时进行大模型训练的需求。

为了应对这些问题，目前最常见的解决方案是采用分布式训练方法。将任务放到分布式环境中，这种解决方案比在单机环境下更为复杂。分布式训练方法中最主要的技术难点是如何进行有效的并行训练。接下来我们介绍几种主流的并行训练方法。

**（1）数据并行**

数据并行是一种常用的分布式训练策略，它通过将相同的模型权重复制到多个设备上，并将数据分割成多个批次，将这些批次分配给每个设备进行并行处理。在数据并行中，每个设备独立地计算其分配的数据批次的损失函数，并通过反向传播算法将梯度信息传递回主模型。最后，通过聚合各个设备的梯度更新，更新主模型的权重。

如图3 - 4所示，将训练数据分配给节点1、节点2、节点3、节点4共4个批次。节点都维护一个模型副本，并独立地对分配的数据批次进行前向传播（用于计算网络的预测结果）和反向传播（用于计算梯度并更新网络的参数）计算。这种方式允许多个设备同时处理不同的数据批次，从而加快训练过程，有效利用多个设备的计算资源。


![image](https://github.com/user-attachments/assets/5e7cef60-b688-47e0-af3e-9a60d0980f44)


**图3 - 4 数据并行**

展示了训练数据分配给4个节点，节点进行模型计算并更新的过程 。

然而，这种简单的并行方式仍然存在一些问题。例如，将数据分配给多个设备进行处理可能导致负载不均衡的问题。主GPU（或主设备）承担较大的计算负载，而其他设备的利用率较低。这样负载不均衡可能导致训练过程的效率下降，也限制了整个系统的可扩展性。

除此之外，在数据并行中，不同设备之间需要进行通信以同步模型权重和梯度信息。当使用参数服务器（PS）架构时，通信开销可能会变得更大。在参数服务器架构中，所有设备都需要与中心化的参数服务器进行通信，以发送和接收模型权重与梯度信息。这种集中式通信模式可能会导致网络瓶颈和延迟，尤其是在大规模的模型和大规模的集群上。通信开销的增加可能会降低训练的速度，并增加整个系统的复杂性。


![image](https://github.com/user-attachments/assets/d0dffa26-1f68-4710-ad94-f5b7d1889e3f)


**（2）张量并行**

张量并行的核心原理是将矩阵进行分块并计算，即将计算进行并行处理。例如，对于矩阵乘法Y = XA，其中A、X和Y都是二维矩阵，矩阵形状分别为(a,b)、(b,c)、(a,c)，有两种切分方式可以进行分块计算。

- **第一种列并行**：把A按照第二维（列）分割成k份，每一份的形状都为(a,b_split)，每一份放在一个GPU上与X相乘，得到k个(a,b_split)，最后将各个GPU上的结果按照第二维进行顺序拼接就得到了最终结果Y = [Y1,Y2]。张量的列并行示例如图3 - 5所示。

**图3 - 5 列并行示例**

展示了矩阵X与分块后的矩阵A计算，得到结果Y的过程 。

- **第二种行并行**：将A按照第一维（行）分割成k份，每一份的形状都为(a_split,b)；为了保证运算效率，同时将X沿着第二维（列）分割成k份，每一份的形状都为(b,c_split)。在每个GPU上分别计算(b,c_split)×(a_split,b)，最后将各自的结果相加就得到了最终结果Y。

XA = [X1 X2][A1;A2] = X1A1 + X2A2 = Y1 + Y2 = Y

这里X1的最后一个维度等于A1最前的一个维度，X1和A1可以放到第一个GPU上进行计算，X2和A2可以放到第二个GPU上进行计算，然后把结果相加。

张量的行并行示例如图3 - 6所示。


![image](https://github.com/user-attachments/assets/945873f0-0550-4a23-8e14-69f7efb30dc6)


**图3 - 6 行并行示例**

展示了矩阵X、A分块后在不同GPU计算再相加得到结果Y的过程 。

多头自注意力机制中的张量并行示意图如图3 - 7所示。输入数据X首先通过线性变换层f进行编码，将A按照注意力切分成[Q1,K1,V1]、[Q2,K2,V2]两个分块进行张量并行计算，各自通过归一化指数函数（Softmax）计算注意力加权值，并通过Dropout（节点随机抛弃技术，通过将神经网络中的少量节点随机抛弃，从而减少神经网络训练中的过拟合现象）抛弃部分节点后进行注意力加权值计算，并将计算结果[Y1,Y2]分别与[B1,B2]相乘，得到输出向量[Z1,Z2]然后进行Allp.reduce操作g，获得Z = Dropout(Y,B)函数的最终结果Z。


![image](https://github.com/user-attachments/assets/ed17ef3e-df02-4cd4-b8be-75978758c631)


**图3 - 7 多头自注意力机制中的张量并行示意图**

展示了输入数据X经线性变换、分块计算、Softmax、Dropout等操作得到最终结果Z的流程 。

在Transformer中，除了自注意力机制外，张量并行的技术还可以在词嵌入、多层感知机以及交叉熵等的计算过程中使用。

**（3）流水线并行**

流水线并行是一种解决加速卡的存储容量和协同计算问题的方法。它从模型切分和调度执行两个角度来解决这些问题。

在模型切分方面，通常有张量切分和图切分两种方式。张量并行可以将大尺寸的张量切分到多张加速卡上，从而有效减少每张卡要处理的参数量。然而，采用这种方式时，单机上最多能训练约10B参数的模型，而且由于通信和计算不能重叠的特点，不太适合多机环境。

更大的模型则需要进行图切分，按层级将模型切分到不同的设备上，以降低单个加速卡的存储需求，并隐藏加速卡之间的通信时间，更充分地利用GPU卡的计算能力。从图切分的角度来看，流水线并行将模型进一步按层切分到不同设备上，相邻设备在计算时只需要传递邻接层的中间变量和梯度即可。例如，对前馈网络可以采用张量并行，将全连接层的参数切分到不同加速卡上，并在同一设备内进行全量通信的参数求和。而采用流水线并行，可将前馈网络层作为切分点，将模型层切分到不同设备上，设备之间只需要发送/接收较少的参数量即可。相比张量并行，流水线并行的通信参数量更少。

张量并行与流水线并行如图3 - 8所示。


![image](https://github.com/user-attachments/assets/68ea0696-1dd5-429d-a25c-c961bb56cab3)


**图3 - 8 张量并行与流水线并行**

对比展示了张量并行和流水线并行在计算和通信参数传递上的差异 。

![image](https://github.com/user-attachments/assets/e891f14e-a180-435d-a8b0-c7b76956e86f)



可以看出，AllReduce求和的全量通信参数数量为2MK，而采用流水线并行将全连接层作为切分点切分模型层，在设备间只需要传输MK的参数量。

朴素流水线并行是实现流水线并行训练的最直接方法，即对模型层进行划分，然后将划分后的部分放置在不同的GPU上。

如图3 - 9所示，以一个使用4条设备流水线并行的模型更新过程为例，在朴素的流水线并行中，需要从F1开始执行，经过F2、F3、F4、B4、B3、B2，直到B1。

![image](https://github.com/user-attachments/assets/2cf2d8a3-f2a7-4db2-9f8a-443e135e02a2)


**图3 - 9 4条设备流水线并行的模型示例**

展示了4条设备流水线并行时模型前向（F）和反向（B）计算流程 。

如图3 - 10所示，同一时刻只有一个设备进行计算，其他设备处于空闲状态，计算设备的利用率通常较低。

![image](https://github.com/user-attachments/assets/58ff1b0c-328f-4d3b-9524-6268e60bc7b1)


**图3 - 10 朴素流水线并行**

展示了朴素流水线并行中设备计算时存在空闲状态的情况 。

与朴素流水线并行相比，将批次再进行切分可以减少设备间的空闲状态时间，从而显著提高流水线并行的设备利用率。这种切分方式被称为F - then - B（Forward - then - Backward）调度方式。如图3 - 11所示，在F - then - B调度方式中，原本数据并行切分的批次进一步分为多个微批次（Micro - batch），如将F1切分成F1.1,F1.2,F1.3。每个流水线并行的计算单元首先整体进行前向计算，从F1.1,F1.2,F1.3开始，经过F2.1,F2.2,F2.3、F3.1,F3.2,F3.3、F4.1,F4.2,F4.3，然后进行反向计算，从B4.1,B4.2,B4.3开始，经过B3.1,B3.2,B3.3、B2.1,B2.2,B2.3、B1.1,B1.2,B1.3。通过在同一时刻分别计算模型的不同部分，F - then - B可以显著提高设备资源的利用率。

**图3 - 11 Forward - then - Backward调度方式**

展示了F - then - B调度方式中模型前向和反向计算按微批次进行的流程 。

然而，F - then - B模式的缺点是它需要缓存多个微批次的中间变量和梯度，这可能导致显存的实际利用率不高。因此，有人进一步提出了一种解决显存利用率不高问题的方法：1F1B（1 Forward 1 Backward）流水线并行。在1F1B模式下，前向计算和反向计算交叉进行，可以及时释放不必要的中间变量，从而减少显存的占用。在图3 - 12中，1F1B模式下的F42（第2个微批次的前向计算）在计算之前，F41的反向计算B41（第1个微批次的反向计算）已经完成，这意味着可以释放F41的中间变量。因此，F42可以复用F41中间变量的显存，从而节省显存的使用。

**图3 - 12 1 Forward 1 Backward流水线并行**

展示了1F1B流水线并行中前向和反向计算交叉进行，节省显存的原理 。


![image](https://github.com/user-attachments/assets/a3035f58-f351-4d6f-b00c-e68995568df2)


![image](https://github.com/user-attachments/assets/5533d271-b3f7-4390-9f28-d33260c26b2b)


相比F - then - B模式，1F1B方式可以节省峰值显存。与朴素流水线并行相比，1F1B模式明显降低了显存的需求，同时显著提高了设备资源的利用率。这使得1F1B成为一种有效的流水线并行方式。

**（4）MOE并行**

上述的几种方法都是针对单个模型的并行训练，实际上MOE并行（也称专家并行）能够通过条件将计算拓展到多个模型，实现比传统神经网络架构高达1000倍的模型容量，并且在语言建模和机器翻译任务中取得了比以往更好的结果。

在传统的神经网络架构中，每个数据计算都会激活整个网络，随着模型规模和数据量的增加，这会导致训练成本迅速增长。MOE并行技术是一种基于稀疏专家网络（expert networks）的深度学习模型架构。其核心思想是将大模型拆分成多个小模型（即专家），每轮迭代根据输入样本动态选择一部分专家进行计算，从而实现计算资源的有效利用。这一技术不仅降低了训练成本，还通过专家间的互补提升了模型的整体性能。在该方法中，通过利用门控网络，有选择地激活与特定输入相关的专家网络部分，具体而言，每个输入都会被分配给一组专家网络，只有被激活的专家网络会对输入的特征进行计算，并将它们的输出组合起来以生成最终的输出，从而避免了整个网络都进行计算。 

