### 第3章 大模型技术拓展 
#### 图3-13 MOE并行

MOE并行技术为大模型训练提供了有力支持。通过稀疏专家网络的设计和优化，MOE在降低训练成本、提升模型性能等方面展现出了显著优势。


![image](https://github.com/user-attachments/assets/5e463d13-f09b-461b-ab5d-446e16c4d828)


#### 3.2.2 训练容错
由于大模型训练过程中使用的硬件和软件较多，难免在运行中经常出现一些异常情况。由于训练任务通常涉及数百个GPU节点，因此可能导致任务在几小时到几天内频繁出现异常。

根据Bloom报告，在一个包含大约400个GPU的新集群上，通常每周平均会有1 - 2个GPU故障。Meta公司的175B OPT训练记录显示，在半个月内，模型参数为175B的训练实验由于硬件、基础设施和其他问题而中断了40多次。此外，即使单个节点出现故障，任务中所有其他节点上的进程也必须停止，并且任务需要被终止和重新提交。这个过程增加了异常阶段的时间开销。

更困难的是，训练任务出现异常的原因有很多，例如节点硬件故障、系统故障、网络问题、存储问题和训练代码错误等。而部分错误甚至无法仅通过重新调度来解决，需要深入分析，找出导致错误的节点，并隔离故障节点以恢复训练。例如，识别超时异常背后的原因可能非常复杂。既有可能是由于速度慢或存在故障节点，也有可能是因为存储系统故障或用户的通信数据发送和接收代码中存在错误而造成的。此外，不同的原因对应不同的恢复策略，给错误诊断带来了重大挑战，确定这些问题通常需要几个小时甚至更长时间。

总而言之，由于大模型的训练过程涉及很多硬件及软件的协同工作，因此异常情况的出现不可避免。因为其数量大、类别多，且恢复开销大，所以训练容错机制十分重要。下面介绍两种常用的训练容错机制。
- **（1）检查点**

在模型并行训练中，检查点是指在特定时间点保存模型参数和优化器状态的副本。检查点的作用是记录模型的当前状态，以便在遇到故障或需要中断训练时能够恢复到先前的状态，而无须从头开始。

除此之外，通过检查点，可以检查模型在不同时间点的情况，分析训练过程中的变化和趋势，并进行模型评估和比较。

- **（2）启动器 - 工作器**

在传统的大语言模型训练过程中会提交训练任务给节点的工作器，如果发生故障，所有的工作器将被终止，并需要手动重新提交训练任务。传统大模型的训练过程如图3-14所示，这种方法存在显著的问题，即它会产生高昂的重新启动成本，并且如果检查点的频率过低，可能会导致大量的训练进度丢失。


![image](https://github.com/user-attachments/assets/8c4f6854-db6b-4e97-a8ff-9f15f6b93188)


因此，采用启动器 - 工作器（Launcher-Worker）架构来管理训练任务的生命周期并确保容错性，如图3-15所示。

启动器的主节点负责将任务分配给工作器，包括屏障同步、启动训练进程、执行错误检查和停止训练进程等任务。工作器在接收到任务后立即执行，并将结果报告给主节点。主节点根据任务结果和用户交互的情况确定适当的操作，例如通知工作器退出等。这种架构可以高效地管理训练任务，并在删除有问题的进程后重建新的启动器来确保系统能够从故障中恢复。

此外，启动器 - 工作器架构允许在训练进程开始之前执行基础设施预热任务，确保基础设施准备就绪，并提前识别潜在问题。

![image](https://github.com/user-attachments/assets/0642b049-7b91-4ae1-986e-9dd531173db0)


#### 图3-15 启动器 - 工作器架构
#### 3.2.3 混合精度训练
混合精度训练是一种在深度学习训练中使用不同精度（FP16和FP32）的权重和梯度的训练加速技术，通过使用半精度浮点数（FP16）来存储模型的权重和梯度。这种方法在尽量减少精度损失的同时，可以显著提高训练速度。然而，对比IEEE标准下的FP16和FP32的范围可知二者可以表示的精度差距很大，如图3-16所示。因此，将FP32神经网络计算转换为FP16的最大问题是精度。


![image](https://github.com/user-attachments/assets/d8149a85-1079-46ee-9291-e1cc6f0cecf1)


#### 图3-16 FP16与FP32

为了解决FP16带来的精度损失问题，技术人员提出了以下解决方法。

- **（1）保留FP32并放大损失值**

在混合精度训练中，权重、损失值和梯度值都以FP16的形式存储。为了保持与FP32网络相同的精度，会维护一个FP32的主权重副本，并在优化器步骤中使用权重梯度更新该主权重副本。在每次迭代中，使用主权重的FP16副本进行前向和反向传播，从而减少FP32训练所需的存储和带宽。图3-17说明了混合精度训练过程。


![image](https://github.com/user-attachments/assets/f22b729c-8dcf-467d-80d5-d637cef65624)


#### 图3-17 混合精度训练过程

由于在FP16中，无法表示绝对值小于2⁻²⁴的值。而较小的梯度值在与学习率相乘时，如果梯度值非常小，接近或等于FP16无法表示的下限，那么这种计算可能会导致精度损失或错误的结果。使用单精度副本来进行更新可以克服这个问题并确保准确性。另外，即使梯度值在FP16范围内可以表示，但在更新权重时，为了与权重对齐二进制小数点而执行的右移操作仍可能导致梯度变为零。因此，在更新权重之前，通常会将放大后的梯度转换回FP32格式，应用优化器的更新规则后，再将更新后的权重参数转换回FP32格式，以确保准确性。

- **（2）改进算术运算**

神经网络的算术运算可以分为三类：向量点积、归约和逐点操作。在降低精度进行算术运算时，这些类别需要不同的处理方式。为了保持模型的准确性，在向量点积中，通常使用FP16向量点积将部分乘积累加到FP32值中，然后在写入内存之前将其转换为FP16。简而言之，就是利用FP16进行矩阵相乘，利用FP32来进行加法计算，弥补丢失的精度。这样可以有效减少计算过程中的舍入误差，尽量减少精度损失的问题。改进的向量点积如图3-18所示。


![image](https://github.com/user-attachments/assets/334b7dc1-90bd-4762-a164-78eaa9bf0af8)


#### 图3-18 改进的向量点积
如果没有使用FP32进行累加，一些FP16模型无法达到最佳的准确性。而大规模的归约（对向量元素求和）同样应该使用FP32进行。这类归约主要出现在批归一化层和Softmax层中，用于累积统计信息。而对于逐点操作，例如非线性函数和逐元素矩阵乘法，由于算术精度不影响这些操作的速度，因此可以使用FP16或FP32进行运算。 



### 3.2.1 并行训练（续）


![image](https://github.com/user-attachments/assets/304e4fbc-750b-42e8-a82b-8b84579e55a0)


如图3 - 13所示，假设模型由4个专家网络（Expert 1 - Expert 4）组成，对于给定的输入，门控网络（Gating Network）会根据输入特征计算每个专家网络的“相关性得分”，然后选择得分最高的一个或多个专家网络（图中假设选择了Expert 1和Expert 3）来处理该输入。被选中的专家网络分别对输入进行计算，最后将它们的输出进行合并（例如加权求和等方式），得到最终的模型输出。

**图3 - 13 混合专家（MOE）架构示例**

展示了输入经门控网络选择专家网络（Expert）处理后合并输出的流程 。

MOE并行的优势在于可以在不显著增加计算资源的情况下，大幅提升模型的容量和表达能力。通过动态选择专家网络，模型能够更灵活地应对不同类型的输入，提高了模型在复杂任务上的性能表现。同时，由于不是所有专家网络都参与每一次计算，也降低了计算成本和训练时间。

此外，MOE并行还可以与其他并行训练方法（如数据并行、张量并行等）结合使用，进一步提高训练效率和模型扩展性。例如，在数据并行的基础上，每个计算节点可以采用MOE架构，使得不同节点处理不同数据批次时，内部又能根据输入动态分配专家网络进行计算，从而充分利用分布式计算资源，加速大模型的训练过程。

#### 3.2.2 训练容错
在大模型训练过程中，由于训练任务的复杂性和长时间运行，可能会遇到各种故障，如硬件故障（GPU故障、服务器故障等）、软件故障（程序崩溃、内存泄漏等）以及网络故障（网络中断、延迟过高）等。这些故障如果不能及时处理，可能导致训练任务失败，之前投入的大量计算资源和时间都将浪费。因此，训练容错技术在大模型训练中至关重要，它能够提高训练过程的稳定性和可靠性，减少因故障带来的损失。

**1. 检查点与恢复机制**
检查点（Checkpoint）是在训练过程中定期保存模型的参数、优化器状态以及训练进度等信息的一种机制。通过保存检查点，当训练过程中出现故障时，可以从最近的检查点恢复训练，而不需要从头开始训练。

具体来说，训练过程中每隔一定的训练步数（例如每训练1000步）或时间间隔（如每小时），系统会将当前模型的参数（如神经网络中各层的权重）、优化器的状态（如梯度累计值、学习率等）以及训练的进度信息（如当前训练的轮数、已处理的样本数量等）保存到磁盘或其他存储设备上。当故障发生后，训练系统可以读取最新的检查点文件，重新加载模型参数和优化器状态，并从记录的训练进度继续训练。

为了提高检查点的保存和恢复效率，通常会采用一些优化策略。例如，采用增量式检查点保存方法，只保存与上一个检查点相比发生变化的参数部分，而不是保存整个模型参数，这样可以减少存储开销和保存时间。在恢复时，快速定位和加载变化的参数，加速恢复过程。此外，还可以利用分布式存储系统，将检查点数据分散存储在多个存储节点上，提高存储的可靠性和读取速度。

**2. 故障检测与隔离**
故障检测是及时发现训练过程中出现的故障的关键环节。常见的故障检测方法包括硬件监控和软件监控。硬件监控主要通过监控GPU、CPU、内存等硬件设备的运行状态，如检测GPU的温度是否过高、显存使用率是否异常、CPU的负载是否过高、内存是否存在泄漏等指标。当这些硬件指标超出正常范围时，系统可以判断可能存在硬件故障，并及时发出警报。

软件监控则侧重于监控训练程序的运行状态，例如检查程序是否出现崩溃、是否存在死循环、模型训练的损失值是否突然异常增大或减小等。通过在训练代码中插入监控代码，定期检查程序的运行状态变量和关键指标，一旦发现异常情况，立即触发故障检测机制。

一旦检测到故障，故障隔离机制会将发生故障的设备或程序部分与正常运行的部分隔离开来，防止故障扩散影响到整个训练任务。对于硬件故障，可能会将故障的GPU或服务器从训练集群中暂时移除，避免其对其他正常设备产生干扰。对于软件故障，可能会终止出现问题的训练进程，并尝试重新启动该进程，或者将相关的计算任务重新分配到其他正常的计算节点上。

**3. 弹性训练**
弹性训练是一种能够根据训练环境的变化动态调整训练资源的技术。在大模型训练中，训练资源（如GPU数量、服务器数量等）可能会因为各种原因发生变化，例如部分GPU出现故障需要移除、有新的计算资源加入训练集群等。弹性训练技术能够使训练任务在资源变化的情况下继续正常进行，无需重新启动整个训练过程。

实现弹性训练通常需要借助分布式训练框架和资源管理系统。当检测到训练资源发生变化时，资源管理系统会重新分配计算任务，将原本在故障设备上的训练任务迁移到其他正常设备上，或者将新加入的计算资源纳入训练任务的分配中。分布式训练框架则需要具备动态调整模型并行策略的能力，例如根据新的资源配置重新划分数据并行的批次、调整张量并行和流水线并行的参数等，以确保训练任务能够在新的资源环境下高效运行。

#### 3.2.3 混合精度训练
在大模型训练中，数据的精度（即数据所使用的数值表示方式，如单精度浮点数、双精度浮点数等）对训练效率和模型性能都有重要影响。传统的训练方法通常使用单精度浮点数（float32）进行计算，虽然能够保证一定的计算精度，但在大规模计算场景下，会消耗大量的计算资源和内存空间。为了在保证模型精度的前提下提高训练效率，混合精度训练技术应运而生。

**1. 精度概念与选择**
- **单精度浮点数（float32）**：是一种常用的数值表示方式，在计算机中占用32位存储空间。它能够提供足够的精度来表示大多数数值，在深度学习训练中广泛应用。然而，对于大规模的模型和数据集，使用单精度浮点数进行计算会导致计算量和内存占用较大，影响训练速度和资源利用率。
- **半精度浮点数（float16）**：占用16位存储空间，相比单精度浮点数，它在存储和计算上都更加高效。使用半精度浮点数可以减少内存占用，加速计算过程，尤其是在现代GPU等硬件设备上，对float16的计算有专门的优化支持。但是，半精度浮点数的精度相对较低，可能会在一些对精度要求较高的计算中引入误差，影响模型的训练效果。
- **混合精度**：混合精度训练结合了单精度和半精度浮点数的优势。在训练过程中，对于一些对精度要求不高的计算（如矩阵乘法等），使用半精度浮点数进行计算，以提高计算效率和减少内存占用；而对于一些关键的计算（如梯度计算、模型参数更新等），则使用单精度浮点数，以保证计算的准确性，避免因精度不足导致模型训练失败或性能下降。

**2. 混合精度训练实现**

实现混合精度训练通常需要借助深度学习框架的支持，如PyTorch和TensorFlow等。这些框架提供了相关的工具和接口，方便开发者在训练过程中动态切换数据精度。

在训练开始前，开发者可以设置精度策略，指定哪些计算使用半精度浮点数，哪些使用单精度浮点数。例如，在PyTorch中，可以使用`torch.cuda.amp`（Automatic Mixed Precision，自动混合精度）模块来实现混合精度训练。通过调用`GradScaler`类来自动调整梯度的缩放，防止在使用半精度浮点数计算梯度时出现下溢（数值过小无法准确表示）问题。具体来说，在正向传播计算中，框架会自动将输入数据转换为半精度浮点数进行计算，以加速计算过程；在反向传播计算梯度时，通过梯度缩放技术，将梯度从半精度转换为单精度进行计算和更新模型参数，确保梯度计算的准确性。



同时，硬件设备也需要对混合精度计算提供支持。现代的GPU（如NVIDIA的部分高端GPU）具备专门的半精度计算单元，能够高效地执行半精度浮点数的计算操作，进一步提高混合精度训练的效率。



通过采用混合精度训练技术，可以在不显著降低模型精度的前提下，大幅提高大模型的训练速度，减少内存占用，从而更有效地利用计算资源，加速大模型的训练过程，使其能够在更短的时间内完成训练并达到较好的性能表现。 
