### 3.3 大模型评估

随着大模型相关应用在不同的领域大放异彩，并在研究和日常生活中扮演越来越重要的角色，对它们进行科学系统评估就变得越来越重要。大模型的评估是基于大模型应用评估的扩展，接下来基于大模型应用任务引入不同的评估数据集、评估方法（自动和人工），并使用开源大模型评测工具OpenCompass调用数据集CMB进行评估。



#### 3.3.1 大模型评估概述

大模型评估是评估训练好的大模型的性能，目标是评估大模型在未见过的数据上的泛化能力和预测准确性，以便更好地了解大模型在真实场景中的表现。



微软亚洲研究院的研究员们调研了200多篇大模型相关文献，撰写了介绍大模型评测领域的第一篇综述文章“A Survey on Evaluation of Large Language Models”，该文章从评测对象、评测领域、评测方法和目前的评测挑战等几大方面对大模型评估进行了详细的梳理和总结。综合来说，大模型的评估重点关注“评估什么”“根据什么评估”以及“如何评估”3个维度。

1. **“评估什么”**：是指大模型的评估任务，包括语言生成任务、知识利用任务、复杂推理任务等。

2. **“根据什么评估”**：是指使用哪些数据来评估大模型，包括现有研究中广泛使用的基准数据集，开放领域数据集和一些特殊领域的数据集。 

3. **“如何评估”**：是指应该使用哪些大模型评估方法，包括广为接受的评估指标和传统的评估方法，以及自动化评估方法、自适应评估方法等。



模型评估是模型开发过程中不可或缺的一部分。通过评估大模型有助于人们更好地了解大模型的优势和劣势，辨识出最适合处理特定数据的模型和预测所选模型在未来的表现。



#### 3.3.2 大模型评估任务
大模型评估任务涵盖了对大模型能力评估的多个方面，如判断大模型是否具备基础的理解和判断推理能力，以及在各个具体行业应用的评估等。以国内通用的星火大模型为例，大模型评估任务主要包括语义理解、知识推理、专业能力、应用能力、指令跟随、鲁棒性、偏见、幻觉和安全性等。这些评估维度不仅涵盖了模型的基本能力，如理解和推理，还扩展到了模型在特定领域内的应用能力和潜在的风险因素。

#### 3.3.3 大模型评估数据集
模型评估还涉及选择合适的评估数据集。评估数据集和训练数据集应该是相互独立的，避免数据泄露的问题。大模型评估数据集用于测试和比较不同语言模型在各种任务上的性能。选择的评估数据集需要具有代表性，这意味着它应该涵盖了各种情况和样本，以便大模型在各种情况下都能表现良好。评估数据集的规模也应该足够大，以充分评估大模型的性能。此外，评估数据集应该包含一些特例样本，以确保大模型在处理异常或边缘情况时仍具有良好的性能。


目前，针对单一任务的NLP算法，通常需要构造独立于训练数据的评估数据集，使用合适的评估函数对大模型在实际应用中的效果进行预测。限于篇幅，本节不讨论单一任务语言模型的单个数据集，仅讨论大模型能力评估的基准数据集。随着大模型基准数据集的不断发展，出现了各种各样的基准数据集来评估其性能。这些基准数据集分为两类：通用语言任务基准数据集和特定领域任务基准数据集。

1. **通用语言任务基准数据集**：


旨在评估大模型在不同任务下的性能。常用的基准数据集包括Chatbot Arena、MT-Bench（Machine Translation Benchmark，机器翻译基准测试）、HELM、Big-Bench、KoLA、DynaBench、AlpacaEval、OpenLLM等。
    - **Chatbot Arena**：提供了一个平台，通过用户参与对大模型处理结果进行投票，以评估和比较不同的聊天机器人模型。用户可以参与匿名模型并通过投票表达他们的偏好。该平台收集了大量的选票，有助于评估模型在现实场景中的性能。同时，它也提供了针对聊天机器人模型的优势和局限性的宝贵见解，从而推动了聊天机器人的研究和发展。
    - **MT-Bench**：使用为对话处理量身定制了全套的综合问题（数据集）来评估大模型的多回合对话能力。相比传统评估方法，MT-Bench擅长模拟现实世界的对话场景，从而有助于更精确地评估模型的实际性能。此外，MT-Bench在衡量大模型处理复杂的多回合对话能力方面克服了传统评估方法的局限性。 
    - **HELM**：从语言理解、生成、连贯性、上下文敏感性、常识推理和特定领域知识等各个方面评估大模型，旨在全面测试它在不同任务中的性能。同时，有技术人员提出了一个综合组件，用于评估不同学科领域的大模型知识水平，使得技术人员能够理解这些模型中固有的显著限制，有助于深入理解它们在不同领域的能力。 
    - **Big-Bench**：能够评估多种能力的大模型。它由来自132个机构的450位作者贡献的204个具有不同挑战性的任务集合组成。这些任务涉及数学、儿童发展、语言学、生物学、常识推理、社会偏见、物理、软件开发等各个领域。 
    - **KoLA**：是一种面向知识的大模型评估基准数据集，也是评估大模型理解和推理深度的关键平台。 
    - **DynaBench**：探索了一些新的研究方向，例如循环中集成的影响、适应分布变化的特性、评估标注者效率、研究专家型标注者带来的影响以及增强模型在交互式环境中抵御目标对抗性攻击的鲁棒性。它支持在语言任务中进行众包评估，旨在实现动态基准测试，从而促进动态数据收集的研究，并可在一般的人机交互领域进行跨任务分析。 
    - **AlpacaEval**：是一个自动评估基准数据集，它专注于评估大模型在各种NLP任务中的性能，提供了一系列指标、稳健性度量和多样性评估指标来衡量大模型的能力。有助于推进不同领域的大模型应用，并促进人们对大模型性能的更深入理解。此外，AGIEval是一个专门的评估框架，用于评估基础模型在以人为中心的标准化考试领域的表现。 
    - **OpenLLM**：通过提供公共竞争平台来比较和评估不同大模型在各种任务上的性能，作为开源基准数据集，它鼓励技术人员提交模型并在大模型研究领域的不同任务上竞争，通过竞争推动发展。

还有一些其他的数据集，例如GLUE和SuperGLUE，旨在模拟现实世界的语言处理场景，涵盖文本分类、机器翻译、阅读理解和对话生成等多种任务。GLUE-X评估NLP模型在面向对象设计（OOD）场景中的鲁棒性，并提供了衡量和增强模型鲁棒性的建议。BOSS是一个用于评估NLP任务中分布外样本检测的鲁棒性基准数据集。PromptBench提供了标准化的评估框架，专注于提示工程在微调大模型中的重要性，用来比较不同的提示工程技术，并评估它们对模型性能的影响，促进了大模型微调方法的增强和优化。为了确保评估的公正性和公平性，提出了PandaLM模型进行专门的评估。与传统的主要强调客观正确性的评估数据集相比，PandaLM包含了重要的主观元素，如简洁性、清晰度、遵循指令的能力、全面性和正式程度。


2. **特定领域任务基准数据集**：


除了通用语言任务的基准数据集，还存在针对特定领域任务设计的基准数据集，来看一些可能用到的数据集。

为了评估大模型在多样化和高要求任务中的能力，引入了一系列专门的基准数据集来评估大模型在特定领域和应用程序中的能力。其中，ARB侧重于探索大模型在跨多个领域的高级推理任务中的性能；TRUSTGPT为解决大模型上下文中的伦理而设计，特别关注有害性、偏见和价值观对齐。EmotionBench基准数据集强调大模型对人类情绪反应的模拟；SafetyBench基准数据集专门用于测试一系列主流的中文和英文大模型的安全性能，该评估结果可指明当前大模型存在的重大安全缺陷。Choice-75用于评估智能系统日常决策能力；CELLO用于评估大模型理解复杂指令的能力。

此外，大模型的多模态能力也是一个重要的评价维度。为了评估MLLM（多模态大语言模型），MME采用精心设计的问题 - 答案对以及简明的指令设计，评估大模型的感知和认知能力，从而确保评估的公平性。

#### 3.3.4 大模型评估方法

在模型评估过程中，常用的评估方法会使用一系列评估指标（Evaluation Metrics）来衡量模型的表现，如准确率、精确率、召回率、F1分数、ROC（受试者操作特性曲线）和AUC（ROC下方面积的大小）等。基于评估指标是否可以自动计算，大模型评估方法分为自动评估和人工评估两种。如果可以自动计算，则将它归类为自动评估；否则归类为人工评估。

1. **自动评估**：是一种常见的评估方法。自动评估的原理和其他AI模型评估过程一样，用一些标准的指标计算出一定的值，作为模型性能的指标，如BLEU（双语评估研究）、ROUGE（面向召回率的词序列重叠度指标）等。
    - **BLEU**：是一种用于评估机器翻译结果质量的指标。它主要侧重于衡量机器翻译输出与参考翻译之间的相似程度，着重句子的准确性和精确匹配。BLEU通过计算N-Gram的匹配程度来评估机器翻译的精确率。 
    - **ROUGE**：是一种用于评估文本摘要（或其他自然语言处理任务）质量的指标。与BLEU不同，ROUGE主要关注机器生成的词序列中是否捕捉到用户提供的信息，重点评估生成摘要内容和信息的完整性。ROUGE通过计算N-Gram的共现情况，来评估机器生成的摘要的召回率。

简而言之，BLEU侧重于衡量翻译的准确性和精确匹配程度，更偏向于精确度，而ROUGE侧重于衡量生成的摘要信息的完整性和覆盖率，更偏向于召回率。这两个指标在不同的任务和应用场景中都有应用，因此在评估NLP模型时，经常会同时使用它们来综合评估模型的表现。

随着大模型的发展，也出现了一些先进的自动评估技术，比如：LLM-EVAL方法使用大模型进行开放域对话的统一多维度自动评估；PandaLM通过训练大模型来评估不同的模型，从而实现可复现和自动化的评估。

受心理测量学中的CAT（计算机自适应测试）的启发，有研究人员提出了一个用于大模型评估的自适应测试框架：并非简单计算答对率，而是根据各个被试（模型）的表现动态地调整测试问题的特征（如难度等），为模型“量身定制”一场考试。具体来说，CAT中的诊断模型CDM会根据被试之前的作答行为（对/错）对其能力进行估计，给出一个估计值。接着，选题算法会根据该估计值选择最具信息量或最适合它的下一道题，例如选择难度和被试能力最接近的题目。如此循环往复，直到测试结束。相比传统评估方法，该框架能用更少的题目，更准确地估计模型的能力。

2. **人工评估**：功能日益强大的大模型已经无法通过一般自然语言任务的标准评估指标来进行评估。因此，在一些不适合自动评估的非标准情况下，人工评估成为一种自然的选择。人工评估通过人工参与来对生成的结果进行质量和准确性的综合评估。与自动化评估方法相比，人工评估更接近实际应用场景，并且可以提供更全面和准确的反馈。

在评估中，通常会邀请评估人员（如技术专家、工程人员或普通用户）对模型生成的结果进行评估。评估者可以对大语言模型生成结果的整体质量进行评分，也可以根据评估体系从语言层面、语义层面以及知识层面等进行细粒度评分。

此外，对于一些文本生成类任务（比如机器翻译、文本摘要等），源于语言的灵活性和多样性，同样一句话可以有非常多种的表述方法。虽然可以采用某些自动评估协议，但这些任务中的人工评估相对更准确。但是由于人工评估成本高昂，如何有效地评测文本生成类任务的结果仍面临极大的挑战。例如，由于文化和个体差异，人工评估可能存在不稳定性。首先，由于人的主观性和认知差异，评估结果可能存在一定程度的主观性。其次，人工评估需要大量的时间、精力和资源，因此成本较高，而且评估的周期长，不能及时得到有效的反馈。此外，评估者的数量和质量也会对评估结果产生影响。

3. **评估工具**：大模型有很多评估工具，比如FlagEval、OpenCompass、Xiezhi（獬豸）和C-Eval等，可以选用适合的大模型基准数据集进行测试。这里选用上海人工智能实验室开源的大模型评测工具OpenCompass，使用上文提到医学评价基准数据集CMB，对第9章微调后的大模型进行评估实践。OpenCompass支持很多常用的大模型，测试数据集也很丰富，可以从语言、知识、推理、理解、长文本、安全、代码等多个维度测试大模型的能力。下面是简单的使用流程。
    - **OpenCompass安装**：首先，使用install命令安装OpenCompass。
```bash
git clone https://github.com/open-compass/opencompass.git
cd opencompass
pip install -e.
```
    - **数据集准备**：OpenCompass支持的数据集主要包括两个部分。
        - **HuggingFace数据集**：HuggingFace Dataset提供了大量的数据集，这部分数据集会在运行时被自动下载。 
        - **自建以及第三方数据集**：OpenCompass还提供了一些第三方数据集及自建中文数据集。 
    - **进行评测**：在OpenCompass项目根目录下运行指定命令，将医疗数据集CMB加载至项目根目录下的configs/datasets/cmb/data目录下。OpenCompass支持大多数常用于性能比较的数据集，具体支持的数据集列表请直接在configs/datasets下查找。具体实践可以参考第9章。


![image](https://github.com/user-attachments/assets/9cbfd53c-bf79-4e5a-9387-2106ae53fac1)


### 3.4 大模型部署
国内很多大厂都在做自己的基础大模型，比如Qwen、Baichuan、文心一言、星火、盘古、豆包等。学习大模型的有效方式就是基于基座大模型进行微调，然后基于微调后的模型进行部署运行。

#### 3.4.1 模型环境搭建

相比普通模型，大模型部署对GPU硬件的要求较高。这里以第6章智能客服问答实践中使用的ChatGLM3-6B为例，其GPU硬件需求，如下表所示。

| 量化等级 | 最低GPU显存（推理） | 最低GPU显存（参数微调） |
| ---- | ---- | ---- |
| FP16（无量化） | 13GB | 14GB |
| INT8 | 8GB | 9GB |
| INT4 | 6GB | 7GB |

大模型部署包括云环境部署及本地环境部署，根据条件和需求选择合适的部署方式。云环境部署可以使用AutoDL上的GPU进行。以ChatGLM3-6B为例，推理的最低GPU显存需要13GB以上，可以选择RTX4090、RTX3090、RTX3080*2、A5000等GPU规格。本地部署硬件要求请参考云环境部署。

下面以开源大模型ChatGLM3-6B为例介绍软件依赖的安装和配置。与一般的Python模型部署类似，首先在开源大模型项目中找到包含项目依赖模块的requirements文件，然后使用pip安装依赖即可，指令如下所示。
```bash
cd ChatGLM3
pip install -r requirements.txt
```

**模型部署操作可以参考第6章，包括搭建部署环境及下载开源模型。**

注意，ChatGLM3-6B中Transformers库的版本推荐用4.27.1，但理论上不应低于4.23.1。此外，如果需要在CPU上运行量化后的模型，还需要安装GCC与OpenMP，多数Linux发行版默认已安装。对于Windows系统，可在安装TDM-GCC时勾选OpenMP。Windows测试环境GCC版本为TDM-GCC 10.3.0。这里列出的版本号仅供参考，需要根据实际版本限制情况进行调整。
```bash
cd ChatGLM3
pip install -r requirements.txt
```

#### 3.4.2 模型运行测试
接下来，加载模型并使用命令行来测试。由于笔者GPU资源受限，因此加载ChatGML2-6B模型来演示，命令同ChatGML3-6B基本一致。
```bash
python basic_demo/cli_demo.py
```

成功运行后，就可以在类似如图3-20所示的命令行终端与大模型进行交互。


![image](https://github.com/user-attachments/assets/b347aac4-9fa4-4021-b1a0-3dbcc1e6e6fc)


大多数开源大模型的部署Demo会提供一个编写好的Web程序。例如，ChatGLM提供了一个Web界面，可以在加载模型后使用web_demo来进行问答。启动Web服务的指令如下。
```bash
python basic_demo/web_demo_gradio.py
```

在浏览器中输入对应的Web地址，就可以得到类似如图3-21所示的网页。

在ChatGLM中，也可以通过命令启动基于Streamlit的网页版Demo，类似如图3-22所示，效果与Gradio相同，但是交互更加流畅。
```bash
python basic_demo/web_demo_streamlit.py
```

当前多数开源大模型的Web界面会选择Streamlit框架来进行功能和可视化的设计。下面简要介绍Streamlit框架及其使用。

Streamlit框架是一个用于创建数据科学和机器学习应用程序的开源Python库，它能以简单的方式快速构建交互式的在线数据应用。Streamlit基于React构建，组件库十分精细，组件类型也比较多，支持Matplotlib等可视化依赖集成，社区生态也比较好。Streamlit支持热更新，当技术人员改动代码文件时，只需要刷新浏览器页面，即可完成组件热部署。


![image](https://github.com/user-attachments/assets/3930adb6-f3e4-4484-8c78-9d7cfcfb237d)

![image](https://github.com/user-attachments/assets/809fdb55-3e8a-49a8-b866-6da292cd4972)


Streamlit的set_page_config()方法用于来设置页面的标题、图标和布局。注意，该方法只能调用一次，且必须作为第一个调用。其中，page_title是页面标题、page_icon是网页图标，layout是



页面内容布局方式。示例代码如下：
```python
import streamlit as st
# 设置页面的标题、图标和布局，注意该方法只能调用一次，且必须作为第一个调用
st.set_page_config(
    page_title="Streamlit 教学",
    page_icon="",
    layout="wide"
)
```

此外，可以使用markdown()方法在页面上添加Markdown块来支持Markdown语法，使用title()方法添加网页标题，使用header()方法添加一个一级标题，使用subheader()方法添加一个二级标题，使用text()方法添加一个文本块，使用code()方法在页面添加代码块，使用chat_message()方法添加聊天框，使用chat_input()方法添加输入框，使用button按钮在页面实现一个按钮，使用rerun()方法刷新当前页面。

大模型应用可以使用Streamlit框架创建一个用于服务启动的Python脚本文件，并将该应用部署在服务端。

### 3.5 本章小结
本章从大模型的推理优化讲起，之后围绕大模型的并行训练、训练容错、检查点、启动器 - 工作器、混合精度训练等模型训练技术展开讲解。最后，对模型能力评估涉及的评估数据集、评估方法、评测工具，以及模型部署的硬件需求、运行测试与部署实践进行讲解。 
