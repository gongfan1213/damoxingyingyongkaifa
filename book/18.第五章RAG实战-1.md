### 第5章 RAG实践
为了提高大模型的能力，除了通过模型微调来更新模型之外，还可以通过RAG实现更准确的信息检索和文本生成。

#### 5.1 应用概述

大模型在文本生成、文本到图像生成等任务中的表现让人印象深刻。但大模型也存在着局限性，包括产生误导性的幻觉、内容过时、对最新知识的了解有限、处理特定知识时效率不高、缺少对生成文本的可解释性、缺乏对专业领域的深度洞察、存在数据安全以及推理能力欠缺等问题。

为了解决以上这些问题，可以让模型与外部世界互动，以多样化的方式获取知识，从而提升其回答能力。例如，使用LangChain插件，帮助人们快速开发基于大模型的应用。

其实，LangChain的很多功能属于RAG的范畴。

RAG是在不改变大模型本身的前提下，通过利用外部知识信息源，为模型提供特定领域的数据信息，实现对该领域信息的准确检索和生成。有效缓解了幻觉问题，提高了知识更新的速度，并增强了内容生成的可追溯性，解决了大模型落地过程中的定制性、确定性、可解释性等问题，使得大模型在实际应用中变得更加实用和可信。

RAG的效果增强方式（见图5-1），具体如下。

① 解析增强：通过扩展文档元素的解析范围，提升对文档内容的理解。

② 索引增强：通过抽取元数据和解决多跳问题，提高索引的召回率。

③ 检索增强：利用多向量技术和定制的检索策略，优化检索过程。面向行业应用场景，常用的应用技术解决方案包括基于企业知识问答的通用方案、基于长文本应用的扩展方案等。

RAG能有效帮助企业利用大模型快速处理私有数据，特别适用于数据资源基础较好、需要准确引用特定领域知识的企业，如客服问答、内容查询与推荐等场景。

![image](https://github.com/user-attachments/assets/c82e8e3d-78b1-429d-bac1-0dbb97b2a6f4)


RAG主要优点如下。

1）提高模型应用的专业性和准确性，让模型能基于特定数据生成内容、降低幻觉。

2）满足企业自有数据所有权保障的需求，模型只会查找和调用外挂的数据，不会吸收数据并训练成模型内含的知识。

3）具备较高的性价比，无须对底层大模型进行调整，不用投入大量算力等资源进行微调或预训练，能够更快速地开发和部署应用。


#### 5.2 RAG流程


RAG的主要功能是生成信息或解答专业问题，但其过程涉及对现有文档资料的检索，而非完全依赖大模型自主生成结果。RAG通过在语言模型生成答案之前，先从广泛的文档数据库中检索相关信息，然后利用这些信息来引导生成过程，极大地提升了内容的准确性和相关性。

为了更直观地理解RAG的工作机制，我们提供一个简化的示例。

设想一位工程师需要从详尽的《业务操作手册》中提取必要的业务知识以协助其完成任务，他可以采取以下几种方式。

1）传统方法：直接翻阅实体或电子版的《业务操作手册》，详细阅读并理解操作流程。对于复杂的业务流程，可能需要整合手册中多个章节的信息，并进行综合理解。

2）利用问答机器人：向一个预训练好的问答机器人咨询，它会提供相关知识。然而，这种方法可能面临两个问题。一是可能局限于常见问答形式，用户仍需自行整合信息；二是构建和维护这样的机器人需要大量的前期工作与专业知识输入。

3）应用RAG技术：通过将《业务操作手册》的电子版上传至RAG系统，系统能在几分钟内创建该文档的索引。当工程师查询时，RAG不仅会提供相关信息，还会综合手册中的多个相关点，并以专业的口吻给出解答：“为解决此问题，你需要先完成两个前提条件……”。

通过上述介绍，我们了解到RAG的功能及其潜力。值得注意的是，RAG不仅可以替代传统的FAQ（Frequently Asked Questions，常见问题解答）系统，还能作为多种应用的中间件。此外，RAG的应用不限于文本，同样适用于语音、视频和图像等多模态场景，前提是这些内容可以进行嵌入表达。关于这些扩展应用的具体细节，感兴趣的读者可以自行探索。

RAG的能力核心是有效结合了索引、检索和生成。把私有文档数据进行切片，向量化后形成矢量块，之后依据索引，通过向量检索进行召回，再作为上下文结合提示指令输入到通用大模型，大模型再进行分析和回答。具体应用时，RAG首先对用户提供的文档进行切分，再分块后进行向量化处理，生成索引和矢量块，并最终形成向量库。当用户提出一个问题或请求时，用户的问题也会进行向量化处理，形成文本嵌入向量。然后RAG会在向量库中对问题形成的文本嵌入向量进行相似度匹配，并基于检索召回和知识精排技术找到与问题最相关的K段文本向量，并据此检索出文档出处的相关信息，这些信息接着被整合到原始问题中，作为额外的上下文信息和原始问题一起输入到大模型。大模型接到这个增强的提示后，将它与自己内部知识进行综合，最后生成更准确的内容。RAG的检索、增强、生成流程如图5-2所示。

注意，向量化成为RAG提升私有数据调用效率的普遍手段。通过将各种数据统一转化成向量，能高效地处理各类非结构化数据，从而进行相似性搜索，以在大模型数据集中快速找到最相似的向量，特别适合大模型检索和各种数据调用。

![image](https://github.com/user-attachments/assets/7211cfb3-e90b-4963-941c-6ceea1ebe97b)


#### 5.3 环境构建
在项目实践之前，我们需要选择RAG框架，并构建项目开发所依赖的环境。我们以QAnything框架为例进行实践，具体使用方法请参考GitHub地址：https://github.com/netease-youdao/QAnything/blob/qanything-v2/README_zh.md。

RAG包括知识解析、检索、增强和生成等步骤。我们采用的Python版本是3.10，涉及CV和NLP相关的算法依赖包，可以通过Anaconda进行安装。安装相关依赖的requirements.txt文件如下：

```
modelscope==1.13.0
Pillow==10.2.0
numpy==1.24.3
PyMuPDF==1.24.4
opencv-python-headless==4.9.0.80
torch==2.1.2
torchvision==0.16.2
transformers==4.36.2
openai==1.12.0
concurrent-log-handler==0.9.25
sentencepiece==0.1.99
tiktoken==0.6.0
sanic==23.6.0
sanic_ext==23.6.0
faiss-cpu==1.8.0
openpyxl==3.1.2
langchain==0.1.9
pypinyin==0.50.0
python-docx==1.1.0
unstructured==0.12.4
unstructured[pdf]
unstructured[md]
networkx==3.2.1
faster-whisper==1.0.1
python-dotenv==1.0.1
duckduckgo-search==5.3.0b4
html2text==2024.2.26
mistune==3.0.2
xgboost==2.0.3
pdfplumber==0.11.0
PyPDF2==3.0.1
markdownify==0.12.1
datrie==0.8.2
hanziconv==0.3.2
shapely==2.0.4
pyclipper==1.3.0.post5
```
我们需要在Anaconda中创建一个新的虚拟环境，进入新建好的项目路径，输入pip install -r requirements.txt命令。接着，在终端输入pip list命令，查看requirements.txt中的依赖全都正确安装即可。

#### 5.4 应用实践
安装完成RAG所需要的依赖环境，进一步使用QAnything框架建立工程文件，并进行代码编写及实践，代码保存在QAnything_demo文件夹。

##### 5.4.1 知识解析

在RAG系统的构建过程中，对各类文档进行加载并提取文本字符串，从而进行分块处理，该处理过程称为知识解析。知识解析起着至关重要的作用。通过将非结构化数据转换为更易于处理的数据格式，不仅显著提升了信息检索的效率，而且极大增强了生成答案的准确性。

![image](https://github.com/user-attachments/assets/67640ffe-0fba-4cd3-843b-9549f7100f7a)


作为信息承载工具，文档的不同布局代表了各种不同的信息。知识解析是一个从文档中阅读、解析和提取信息的自动化过程。开源组件LangChain和Llama-Index提供的DocumentLoader模块支持多种文件类型的知识解析，满足绝大多数文档格式的需求。

知识解析模块首先需要加载需要检索的文档，文档格式可以是URL网页、Markdown格式的md文件、TXT文本或者Word文档等。下面以Word文档为例来讲解知识解析。新建示例文档“火火兔.docx”，目录组织结构可以参考图5-3，具体内容可在网上获取。

假定火火兔的描述如下：“火火兔是一个乐观开朗，充满童真和想象力，爱笑的小机灵鬼。他自信、胆子大、爱分享、善于思考，是‘点子王’，遇到困难总是第一时间想解决办法，想到办法时，他会灵机一动喊出口号‘火火兔有办法！’。他思路清晰，号召力强，朋友们都非常喜欢他，他也总能替大家解决各种麻烦。”有关火火兔奶奶的描述如下：“火火兔的奶奶是一位温柔的老教师，有一点点孩子气。她喜欢无条件地夸奖孩子，并且对自己的厨艺很有自信。”

接下来对文档进行知识解析，即将不同类型的文件或URL内容分割成文档块。根据文档后缀名进行文档类型判断，并采用不同的解析方式，示例的Word文档可以通过text_splitter.split_documents函数解析，并存储在数组docs中，供后续调用。具体代码如下：
```python
def get_time
def split_file_to_docs(self, ocr_engine: Callable, sentence_size=SENTENCE_SIZE, using_zh_title_enhance=ZH_TITLE_ENHANCE):
    if self.url:
        debug_logger.info("load url:{}".format(self.url))
        loader = MyRecursiveUrlLoader(url=self.url)
        textsplitter = ChineseTextSplitter(pdf=False, sentence_size=sentence_size)
        docs = loader.load_and_split(text_splitter)
    elif self.file_path == 'FAQ':
        docs = [Document(page_content=self.file_content['question'],metadata={'faq_dict': {}})]
    elif self.file_path.lower().endswith(".md"):
        loader = UnstructuredFileLoader(self.file_path)
        docs = loader.load()
    elif self.file_path.lower().endswith(".txt"):
        loader = TextLoader(self.file_path, autodetect_encoding=True)
        texts_splitter = ChineseTextSplitter(pdf=False, sentence_size=sentence_size)
        docs = loader.load_and_split(texts_splitter)
    elif self.file_path.lower().endswith(".pdf"):
        if USE_FAST_PDF_PARSER:
            loader = UnstructuredPaddlePDFLoader(self.file_path, ocr_engine, self.use_cpu)
            texts_splitter = ChineseTextSplitter(pdf=True, sentence_size=sentence_size)
            docs = loader.load_and_split(texts_splitter)
        else:
            try:
                from qanything_kernel.utils.loader.self_pdf_loader import PdfLoader
                loader = PdfLoader(filename=self.file_path, save_dir=os.path.dirname(self.file_path))
                markdown_dir = loader.load_to_markdown()
                docs = convert_markdown_to_langchaindoc(markdown_dir)
                docs = self.pdf_process(docs)
            except Exception as e:
                debug_logger.warning(f'Error in Powerful PDF parsing: {e}, use fast PDF parser instead.')
                loader = UnstructuredPaddlePDFLoader(self.file_path, ocr_engine, self.use_cpu)
                texts_splitter = ChineseTextSplitter(pdf=True, sentence_size=sentence_size)
                docs = loader.load_and_split(texts_splitter)
    elif self.file_path.lower().endswith((".jpg") or self.file_path.lower().endswith((".jpeg") or self.file_path.lower().endswith((".png"):
        loader = UnstructuredPaddleImageLoader(self.file_path, ocr_engine, self.use_cpu)
        texts_splitter = ChineseTextSplitter(pdf=False, sentence_size=sentence_size)
        docs = loader.load_and_split(texts_splitter)
    elif self.file_path.lower().endswith(".docx"):
        loader = UnstructuredWordDocumentLoader(self.file_path)
        texts_splitter = ChineseTextSplitter(pdf=False, sentence_size=sentence_size)
        docs = loader.load_and_split(texts_splitter)
    elif self.file_path.lower().endswith(".xlsx"):
        # loader = UnstructuredExcelLoader(self.file_path, mode='elements')
        csv_file = self.file_path[:-5] + '.csv'
        xlsx_to_pd.read_excel(self.file_path, engine='openpyxl')
        loader = CSVLoader(csv_file, index=False)
        docs = loader.load()
    elif self.file_path.lower().endswith(".pptx"):
        loader = UnstructuredPowerPointLoader(self.file_path)
        docs = loader.load()
    elif self.file_path.lower().endswith(".eml"):
        loader = UnstructuredEmailLoader(self.file_path)
        docs = loader.load()
    elif self.file_path.lower().endswith(".csv"):
        loader = CSVLoader(self.file_path, csv_args={"delimiter": ",", "quotechar": "'"})
        docs = loader.load()
    elif self.file_path.lower().endswith(".mp3") or self.file_path.lower().endswith(".wav"):
        loader = UnstructuredPaddleAudioLoader(self.file_path, self.use_cpu)
        docs = loader.load()
    else:
        debug_logger.info("file_path: {}".format(self.file_path))
        raise TypeError('文件类型不支持，目前仅支持: [md,txt,pdf,jpg,png,jpeg,docx,xlsx,pptx,eml,csv]')
    if using_zh_title_enhance:
        debug_logger.info("using_zh_title_enhance %s", using_zh_title_enhance)
        docs = zh_title_enhance(docs)
    print('docs number:', len(docs))
    print(docs)
    # 不是.csv、.xlsx和FAQ的文件，需要再次分割
    if not self.file_path.lower().endswith(".csv") and not self.file_path.lower().endswith(".xlsx") and not self.file_path == 'FAQ':
        new_docs = []
        min_length = 200
        for doc in docs:
            if not new_docs:
                new_docs.append(doc)
            else:
                last_doc = new_docs[-1]
                if num_tokens(last_doc.page_content) + num_tokens(doc.page_content) < min_length:
                    last_doc.page_content += '\n' + doc.page_content
                else:
                    new_docs.append(doc)
        debug_logger.info(f'before 2nd split doc lens: {len(new_docs)}')
        if self.file_path.lower().endswith(".pdf"):
            if USE_FAST_PDF_PARSER:
                docs = pdf_text_splitter.split_documents(new_docs)
            else:
                docs = new_docs
        else:
            # 将Word文档解析成多段文本，放在docs数组中
            docs = text_splitter.split_documents(new_docs)
            debug_logger.info(f'after 2nd split doc lens: {len(docs)}')
        # 这里给每个文档片段的metadata注入file_id
        new_docs = []
        for idx, doc in enumerate(docs):
            page_content = re.sub(r'[\n\t]+', '\n', doc.page_content).strip()
            new_doc = Document(page_content=page_content)
            new_doc.metadata["user_id"] = self.user_id
            new_doc.metadata["kb_id"] = self.kb_id
            new_doc.metadata["file_id"] = self.file_id
            new_doc.metadata["file_name"] = self.url if self.url else self.file_name
            new_doc.metadata["chunk_id"] = idx
            new_doc.metadata["file_path"] = self.file_path
            if 'faq_dict' not in doc.metadata:
                new_doc.metadata['faq_dict'] = {}
            else:
                new_doc.metadata['faq_dict'] = doc.metadata['faq_dict']
            new_docs.append(new_doc)
        if new_docs:
            debug_logger.info('Analysis content head: %s', new_docs[0].page_content[:100])
        else:
            debug_logger.info('Analysis docs is empty!')
        self.docs = new_docs
```
执行知识解析代码解析文档。通过文档上传页面进行文档上传并进行文档的知识解析，文档的知识解析执行过程输出的信息如图5-4所示。

![image](https://github.com/user-attachments/assets/5b31d826-0e3a-4a15-80de-1378468252a2)


知识解析的过程也包括文档切分，即将大文档分解成较小的片段。切分的目的是确保在内容向量化时尽可能减少噪声，同时保持语义的相关性。使用大模型进行内容嵌入有助于优化从向量数据库中检索出的内容的相关性。

##### 5.4.2 检索
RAG需要在海量文档集合中快速、准确地检索出与查询相关的文档，这依赖于词嵌入模型将离散变量映射到连续向量空间。涉及检索召回和知识精排两种技术。检索召回是指在一个文档的集合中，找出与用户查询相关子集的过程。文档解析将文档切分为一个个语义块之后，我们将每个语义块进行向量转换，将文本块转换为词嵌入向量。
具体代码如下：
```python
class EmbeddingBackend(Embeddings):
    embed_version = "local_v0.0.1_20230525_6d4019f1559ae
