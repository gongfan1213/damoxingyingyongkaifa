### 5.4.2 检索
RAG需要在海量文档集合中快速、准确地检索出与查询相关的文档，这依赖于词嵌入模型将离散变量映射到连续向量空间。涉及检索召回和知识精排两种技术。检索召回是指在一个文档的集合中，找出与用户查询相关子集的过程。文档解析将文档切分为一个个语义块之后，我们将每个语义块进行向量转换，将文本块转换为词嵌入向量。
具体代码如下：
```python
class EmbeddingBackend(Embeddings):
    embed_version = "local_v0.0.1_20230525_6d4019f1559ae84abc2ab8257e1ad4c"
    def __init__(self, use_cpu):
        self.use_cpu = use_cpu
        self._tokenizer = AutoTokenizer.from_pretrained(LOCAL_EMBED_PATH)
        self.workers = LOCAL_EMBED_WORKERS
    # 抽象方法，获取句子的向量表示
    @abstractmethod
    def get_embedding(self, sentences, max_length) -> List:
        pass
    # 这个方法用于获取给定长度文本列表的向量
    @get_time
    def get_len_safe_embeddings(self, texts: List[str]) -> List[List[float]]:
        all_embeddings = []
        batch_size = LOCAL_EMBED_BATCH
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.workers) as executor:
            futures = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                future = executor.submit(self.get_embedding, batch, LOCAL_EMBED_MAX_LENGTH)
                futures.append(future)
            debug_logger.info(f'embedding number: {len(futures)}')
            for future in tqdm(futures):
                embeddings = future.result()
                all_embeddings += embeddings
        return all_embeddings
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs using multithreading, maintaining the original order."""
        return self.get_len_safe_embeddings(texts)
    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        return self.embed_documents([text])[0]
    @property
    def getModelVersion(self):
        return self.embed_version
```
在控制台执行代码，可以得到词嵌入向量代码运行的日志输出结果，如图5-5所示。


![image](https://github.com/user-attachments/assets/d9a13ab1-4d52-42dd-bbc7-2ef93565a425)


完成检索召回，接下可以对检索出来的文档进行排序，获取更精准的结果。

在初始检索阶段，系统根据某种标准（如相似度）返回一组文档。然而，由于初始排序可能并不总是能够准确反映文档与查询的真实相关性，因此需要进行知识精排（Reranker）来提升检索结果的质量。由于精排能够在单路或多路的召回结果中挑选出和问题最接近的文档，将精排整合到RAG应用中可以显著提高生成答案的精确度。

将文档进行解析与向量编码之后，当对用户的问题（Query）进行解答时会通过词嵌入的向量相似度检索相关字段，并对检索后的内容进行精排。精排的相关代码如下：

```python
class RerankBackend(ABC):
    def __init__(self, use_cpu):
        self.use_cpu = use_cpu
        self._tokenizer = AutoTokenizer.from_pretrained(LOCAL_RERANK_PATH)
        self.spe_id = self._tokenizer.sep_token_id
        self.overlap_tokens = 80
        self.batch_size = LOCAL_RERANK_BATCH
        self.max_length = LOCAL_RERANK_MAX_LENGTH
        self.return_tensors = None
        self.workers = LOCAL_RERANK_WORKERS
    @abstractmethod
    def inference(self, batch) -> List:
        pass
    def merge_inputs(self, chunk1_raw, chunk2):
        chunk1 = deepcopy(chunk1_raw)
        chunk1['input_ids'].extend(chunk2['input_ids'])
        chunk1['input_ids'].append(self.spe_id)
        chunk1['attention_mask'].extend(chunk2['attention_mask'])
        if 'token_type_ids' in chunk1:
            token_type_ids = [1 for _ in range(len(chunk2['token_type_ids']) + 1)]
            chunk1['token_type_ids'].extend(token_type_ids)
        return chunk1
    def tokenize_preproc(self,
                         query: str,
                         passages: List[str],
                         ):
        query_inputs = self._tokenizer.encode_plus(query, truncation=False, padding=False)
        max_passage_inputs_length = self.max_length - len(query_inputs['input_ids']) - 1
        assert max_passage_inputs_length > 10
        overlap_tokens = min(self.overlap_tokens, max_passage_inputs_length * 2 // 7)
        # 组成 [query, passage] 对
        merge_inputs = []
        merge_inputs_idxs = []
        for pid, passage in enumerate(passages):
            passage_inputs = self._tokenizer.encode_plus(passage, truncation=False, padding=False, add_special_tokens=False)
            passage_inputs_length = len(passage_inputs['input_ids'])
            if passage_inputs_length <= max_passage_inputs_length:
                if passage_inputs['attention_mask'] is None or len(passage_inputs['attention_mask']) == 0:
                    continue
                qp_merge_inputs = self.merge_inputs(query_inputs, passage_inputs)
                merge_inputs.append(qp_merge_inputs)
                merge_inputs_idxs.append(pid)
            else:
                start_id = 0
                while start_id < passage_inputs_length:
                    end_id = start_id + max_passage_inputs_length
                    sub_passage_inputs = {k: v[start_id:end_id] for k, v in passage_inputs.items()}
                    start_id = end_id - overlap_tokens if end_id < passage_inputs_length else end_id
                    qp_merge_inputs = self.merge_inputs(query_inputs, sub_passage_inputs)
                    merge_inputs.append(qp_merge_inputs)
                    merge_inputs_idxs.append(pid)
        return merge_inputs, merge_inputs_idxs
    @get_time
    def get_rerank(self, query: str, passages: List[str]):
        tot_batches, merge_inputs_idxs_sort = self.tokenize_preproc(query, passages)
        tot_scores = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.workers) as executor:
            futures = []
            for k in range(0, len(tot_batches), self.batch_size):
                batch = self._tokenizer.pad(
                    tot_batches[k:k + self.batch_size],
                    padding=True,
                    max_length=None,
                    pad_to_multiple_of=None,
                    return_tensors=self.return_tensors
                )
                future = executor.submit(self.inference, batch)
                futures.append(future)
            debug_logger.info(f'rerank number: {len(futures)}')
            for future in futures:
                scores = future.result()
                tot_scores.extend(scores)
        merge_tot_scores = [0 for _ in range(len(passages))]
        for pid, score in zip(merge_inputs_idxs_sort, tot_scores):
            merge_tot_scores[pid] = max(merge_tot_scores[pid], score)
        # print("merge_tot_scores:", merge_tot_scores, flush=True)
        return merge_tot_scores
```
知识精排完成之后，运行相关检索程序，执行结果如图5-6所示。


![image](https://github.com/user-attachments/assets/79fc01ed-693e-468b-afa8-10e920433621)


接下来以检索得到的结果（上下文知识）为条件，通过大模型进行信息的归纳生成，生成用户问题的回答。

### 5.4.3 增强
做完语义解析、词嵌入、精排模型工作之后，我们需要进一步设置后端大模型服务，RAG可以对接不同的大模型服务。我们以OpenAI大模型的参数设置为例，将检索得到的文档信息作为上下文和用户的问题或请求一起输送给大模型，相关代码配置如下：
```python
class OpenAILlamaBaseAnswer(ABC):
    model: str = None
    token_window: int = None
    max_tokens: int = config.llm_config['max_token']
    offset_token: int = 50
    truncate_len: int = 50
    temperature: float = 0
    top_p: float = config.llm_config['top_p']  # top_p 须为 (0,1)
    stop_words: str = None
    history: List[List[str]] = []
    history_len: int = config.llm_config['history_len']
    def __init__(self, args):
        super().__init__()
        # 下面是OpenAI相关大模型的参数设置，需要在OpenAI网站提前获取
        base_url = args.openai_api_base
        api_key = args.openai_api_key
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = args.openai_api_model_name
        self.token_window = int(args.openai_api_context_length)
        debug_logger.info(f"OPENAI API_KEY = {api_key}")
        debug_logger.info(f"OPENAI API_BASE = {base_url}")
        debug_logger.info(f"OPENAI API_MODEL = {self.model}")
        debug_logger.info(f"OPENAI API_CONTEXT_LENGTH = {self.token_window}")
    @property
    def _llm_type(self) -> str:
        return "using OpenAI API serve as LLM backend"
    @property
    def _history_len(self) -> int:
        return self.history_len
    def set_history_len(self, history_len: int = 10) -> None:
        self.history_len = history_len
    # 定义函数 num_tokens_from_messages，该函数返回输入及输出消息所使用的Token数
    def num_tokens_from_messages(self, messages, model=None):
        """参考https://github.com/DjangoPeng/openai-quickstart/blob/main/openai_api/count_tokens_with_tiktoken.ipynb，计算返回消息列表使用的Token数"""
        # debug_logger.info(f"[debug] num_tokens_from_messages<model, {self.model}> = {model, self.model}")
        if model is None:
            model = self.model
        # 尝试获取模型的编码
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # 如果模型没有找到，使用 cl100k_base 编码并给出警告
            debug_logger.info(f"Warning: {model} not found. Using cl100k_base encoding.")
            encoding = tiktoken.get_encoding("cl100k_base")
        # 针对不同的模型设置Token数量
        if model in {
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-16k-0613",
            "gpt-4-0314",
            "gpt-4-32k-0314",
            "gpt-4-0613",
            "gpt-4-32k-0613",
            "gpt-4-32k",
            "gpt-4-1106-preview",
        }:
            tokens_per_message = 3
            tokens_per_name = 1
        elif model == "gpt-3.5-turbo-0301":
            tokens_per_message = 4  # 每条消息遵循 {role/name}\n{content}\n 格式
            tokens_per_name = -1  # 如果有名字，角色会被省略
        elif "gpt-3.5-turbo" in model:
            # gpt-3.5-turbo 模型可能会有更新，假设此处返回的为gpt-3.5-turbo-0613的Token数量，并给出警告
            debug_logger.info('Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.')
            return self.num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613")
        elif "gpt-4" in model:
            # GPT-4模型可能会有更新，假设此处返回的为gpt-4-0613的Token数量，并给出警告
            debug_logger.info("Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.")
            return self.num_tokens_from_messages(messages, model="gpt-4-0613")
        else:
            # 其他模型可能会有更新，假设此处返回的为gpt-3.5-turbo-1106的Token数量，并给出警告
            debug_logger.info(f"Warning: {model} may update over time. Returning num tokens assuming gpt-3.5-turbo-1106.")
            return self.num_tokens_from_messages(messages, model="gpt-3.5-turbo-1106")
        num_tokens = 0
        # 计算每条消息的Token数
        for message in messages:
            if isinstance(message, dict):
                num_tokens += tokens_per_message
                for key, value in message.items():
                    num_tokens += len(encoding.encode(value))
                    if key == "name":
                        num_tokens += tokens_per_name
            elif isinstance(message, str):
                num_tokens += len(encoding.encode(message))
            else:
                NotImplementedError(
                    f"""num_tokens_from_messages() is not implemented for message type {type(message)}."""
                )
        num_tokens += 3  # 每条回复Token数以3为例进行计算
        return num_tokens
    def num_tokens_from_docs(self, docs):
        # 尝试获取模型的编码
        try:
            encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            # 如果模型没有找到，使用 cl100k_base 编码并给出警告
            debug_logger.info("Warning: model not found. Using cl100k_base encoding.")
            encoding = tiktoken.get_encoding("cl100k_base")
        num_tokens = 0
        for doc in docs:
            num_tokens += len(encoding.encode(doc.page_content, disallowed_special=()))
        return num_tokens
    async def _call(self, prompt: str, history: List[List[str]], streaming: bool=False) -> str:
        messages = []
        for pair in history:
            question, answer = pair
            messages.append({"role": "user", "content": question})
            messages.append({"role": "assistant", "content": answer})
        messages.append({"role": "user", "content": prompt})
        debug_logger.info(messages)
        try:
            if streaming:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stream=True,
                    max_tokens=self.max_token,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    stop=[self.stop_words] if self.stop_words is not None else None,
                )
                debug_logger.info(f"OPENAI RES: {response}")
                for event in response:
                    if not isinstance(event, dict):
                        event = event.model_dump()
                    if isinstance(event['choices'], List) and len(event['choices']) > 0:
                        event_text = event['choices'][0]['delta']['content'] if 'delta' in event['choices'][0] and 'content' in event['choices'][0]['delta'] else ""
                        if isinstance(event_text, str) and event_text != "":
                            debug_logger.info(f"[debug] event_text = [{event_text}]")
                            delta = {'answer': event_text}
                            yield "data: " + json.dumps(delta, ensure_ascii=False)
            else:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stream=False,
                    max_tokens=self.max_token,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    stop=[self.stop_words] if self.stop_words is not None else None,
                )
                debug_logger.info(f"[debug] response.choices = [{response.choices}]")
                event_text = response.choices[0].message.content if response.choices else ""
                delta = {'answer': event_text}
                yield "data: " + json.dumps(delta, ensure_ascii=False)
        except Exception as e:
            debug_logger.info(f"Error calling OpenAI API: {e}")
            delta = {'answer': f"{e}"}
            yield "data: " + json.dumps(delta, ensure_ascii=False)
        finally:
            # debug_logger.info("[debug] try-finally")
            yield f"data: [DONE]\n\n"
    async def generatorAnswer(self, prompt: str,
                              history: List[List[str]] = [],
                              streaming: bool = False) -> AnswerResult:
        if history is None or len(history) == 0:
            history = [[]]
        debug_logger.info(f"history_len: {self.history_len}")
        debug_logger.info(f"prompt: {prompt}")
        debug_logger.info(f"prompt tokens: {self.num_tokens_from_messages([({'content': prompt})])}")
        debug_logger.info(f"streaming: {streaming}")
        response = self._call(prompt, history[:-1], streaming)
        complete_answer = ""
        async
```

```python
        for response_text in response:
            if response_text:
                chunk_str = response_text[6:]
                if not chunk_str.startswith("[DONE]"):
                    chunk_js = json.loads(chunk_str)
                    complete_answer += chunk_js["answer"]
        history[-1] = [prompt, complete_answer]
        answer_result = AnswerResult()
        answer_result.history = history
        answer_result.llm_output = {"answer": response_text}
        answer_result.prompt = prompt
        yield answer_result
```
完成大模型设置之后即可启动RAG服务，以在Windows操作系统下使用CPU为例，通过bash scripts/run_for_openai_api_with_cpu_in_Linux_or_WSL.sh脚本启动服务，并输入问题“火火兔的奶奶是谁?”，可以得到检索结果如图5-7所示，包含与问题相关的question_tokens、prompt_tokens等字段信息。

### 5.4.4 生成
基于检索结果，通过大模型进行信息归纳生成，最终输出大模型生成的回复，如图5-8所示。

这里利用网上可以获取的“火火兔”故事作为私有数据进行检索，并输送给大模型进行增强生成。实际上，读者可以利用外部知识信源，也可以准备自己的私有数据，并使用RAG进行检索。


![image](https://github.com/user-attachments/assets/f199013d-ff79-4c7c-b5f4-0adc5ecefabd)



![image](https://github.com/user-attachments/assets/88476ff5-4a50-41dc-b737-2b008452f21d)


### 5.5 本章小结
本章首先对RAG的应用情况进行概述，接着，通过QAnything框架建立RAG工程文件，将知识解析、检索、增强、生成等的整体流程进行串联，实现一个简单的RAG检索问答系统。 
