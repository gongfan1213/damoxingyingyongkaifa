### 第6章 智能客服问答实践
客服行业具有客户群体大、咨询频次高、问题重复度高等显著特征。借助大模型在NLU、多轮对话、多模态等方面的优势，构建智能客服问答系统，快速生成回复内容，提供高度自然和流畅的对话体验，从而提升客户服务的效率和质量。

#### 6.1 应用概述

智能客服是在大规模知识处理的基础上发展起来的一项面向行业应用的技术手段。智能客服系统集合了大规模知识处理、NLU、知识图谱、智能问答和推理等技术，不仅能够为企业提供细粒度知识管理及精细化管理所需的统计分析信息，还能够为企业与用户之间的良好沟通建立桥梁。

智能客服在实际应用中可以带来以下好处。

1）简易问题解答。在客服咨询场景中，绝大多数问题都是简单且重复的。例如，“我收到的商品有问题应该怎么处理？”“我腰疼该挂哪个科室的号？”“明天从北京到合肥的高铁还有余票吗？”等。我们将这些简易问题交给智能客服处理，使人工客服专注于较为复杂的问题，将会极大地减少人力成本、提高服务客户的效率。

2）高峰时段解答。在高峰时段，人工客服面临着大量用户的咨询，无法同时处理，这导致用户需要进行排队等待。智能客服可以通过自助服务的方式避免用户长时间的等待，显著提升用户体验，减轻人工客服的负担。

3）个性化推荐。智能客服可以充分利用用户的个人信息，为用户提供定制化的服务。通过分析用户的购买历史、偏好和兴趣，智能客服可以主动推荐相关的产品或服务，以满足用户的特定需求。例如，如果用户经常购买健身器材，智能客服可以向他推荐最新的健身器材或提供健身训练建议。这种个性化的推荐不仅能够提高用户的满意度，还能够增加销售机会和提高用户的忠诚度。

智能客服可以应用在各行各业。在电商行业中，智能客服可用于订单查询、商品推荐、支付支持等。它能帮助用户快速找到所需商品，解决购物过程中的问题，提升购物体验。在医疗行业中，智能客服可用于医疗咨询、预约挂号、病症辨识等服务。它能为患者提供迅速、准确的医疗信息，减轻医院排队等待的压力。在政务行业，智能客服可用于政策咨询、办事指南、在线申报等服务。它能帮助公民迅速获取相关信息，提升政务服务的效率和透明度。

#### 6.2 环境构建
在开始项目实践之前，需要构建项目开发所依赖的环境，并进行开源模型下载。我们可以从Hugging Face和ModelScope下载智谱AI基座大模型ChatGLM3。

##### 6.2.1 开发环境搭建

本项目使用智谱AI开源的第三代基座大模型ChatGLM3作为基础大模型，构建项目所需环境。

访问ChatGLM3的GitHub页面，单击Code按钮，并选择拉取方法，如图6 - 1所示。

![image](https://github.com/user-attachments/assets/5bd1ee56-8b4d-46d2-b821-cebdfb9ed506)



1）复制项目地址，在Shell终端输入“git clone <项目地址>（例如git clone https://github.com/THUDM/ChatGLM3.git）”，自动拉取仓库。

2）单击Download ZIP，下载项目文件的压缩包。



项目下载完成后，在Anaconda中创建一个新的虚拟环境，进入已下载的项目路径中，输入pip install -r requirements.txt。接着，在终端输入pip list，确认requirements.txt中的依赖全都正确安装即可。

##### 6.2.2 开源模型下载
完成依赖环境安装，接下来下载开源大模型ChatGLM3。如表6 - 1所示，ChatGLM3包含3个版本，可根据实际需求选取不同的版本，本项目使用的是ChatGLM3 - 6B版本。

|模型|输入长度限制|特点|
| ---- | ---- | ---- |
|ChatGLM3 - 6B|8K|设计了对话提示，支持工具调用、代码执行和Agent任务执行等复杂场景|
|ChatGLM3 - 6B - Base|8K|基座模型|
|ChatGLM3 - 6B - 32K|32K|支持更长的输入|


登录https://huggingface.co/，并搜索chatglm3，并选择ChatGLM3 - 6B对应的版本，如图6 - 2所示。

![image](https://github.com/user-attachments/assets/aeea82f5-a0c0-40bf-9ecc-a7d686c88968)


Hugging Face包含页面直接下载、使用SDK下载和使用Git下载3种方法。

1）页面直接下载：单击Files and versions标签页可以看到仓库中包含的文件，每个文件旁都有下载按钮，分别单击即可。

2）使用SDK下载：撰写代码，如图6 - 3所示。注意，该方法一般适合下载单个文件。

```python
# HuggingFace SDK下载方法
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).quantize(4).cuda()
```
3）使用Git下载：在终端输入git lfs install，安装git - lfs，接着输入git clone https://huggingface.co/THUDM/chatglm3 - 6b下载大模型文件。

![image](https://github.com/user-attachments/assets/0e078ac1-4705-4d5f-b6dc-f08dc41bc079)


ModelScope提供了使用SDK下载、使用Git下载2种方法。


1）使用SDK下载：编写代码，使用snapshot_download方法实现大模型文件下载。
```python
# Modelscope SDK下载方法
from modelscope import snapshot_download
model_dir = snapshot_download('ZhipuAI/chatglm3-6b')
```
2）使用Git下载：在终端输入git clone https://www.modelscope.cn/ZhipuAI/chatglm3 - 6b.git实现大模型文件下载。

#### 6.3 应用开发
智能客服系统的核心功能是多轮对话，接下来利用ChatGLM3构建一个简单的多轮对话系统，实现多轮对话功能。

##### 6.3.1 实现多轮对话系统
具体步骤如下。

1）加载大模型和分词器。相关代码存放在loadModel.py，该代码可实现大模型和分词器的加载。其中，代码中的大模型路径可以换成模型本地存放的位置。注意，如果显卡资源不足，需要指定量化加载，即在使用from_pretrained()方法加载预训练模型的时候，需加上.quantize(4)函数以进行量化加载。
```python
import os
import platform
import signal
from transformers import AutoTokenizer, AutoModel
import readline

model_path=''
# 加载分词器和大模型
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).cuda()
# 如果有多显卡支持，可使用下面两行代替上面一行，将num_gpus改为实际的显卡数量
# from utils import load_model_on_gpus
# model = load_model_on_gpus("THUDM/chatglm3-6b", num_gpus=2)
# 将模型设置为评估模式后，可使用大模型进行预测或测试
model = model.eval()
```


2）使用提示实现多轮对话。使用build_prompt()方法构建输入的提示，将历史对话信息和最新的对话查询组合一起传给大模型，让大模型利用之前的对话内容，实现多轮对话。
```python
# 根据操作系统的类型，设置清除历史记录的命令标志
os_name = platform.system()
clear_command = 'cls' if os_name == 'Windows' else 'clear'
stop_stream = False
# 欢迎消息的提示文本
welcome_prompt = "欢迎ChatGLM-6B模型，输入内容即可进行对话（clear清空对话历史；stop终止程序）"
# 根据对话历史构建提示文本
def build_prompt(history):
    prompt = welcome_prompt
    for query, response in history:
        prompt += f'\n\n用户: {query}'
        prompt += f'\n\n大模型: {response}'
    return prompt
# 信号处理函数，用于处理程序终止
def signal_handler(signal, frame):
    global stop_stream
    stop_stream = True
```



3）实现main()方法。首先获取用户输入，如果输入是“clear”就清空历史记录，如果是“stop”则退出系统。如果是其他内容，则交由大模型处理。使用model.stream_chat()模型推理方法，其中需要传入分词器、用户查询、对话历史信息。为了节省推理时间，可以传入past_key_values避免历史信息的重复计算。注意，past_key_values参数需要设定return_past_key_values=True，即要求模型返回时才可以获得传入的对话历史信息。
```python
def main():
    past_key_values, history = None, []
    global stop_stream
    print(welcome_prompt)
    while True:
        # 获取用户输入
        query = input("\n用户: ")
        # 检查终止命令或清除历史记录命令
        if query.strip() == "stop":
            break
        if query.strip() == "clear":
            past_key_values, history = None, []
            os.system(clear_command)
            print(welcome_prompt)
            continue
        print("\n大模型: ", end="")
        current_length = 0
        # response: 大模型生成的回复
        # history: 大历史对话
        # past_key_values: 大记录每个时同步的键和值，避免重复计算
        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history,
                                                                    past_key_values=past_key_values,
                                                                    return_past_key_values=True):
            if stop_stream:
                stop_stream = False
                break
            else:
                print(response[current_length:], end="", flush=True)
                current_length = len(response)
        print("")

if __name__ == "__main__":
    main()
```
实现多轮对话问答系统的具体代码存放在clientDemo.py，运行clientDemo.py，输出结果如图6 - 4所示。

![image](https://github.com/user-attachments/assets/7f7ce2ca-0c59-4c00-b03e-f74c4064d833)


此时，我们利用ChatGLM3构建了一个简单的多轮对话系统。接下来为实现更好的交互效果，我们需要针对提示进行重点优化。



##### 6.3.2 提示优化

提示最初是NLP技术人员为特定领域任务设计出来的一种输入形式或模板，在ChatGPT引发大模型广泛关注之后，提示成为用户与大模型交互输入的代表。

编写提示指令最重要的一点是清晰、具体。编写者要明确自己的需求，在提示中不应该存在歧义。值得注意的是，很多人在写提示时将“清晰、具体”误解成短小精悍，事实上，提示的质量并不和其长度挂钩。有时候，较长的提示会提供丰富的上下文，帮助大模型去理解需求，增强大模型的语义理解能力。下面我们利用提示设计技巧来优化大模型的回答。

1. **预防提示注入**

提示注入（Prompt Rejection）是指用户输入的文本可能包含和预设提示相冲突的内容，如果不加分隔，这些输入就可能“注入”并操纵大模型，导致大模型产生毫无关联的输出。简单来说，提示实质是一段由提示模板加查询指令组成的文本，我们可以用特殊符号将查询与注入分隔，避免混淆。这些特殊符号可以是```、"""、<>、<tag> </tag>等。以下是一段代码示例：

```python
from zhipuai import ZhipuAI
# 请根据大模型的要求，自行申请并替换API
client = ZhipuAI(api_key="afl15fb9e272d9910419920b9e116352.VWKlKocMDKbFEakl")
text='介绍一下华为watch GT4'
prompt=f"""
现在你要扮演一位客服，来回答用户提出的问题。\
你回答的语气要保持谦逊、礼貌。\
你的回答要根据提供的规则来回答，不要回答无关内容，尽量保持简洁。\
如果你无法回答用户提出的问题，你要回复“无法回答该问题，请联系人工客服”。

规则:
商品名称: 华为watch GT4 \
表盘大小: 46mm、41mm \
商品规格:
46mm包含三种: \
1. 颜色: 云杉绿，表带: 立体编织复合表带，价格: 2088，是否有货: 无 \
2. 颜色: 山茶棕，表带: 经典皮质表带，价格: 1788，是否有货: 有 \
3. 颜色: 曜石黑，表带: 氟橡胶表带，价格: 1588，是否有货: 有 \
41mm包含三种: \
1. 颜色: 幻夜黑，表带: 氟橡胶表带，价格: 1488，是否有货: 有 \
2. 颜色: 凝霜白，表带: 细腻皮质表带，价格: 1688，是否有货: 有 \
3. 颜色: 皓月银，表带: 间金工艺表带，价格: 2688，是否有货: 有 \

你要回答用三个反引号括起来的问题: ```{text}```
"""
response = client.chat.completions.create(
    model="glm-4-0520",
    messages=[
        {
            "role": "system",
            "content": "你是一个乐于解答各种问题的助手，你的任务是为用户提供专业、准确、有见地的建议。"
        },
        {
            "role": "user",
            "content": prompt
        }
    ],
    top_p=0.7,
    temperature=0.95,
    max_tokens=4095,
    stream=True,
)
answer=str()
for trunk in response:
    answer+=trunk.choices[0].delta.content
print(answer)
```
预防注入输出示例如图6 - 5所示。

![image](https://github.com/user-attachments/assets/b784d304-b4ab-41a3-8fbf-414fb2fcb2d0)


通过输出，我们可以看出大模型按照规则输出了“华为watch GT4”的款式信息。

2. **结构化输出**

结构化输出就是按照某种格式组织（例如JSON、HTML等）并输出的内容。大模型的输出一般是连续的文本，然而在一些特殊的任务中，需要大模型输出结构化的文本以便进行下一步的处理。

```python
text='介绍一下华为watch GT4，以JSON格式输出，其中包含以下键: 表盘大小，颜色，表带，价格和是否有货。'
prompt=f"""
现在你要扮演一位客服，来回答用户提出的问题。\
你回答的语气要保持谦逊、礼貌。\
你的回答要根据提供的规则来回答，不要回答无关内容，尽量保持简洁。\
如果你无法回答用户提出的问题，你要回复“无法回答该问题，请联系人工客服”。

规则:
商品名称: 华为watch GT4 \
表盘大小: 46mm、41mm \
商品规格:
46mm包含三种: \
1. 颜色: 云杉绿，表带: 立体编织复合表带，价格: 2088，是否有货: 无 \
2. 颜色: 山茶棕，表带: 经典皮质表带，价格: 1788，是否有货: 有 \
3. 颜色: 曜石黑，表带: 氟橡胶表带，价格: 1588，是否有货: 有 \
41mm包含三种: \
1. 颜色: 幻夜黑，表带: 氟橡胶表带，价格: 1488，是否有货: 有 \
2. 颜色: 凝霜白，表带: 细腻皮质表带，价格: 1688，是否有货: 有 \
3. 颜色: 皓月银，表带: 间金工艺表带，价格: 2688，是否有货: 有 \

你要回答用三个反引号括起来的问题: ```{text}```
"""
```
将该提示输入大模型，输出如图6 - 6所示。

![image](https://github.com/user-attachments/assets/7e7f964f-f098-4983-9144-15a439e039ef)


通过输出，我们可以看出大模型按照规则输出了结构化的信息。

3. **要求模型扮演角色**

通过将大模型置于特定角色的位置上，可以引导大模型从该角色的视角来理解和解决具体问题，并基于特定背景知识提供更准确和相关的答案。我们可以要求大模型扮演成客服角色，检查提供的商品资料是否满足用户的需求。

```python
text='手表支持支付宝支付吗'
prompt=f"""
现在你要扮演一位客服，来回答用户提出的问题。\
你回答的语气要保持谦逊、礼貌。\
你的回答要根据提供的规则来回答，不要回答无关内容，尽量保持简洁。\
如果你无法回答用户提出的问题，你要回复“无法回答该问题，请联系人工客服”。

规则:
商品名称: 华为watch GT4 \
具体功能:
1. 融合智能检测技术，计算每日活动小时数、锻炼时长和活动量，计算每日卡路里缺口。\
2. 支持走路骑行多种专业运动模式，实时监测各项运动数据，提供专业的训练建议。\
3. 记录心率、跑步轨迹和睡眠监测。\
4. 可以查看微信消息并且可以快速回复，以及微信支付。\
5. 支持蓝牙通话。\
6. 集成多种应用。\
7. 支撑NFC，可以解锁门禁和刷交通卡。\
8. 兼容安卓和IOS系统。\
9. 超长续航，常规场景可使用8天，最长使用场景可使用14天。

你要回答用三个反引号括起来的问题: ```{text}```
"""
```
客服输出示例如图6 - 7所示。

由该商品资料可知，手表支持微信支付是正确的，但不支持支付宝支付。


![image](https://github.com/user-attachments/assets/a33a1887-57cc-440a-ae3e-93a306251706)


4. **少样本学习示例**

在有些场景下，我们期望自定义模型的输出格式。为了满足这个需求，我们可以使用少样本（Few - shot）学习，即在模型


执行任务前，提供少量示例告诉模型期望的输出格式。

在接下来的示例中，我们要求智能客服在回答用户问题的时候，首先向用户问好，然后根据相应的规则来回答用户的问题，最后提示用户如果无法解决问题可以联系人工客服介入做进一步处理。

```python
text='我买的衣服穿着不合适，但是价签被我撕了，还能退货吗？'
prompt=f"""
现在你要扮演一位客服，来回答用户提出的问题。\
你回答的语气要保持谦逊、礼貌。\
你的回答要根据提供的规则来回答，不要回答无关内容，回答尽量保持简洁。\
如果你无法回答用户提出的问题，你要回复“无法回答该问题，请联系人工客服”。

规则:
买家提出“七天无理由退货”服务的申请条件:
1. 买家在签收商品之日起七天内，对支持七天无理由退货并符合完好标准的商品，可发起七天无理由退货申请。
2. 选择无理由退货的买家应当自收到商品之日起七天内向卖家发出退货通知。七天期限为自物流显示签收商品的次日零时开始起算，满168小时为7天。
3. 买家退回的商品应当完好。
4. 支持七天无理由退货的商品，卖家单方或买卖双方约定不支持七天无理由退货的行为无效。
5. 不同品类的商品适用七天无理由退货与否的情形:

你的回答可以遵循以下风格:
<用户>: 卖家在商品上标了七天无理由退货，但是反悔了怎么办？
<客服>: 尊敬的客户你好，根据“七天无理由退货”服务的申请条件的第4条，支持七天无理由退货的商品，卖家单方或买卖双方约定不支持七天无理由退货的行为无效。你可以继续和商家沟通退货，如果无法解决，可联系人工客服介入。

你要回答用三个反引号括起来的问题: ```{text}```
"""
```
将该提示输入大模型后，得到的输出如图6 - 8所示。 
