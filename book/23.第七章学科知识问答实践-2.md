在“选择LLM模型”中可以选择不同的大模型完成问答。以选择xinghuo - api（Running）为例。

![image](https://github.com/user-attachments/assets/65484f4f-e292-41d9-90a4-6b1971f6a162)


对话模式使用LLM对话功能，可以正常地和大模型对话。

![image](https://github.com/user-attachments/assets/99c7b80f-2bcc-4a7a-a164-13e97a855ea6)


切换对话模式至知识库问答。


![image](https://github.com/user-attachments/assets/80d74221-00cd-4ccf-9ead-07d53337809e)


还可以选择提示模板（可以选择default模板）功能，加载预设的提示模板，用于对话。

### 7.4.2 知识库构建

本节将基于学科知识建立知识库。打开项目网页，选择“知识库管理”，之后选择“新建知识库”。填写知识库名称和知识库简介，并选择向量库类型（Faiss）和Embedding模型（m3e - base）。

接着，上传之前处理过的学科知识文件到知识库。注意，这里的单段文本的最大长度需要小于每段分割文本的长度，这样分割器就可以实现按照“\n\n”来分割，如果设置过大，多个知识点会被划分成一个组块（chunk），就会产生“噪声”。

至此，我们完成了知识向量库的构建。

![image](https://github.com/user-attachments/assets/f16ed862-12c1-4ede-8537-2a8f7f986bb3)

![image](https://github.com/user-attachments/assets/3912103e-5c15-438e-9fd1-94cb35fdde76)



接下来测试知识库问答功能。为了防止大模型自己编造答案，需要把Temperature（温度）调低。Temperature是用于调整大模型生成文本时的创造性和多样性的超参数，是一个大于0的数值，通常在0～1之间。在每次“生成”时，相同的提示可能会产生不同的输出。Temperature主要用于控制创造力，为0时将始终产生相同的输出。Temperature越高，则生成结果的随机性越大，它会影响大模型生成文本时采样预测词汇的概率分布。当大模型的Temperature较高时（如0.8或更高），大模型会更倾向于从多样且不同的词汇中选择，这使得生成的文本创意性更强，但也可能产生更多的错误和不连贯之处。而当Temperature较低（如0.2、0.3等）时，大模型会主要从具有较高概率的词汇中选择，从而产生更平稳、更连贯的文本。但此时，生成的文本可能会显得过于保守和重复。因此在实际应用中，需要根据具体需求来权衡选择合适的Temperature值。

历史对话轮数用来设置存储的历史对话轮数，导出记录可以导出对话的内容，清空对话会清空对话的历史记录。

在知识库配置菜单中，选择新创建的知识库，然后将“知识匹配分数阈值”调整为0.5。知识库匹配相关度阈值的取值范围在0～1之间，阈值越小，相关度越高，取值为1相当于不筛选，建议设置在0.5左右。

尝试询问大模型深度学习包含哪些知识点，可以发现大模型是根据检索出的知识进行的回答。

![image](https://github.com/user-attachments/assets/4f209045-744e-4d93-9129-ee49a39e65a0)


至此，我们已完成了知识问答设置与测试。

### 7.4.3 基于LangChain的问答实践
LangChain作为一个大模型开发框架，可以将大模型（对话模型、嵌入模型等）、向量数据库、交互层提示、外部知识、外部代理工具整合到一起，进而可以自由构建大模型应用。LangChain具有模型输入/输出（Model I/O）、检索（Retrieval）以及代理（Agents）3大核心模块。

当开发者基于大模型构建应用时，最核心的要素就是输入和输出。LangChain提供了一套完整的流程可以让开发者和任何大模型进行交互。

![image](https://github.com/user-attachments/assets/1459fa51-979e-49be-b86a-6a324f9073aa)


首先，LangChain内置了多种提示模板以供选择。你可以使用PromptTemplate创建一个字符串提示模板，也可以使用ChatPromptTemplate来模拟一段对话，如果没有找到合适的内置模板，LangChain也支持用户自定义模板。创建好的模板会整合进大模型进行预测，由于目前大模型主要用于对话，很多组织、公司在构建大模型的时候会同时推出更适用于对话的Chat版，而LangChain对两者都进行了兼容适配。在很多任务中，我们期望得到结构化的信息，而大模型的输出字符串形式无法满足需求，LangChain提供了一系列输出解析器，将大模型的输出转换为结构化的信息。

提示模板是用于生成大模型提示的预定义模板。模板可以包括说明、少量示例以及适合给定任务的特定上下文和问题。LangChain提供了创建和使用提示模板的工具。LangChain旨在创建通用性的模板，以便能够轻松地跨大模型重用现有模板。

下面是大模型输入/输出的示例代码。

1）模型输入。我们使用PromptTemplate创建一个提示模板，该提示要求模型介绍一个知识点，并以JSON格式返回回答。同时，该提示中包含一个变量knowledge，在之后的查询中可以根据自己的需求设置不同的内容。

```python
# 创建提示
from langchain.prompts import PromptTemplate
prompt_template = PromptTemplate.from_template(
    "介绍一下{knowledge}，要求以JSON格式返回内容，其中要包含该知识点的概念、重要知识点以及难易度。"
)
input=prompt_template.format(knowledge="文本摘要")
input
```
2）预测。在示例中，我们选用百度的千帆大模型，将提示输入并进行预测，查看大模型的输出。另外，大模型的输出类型为字符串类型。
```python
# 创建一个模型
from langchain.llms import QianfanLLMEndpoint
import baidu_api
model = QianfanLLMEndpoint(model="ERNIE-Bot",qianfan_ak=baidu_api.APIKey,
                           qianfan_sk=baidu_api.secretKey)
output=model(input)
output
```
输出如下：
```json
{
    "知识点": "文本摘要",
    "概念": "文本摘要是从一段较长的文本中提取关键信息，生成一个简洁、包含主要观点的短文本的过程。",
   ...
}
```
```python
# 查看大模型的输出类型
type(output)
str
```

3）输出解析。在第2步中，虽然大模型以JSON的形式输出了回答，但是其格式还是字符串类型。因此，我们加载JSON输出解析器对大模型的输出进行转换，转换后回答的数据类型变成了结构化的字典（dict）。

```python
# 创建输出解析器
from langchain.output_parsers.json import SimpleJsonOutputParser
json_parser = SimpleJsonOutputParser()
json_output=json_parser.parse(output)
# 查看经过解析器后的数据类型
type(json_output)
dict
```


开发者构建的应用大多基于特定的领域知识，这时候就需要额外的数据进行支撑，而使用额外数据对大模型进行微调或者训练（对硬件设备有一定要求），这无疑增加了开发的成本，因此RAG成为开发者首选的解决方案。

LangChain提供了构建RAG整个流程的模块。其中关键的模块包括：数据读取（Load）、数据转换（Transform）、向量嵌入（Embed）、向量存储（Store）和检索（Retrieve）共5大模块。首先是数据读取模块，开发者的源数据可能是各种格式的，LangChain提供了CSV、HTML和JSON等多种格式的文件读取器，使用这些读取器可以轻松地加载用户的数据。接着，我们需要使用文档转换器将文档分割成小块，这是因为在检索的时候，对提示有价值的内容可能只是文档中的一小段，如果将整篇文档加入提示反而会造成提示冗余，降低模型回答的有效性。LangChain内置的转换器不仅可以分割字符，还可以分割代码、Markdown格式的标题等。分割完成后，LangChain可以加载嵌入模型将文档向量化以便进行相似度查询。为了方便后续使用，向量化完成后可以存储起来，LangChain支持Chroma、FAISS和Lance三种向量库。最后是检索模块，LangChain提供了强大的检索功能，你可以进行最常用的语义相似度检索、MMR检索，也可以将多种检索方式按权重结合等。

![image](https://github.com/user-attachments/assets/8ec093c8-6156-4b0d-929a-140e3978cb8f)


通用大模型很强大，但是在逻辑推理、计算等能力上较弱，也无法处理一些实时性的查询，例如天气、查询股票等，这时就需要借助外部程序。代理作为大模型的外部模块，可提供计算、逻辑检查、检索等功能的支持，使大模型能获得异常强大的推理和信息获取能力。在LangChain中，你可以定义各种方法来辅助大模型完成特定的任务。

至此，LangChain的核心模块已介绍完毕，接下来介绍如何将这些模块组合起来。

LangChain提供的组合方法很简单，就是将定义好的各个模块用“|”符号串联起来。简单的示例如下，首先加载聊天模型、输出解析器以及构建提示，接着按流程将模块组合成链，最后进行查询。

```python
from langchain.prompts import ChatPromptTemplate
from langchain.schema import strOutputParser
from langchain.chat_models import ErnieBotChat
import baidu_api
chat=ErnieBotChat(model_name="ERNIE-Bot",ernie_client_id=baidu_api.ApiKey,ernie_client_secret=baidu_api.secretKey)
prompt = ChatPromptTemplate.from_messages(
    [("human","{question}"),]
)
# 将各组件组成一个链
runnable = prompt | chat | strOutputParser()
runnable.invoke({"question":"2018年的世界杯冠军是哪个队伍？"})
```
输出示例如下：

“2018年的世界杯冠军是**法国国家男子足球队**。”

LangChain还拥有很多功能，由于篇幅限制不再介绍，感兴趣的读者可以自行探索。

### 7.5 本章小结
本章利用知识图谱从大量教育信息资源中提炼出了有序的知识关联关系，有效地整理了各学科的知识体系。通过大模型和知识图谱融合可提升大模型的性能和应用效果。之后，基于Langchain - Chatchat框架进一步介绍了学科知识图谱的构建流程，最后基于LangChain完成了知识库构建及问答实践。 
