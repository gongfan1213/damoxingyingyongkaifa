### 第8章 法律领域应用实践

随着社会的发展和法律制度的完善，人们对法律咨询的需求日益增长。本章将基于开源大模型进行新增参数微调，进一步构建法律知识问答系统，为政府部门、企事业单位、广大群众提供法律咨询服务，并提高法律咨询的准确性和效率。

#### 8.1 应用概述

法律领域的知识十分复杂，包括各个法律领域的法规、判例、先例等。法律咨询需要考虑各种法律细节和情境，并结合具体案件的事实进行分析和判断。因此无论是个人还是企业，面临法律问题时都需要专业的意见和指导。然而，律师的数量有限、专业水平参差不齐且收费较高，无法满足所有人的需求。因此，开发一种能够准确、及时提供法律咨询的大模型是非常必要的。

法律大模型是指专门针对法律领域的AI模型，它在通用大模型的基础上，使用高质量的法律数据进行微调，以提高大模型在法律问答、文本生成、案例分析等任务上的专业性和准确性。法律大模型的法律咨询能力在服务政府、企业、群众方面有广阔的应用前景，可以通过在线平台或聊天机器人等形式，为他们提供法律咨询服务。

除了法律咨询之外，法律大模型还为法律从业人员提供智慧化辅助，重点体现在以下几个层面。

1）法律大模型可以提供全流程的辅助办案应用，如构建智能审查、量刑预测、文书生成、自动编目、笔录生成等业务能力，还可以精简答辩状、自动分析争议焦点、分析嫌疑人是否具有刑事行为能力。

2）法律大模型可以提供全方位的司法监督管理应用，如基于大模型建设视频自动巡查、案件裁判偏离预警、案件智能核查等监督助手应用，及时发现和解决问题，加强司法工作流程的规范化。 

3）法律大模型可以提供司法数据深度挖掘应用，快速在海量的法律文本中搜索相关案例、法规和法律文献等信息，探索司法规律和趋势，为司法改革和法律制定提供数据支持。

#### 8.2 对话数据微调

由于开源大模型并不具备完整的专业知识以及法律问答的能力，而采用预训练的方法需要海量的算力支撑，因此一般选用微调模型的方法。相比预训练，微调技术既可以向大模型注入专业知识，又节省了算力资源的消耗。

![image](https://github.com/user-attachments/assets/dfa57db0-bb4c-4b07-9a0b-cc19e7b969e0)


随着大模型的发展，模型的参数量越来越大，达到千亿级别。因此，微调所有模型参数变得不可行。微软提出的LoRA微调方法，通过只微调新增参数的方式，大大减少了下游任务的可训练参数数量。本节介绍如何使用LoRA来微调一个法律问答模型。

##### 8.2.1 法律对话数据预处理
本案例采用的法律对话数据集是问答对的形式。其中，input是法律问题，answer是问题对应的回答。

![image](https://github.com/user-attachments/assets/b3a4bb60-23fa-40ac-93e6-cd8324d486dc)


第1步，我们对法律对话数据集进行预处理，在dataPreprocess.py脚本文件中定义修改数据集的格式，增加instruction键，将answer键改为output，最后保存处理后的数据即可。代码示例如下：
```python
import json
# 打开原文件
with open("data/raw/qa_data_train.json","r",encoding="utf-8") as f:
    data=json.load(f)
f.close()
# 将数据集转换成正确的格式
for d in data:
    d["instruction"] = ""
    d["output"] = d["answer"]
    del d["answer"]
# 将文件写到本地
with open("data/processed/chat_data.json","w",encoding="utf-8") as f:
    for d in data:
        json.dump(d,f,ensure_ascii=False)
        f.write('\n')
f.close()
```
执行上述代码，查看预处理后的微调数据集示例。

![image](https://github.com/user-attachments/assets/71477d96-bb1c-4117-9454-e733be01d059)



第2步，将数据转换成输入大模型的格式。选用智谱AI开源的大模型ChatGLM3作为基础大模型。

首先，将数据转换成ChatGLM3官方指定的提示格式，以下代码实现的功能就是将每条数据转换成“[gMASK] sop <|user|> 问题<|assistant|> 回答”格式。

```python
# 将数据设置成ChatGLM3官方指定的提示格式
class GLM3PromptDataSet(Dataset):
    def __init__(self, data_path, tokenizer, max_len, max_src_len, is_skip):
        self.all_data = []
        skip_data_number = 0
        with open(data_path, "r", encoding="utf-8") as fh:
            for i, line in enumerate(fh):
                sample = json.loads(line.strip())
                skip_flag = False
                src_tokens = [tokenizer.get_command("<|user|>")] + tokenizer.encode("\n", add_special_tokens=False) + \
                               tokenizer.encode(sample["instruction"] + sample["input"], add_special_tokens=False)
                if len(src_tokens) > max_src_len:
                    # 当输入内容超出限定长度时，向后截断
                    src_tokens = src_tokens[:max_src_len]
                    skip_flag = True
                max_tgt_len = max_len - 6 - len(src_tokens)
                tgt_tokens = [tokenizer.get_command("<|assistant|>")] + tokenizer.encode("\n", add_special_tokens=False) + \
                               tokenizer.encode(sample["output"], add_special_tokens=False)
                if len(tgt_tokens) > max_tgt_len:
                    # 当输出内容超出限定长度时，向后截断
                    tgt_tokens = tgt_tokens[:max_tgt_len]
                    skip_flag = True
                # ChatGLM3需要增加[gMASK]、sop两个标记
                input_ids = [tokenizer.get_command("[gMASK]"), tokenizer.get_command("sop")] + src_tokens + tgt_tokens + [tokenizer.eos_token_id]
                context_length = len(src_tokens) + 2
                labels = [-100] * context_length + input_ids[context_length:]
                assert len(input_ids) == len(labels)
                assert len(input_ids) <= max_len
                if is_skip and skip_flag:
                    skip_data_number += 1
                    continue
                self.all_data.append({"input_ids": input_ids, "labels": labels})
        print("the number of skipping data is {}".format(skip_data_number))
    def __len__(self):
        return len(self.all_data)
    def __getitem__(self, item):
        instance = self.all_data[item]
        return instance
```
其次，需要定义一个DataCollator用来批处理数据，对数据进行padding操作，并统一转换成Tensor格式，代码如下所示。

```python
# 数据校准器
class DataCollator(object):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id
    def __call__(self, batch):
        lengths = [len(instance["input_ids"]) for instance in batch]
        batch_max_len = max(lengths)
        input_ids_batch, labels_batch = [], []
        for instance in batch:
            input_ids = instance["input_ids"]
            labels = instance["labels"]
            padding_len = batch_max_len - len(input_ids)
            input_ids = input_ids + [self.pad_token_id] * padding_len
            labels = labels + [-100] * padding_len
            input_ids_batch.append(input_ids)
            labels_batch.append(labels)
        return {"input_ids": torch.tensor(input_ids_batch, dtype=torch.long),
                "labels": torch.tensor(labels_batch, dtype=torch.long)}
```
至此，我们完成了数据预处理，并按照ChatGLM3指定的提示格式进行了数据集格式转换。

##### 8.2.2 对话微调工具编写

微调法律大模型除了需要转换数据集，还需要编写微调工具。接下来开始编写微调法律大模型过程中需要用到的工具。

首先，定义一个print_trainable_parameters()函数，用来打印需要训练的参数。
```python
# 输出需训练的参数
def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for param in model.named_parameters():
        num_params = param.numel()
        if num_params == 0 and hasattr(param, "ds_numel"):
            num_params = param.ds_numel
        all_param += num_params
        if param.requires_grad:
            trainable_params += num_params
    print("trainable params: {} || all params: {} || trainable%: {}".format(trainable_params, all_param,100 * trainable_params / all_param))
```
定义print_rank_0()用来输出训练过程中的一些信息。
```python
# 输出工具
def print_rank_0(msg, rank=0):
    if rank <= 0:
        print(msg)
```
定义to_device()将数据或者模型移动到CPU或者GPU上。
```python
# 移动数据位置
def to_device(batch, device):
    output = {}
    for k, v in batch.items():
        try:
            output[k] = v.to(device)
        except:
            output[k] = v
    return output
```
定义set_random_seed()设置随机种子，复现模型结果。
```python
# 设置随机种子
def set_random_seed(seed):
    if seed is not None:
        set_seed(seed)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
```
定义save_model()，用来保存模型。
```python
# 保存模型
def save_model(model, tokenizer, output_dir, model_name, state_dict=None):
    save_dir = os.path.join(output_dir, model_name)
    if state_dict == None:
        model.save_pretrained(save_dir, torch_dtype=torch.float16)
    else:
        model.save_pretrained(save_dir, state_dict=state_dict, torch_dtype=torch.float16)
    tokenizer.save_pretrained(save_dir)
```

##### 8.2.3 模型微调框架的参数配置

编写完微调工具，还需要配置微调框架的参数。DeepSpeed是一个由微软开发的开源深度学习优化库，旨在提高大模型训练的效率和可扩展性。它通过多种技术手段来加速训练，包括模型并行化、梯度累积、动态精度缩放、本地模式混合精度等。

接下来，我们进行大模型微调框架DeepSpeed的参数配置。DeepSpeed框架常用的参数及说明如下表所示。
|参数|说明|参数|说明|
| ---- | ---- | ---- | ---- |
|train_batch_size|每批数据大小|fp16|开启混合精度训练|
|steps_per_print|打印信息的频率|scheduler|学习率调整器|
|zero_optimization|Zero内存优化|optimizer|设置优化器|

其中，zero_optimization是DeepSpeed对混合精度训练进行优化的设置，zero.optimization中的stage参数可选0、1、2、3这4个阶段。这里选用2阶段，即启用优化器 + 梯度状态分区。offload_param和offload_optimizer分别指参数卸载与优化器卸载，offload指在模型训练时，将用不到的参数或者未使用的优化器移到CPU上，从而可以更高效地利用GPU。在这里，我们将offload设置为auto。更多参数的可参考https://www.deepspeed.ai/。

接下来，设置LoRA的微调参数，具体参数设置如下。

```python
model = MODEL[args.mode]["model"].from_pretrained(args.model_name_or_path)
lora_module_name = args.lora_module_name.split(",")
config = LoRaConfig(r=args.lora_dim,
                    lora_alpha=args.lora_alpha,
                    target_modules=lora_module_name,
                    lora_dropout=args.lora_dropout,
                    bias="none",
                    task_type="CAUSAL_LM",
                    inference_mode=False,)
model = get_peft_model(model, config)
model.config.torch_dtype = torch.float32
```
LoRA的配置参数说明如下表所示。

|参数|说明|参数|说明|
| ---- | ---- | ---- | ---- |
|r|矩阵分解秩|bias|偏差|
|lora_alpha|LoRA中的超参数|task_type|指定任务类型|
|target_modules|选择特定模块进行微调|inference_mode|是否为推理模式|
|lora_dropout|丢失率| | |

其中，读取数据的核心代码如下：
```python
# 读取数据
train_dataset = MODEL[args.mode]["dataset"](args.train_path, tokenizer, args.max_len, args.max_src_len, args.is_skip)
if args.local_rank == -1:
    train_sampler = RandomSampler(train_dataset)
else:
    train_sampler = DistributedSampler(train_dataset)
data_collator = DataCollator(tokenizer)
train_dataloader = DataLoader(train_dataset, collate_fn=data_collator, sampler=train_sampler,
                              batch_size=args.per_device_train_batch_size)
```
完成LoRA微调参数的设置后，可以开始编写训练代码，实现模型训练。该部分训练代码遵循一般的PyTorch模型训练流程。在启用了梯度累积的情况下，需要完成一次完整的梯度更新周期，即累积了预定数量的小批量梯度并进行了一次参数更新后，才计算和显示loss（损失值），以及保存模型的状态。
```python
# 开始训练
for epoch in range(args.num_train_epochs):
    print_rank_0("Beginning of Epoch {}/{}, Total Micro Batches {}".format(epoch + 1, args.num_train_epochs,len(train_dataloader)), args.global_rank)
    model.train()
    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), unit="batch"):
        batch = to_device(batch, device)
        outputs = model(**batch, use_cache=False)
        loss = outputs.loss
        tr_loss += loss.item()
        model.backward(loss)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        model.step()
        if (step + 1) % args.gradient_accumulation_steps == 0:
            global_step += 1
            # write loss
            if global_step % args.show_loss_step == 0:
                print_rank_0("Epoch: {}, step: {}, global_step:{}, loss: {}".format(epoch, step + 1, global_step,(tr_loss - logging_loss) / (args.show_loss_step * args.gradient_accumulation_steps)),args.global_rank)
                print_rank_0("step: {}-{}-{}".format(step + 1, global_step, model.global_steps), args.global_rank)
                if args.global_rank <= 0:
                    tb_write.add_scalar("train_loss", (tr_loss - logging_loss) / (args.show_loss_step * args.gradient_accumulation_steps), global_step)
                    logging_loss = tr_loss
            # 保存模型
            if args.save_model_step is not None and global_step % args.save_model_step == 0:
                # 若进行zero3训练，则模型参数需要合并保存
                if ds_config["zero_optimization"]["stage"] == 3:
                    state_dict = model.zero3_consolidated_16bit_state_dict()
                    if args.global_rank <= 0:
                        save_model(model, tokenizer, args.output_dir, f"epoch-{epoch + 1}-step-{global_step}",state_dict)
                else:
                    if args.global_rank <= 0:
                        save_model(model, tokenizer, args.output_dir, f"epoch-{epoch + 1}-step-{global_step}")
                model.train()
```
编写训练的Shell启动命令示例，在命令行中设置LoRA训练需要的参数。

![image](https://github.com/user-attachments/assets/425bbd13-668a-44b2-8a23-463c13dde54c)

表
8-3给出了LoRA训练中部分参数的说明。

|参数|说明|参数|说明|
| ---- | ---- | ---- | ---- |
|master_port|DeepSpeed和其他软件通信的端口|ds_file|DeepSpeed配置文件的路径|
|max_len|最大输出长度|output_dir|模型保存路径|
|max_src_len|最大输入长度| | |

正常完成训练，且对应目录下有保存的权重表明训练成功。

![image](https://github.com/user-attachments/assets/1556ba65-708f-4957-b26d-4c44ba2d21a3)


至此，我们完成了大模型微调框架DeepSpeed的参数配置与训练。

##### 8.2.4 微调前后的对话问答对比

完成了参数配置及参数训练，可以进行模型预测。

首先，构造一个diy_generate()方法，其作用是让模型输出生成的内容。其中，大模型调用generate()方法以续写的形式生成内容，但会重复用户输入的文本，为了美化输出，需要手动将用户输入的文本部分切掉。
```python
# 模型生成方法
def diy_generate(model,tokenizer,text):
    with torch.no_grad():
        ids = tokenizer.encode(text)
        input_ids = torch.LongTensor([ids]).cuda()
        output = model.generate(
            input_ids=input_ids,
            min_length=20,
            max_length=512,
            do_sample=False,
            num_return_sequences=1
        )[0]
        output = tokenizer.decode(output)
        # 美化输出
        if '<|assistant|>' in output:
            output=output.split('<|assistant|>')[-1]
        else:
            output=output.split('\n',1)[-1]
        return output.strip()
```
首先加载基座模型，并输入问题让它回答，之后获取回答的结果。
```python
# 加载基座模型
model_path='path/to/chatglm3-6b'
base_model=ChatGLMForConditionalGeneration.from_pretrained(model_path).cuda()
tokenizer=ChatGLMTokenizer.from_pretrained(model_path)
query='如果一名犯罪嫌疑人正在接受医院治疗且生活不能自理，其是否还能被解除取保候审？'
print(diy_generate(base_model,tokenizer,query))
```
基座模型的输出结果。

加载微调

![image](https://github.com/user-attachments/assets/114653ae-ff1a-4e2e-8318-430f3df3a30b)



加载微调后的大模型，以及大模型参数训练得到的权重，然后和基座模型相结合。最后提问相同的问题并查看大模型的输出。
```python
# 加载微调模型
peft_model=PeftModel.from_pretrained(base_model,'output-glm3/epoch-2-step-46186')
print(diy_generate(peft_model,tokenizer,query))
```
微调后大模型的输出结果。


![image](https://github.com/user-attachments/assets/1299bf10-4c59-4f93-8f99-ebf60b6067f7)


对基座模型及微调大模型的输出内容进行分析，基座模型输出的内容和数据集的内容相似度很低，而微调后的大模型的输出在很多地方和数据集的内容高度相似，证明了LoRA 微调的有效性。 
