微调的有效性。不过，可以发现微调后的大模型输出的内容虽然形式正确，但缺乏实际含义，这是因为LoRA微调更新参数少，一般适合学习型任务。法律问答需要大量的专业知识，属于学习数据，需要利用预训练的方式将法律知识先注入模型。本案例展示的所有法律内容均来自互联网和模型输出，仅供学习大模型使用，实际的法律问题请咨询专业律师。

### 8.3 指令微调

在法律领域中，有很多概念性的知识。例如想要了解一个罪名，仅仅知道它的基本概念是不够的，还要了解罪名包含的特征，以及相关的司法解释。因此，无论是法律咨询还是律师日常办公，都会有像“某某罪的基本概念是什么？”“某某罪的特征是什么？”和“某某罪有司法解释吗？”这样的问题。这类问题和自由对话不一样，无论是回答格式，还是回答内容，都需要大模型严格遵循相关要求。对这类问题，我们可以采用指令微调方式让大模型完成任务。

指令微调数据集包含一系列涉及“指令输入”和“答案输出”的问答对。“指令输入”代表人类对模型提出的请求，涵盖了分类、概括、改写等多种类型。“答案输出”是模型根据指令生成符合人类期望的响应。

#### 8.3.1 法律指令数据集预处理

在法律领域特定的指令微调数据集中，指令专门为法律领域设计。使用指令微调技术需要构建用于指令微调的数据集，其结构为人类指令和期望输出组成的问答对。本案例随机选取856条罪名作为数据集，其中包含了它们的基本概念、特征和司法解释，存储在kg_crime.json文件中。

首先，读取数据集，将数据集处理成满足指令微调的格式，形成指令微调数据集，核心代码如下所示。
```python
# 读取未处理的数据
with open('./data/raw/kg_crime.json','r',encoding='utf-8') as f:
    kg_data=f.readlines()
f.close()
kg_data=[eval(i.rstrip()) for i in kg_data]
```
然后，进行数据预处理。需要特别说明的是，并不是所有罪名都有司法解释，查询这类罪名的司法解释时，希望大模型的输出是“本罪名没有司法解释。”。核心代码如下。
```python
# 概念、特征和解释
gainian_instruction="查询以下罪名的概念："
tezheng_instruction="查询以下罪名的特征："
jieshi_instruction="查询以下罪名的司法解释："
# 构建指令数据
instruction_data=[]
for data in kg_data:
    crime_name=data['crime_small']
    instruction_data.append({
        "prompt":gainian_instruction+crime_name,
        "response":data['gainian'][0]
    })
    instruction_data.append({
        "prompt":tezheng_instruction+crime_name,
        "response":' '.join(data['tezheng'])
    })
    invalid_jieshi="本罪名没有司法解释。"
    if len(data['jieshi']) != 0:
        instruction_data.append({
            "prompt":jieshi_instruction+crime_name,
            "response":' '.join(data['jieshi'])
        })
    else:
        instruction_data.append({
            "prompt":jieshi_instruction+crime_name,
            "response":invalid_jieshi
        })
```
执行data_process.py脚本文件，查看预处理后的数据集。预处理后的数据集中部分数据示例，其中prompt键对应的是人类的查询指令，response是期望模型给出的输出。

![image](https://github.com/user-attachments/assets/b30fee03-f36b-4280-89fa-0e082fd39add)


以上完成了指令微调数据读取及数据预处理。

#### 8.3.2 指令微调工具编写

完成了数据集预处理，还需要编写指令微调工具。接下来，我们采用ChatGLM3-Base作为基座模型，来应用这些微调数据。

在大模型微调过程中需要进行工具的编写。在preprocess_utils.py脚本文件中，我们只需关注InputOutputDataset类，该类会对数据进行padding操作。
```python
class InputOutputDataset(Dataset):
    def __init__(self, data: List[dict], tokenizer: PreTrainedTokenizer, max_source_length: int, max_target_length: int):
        super(InputOutputDataset, self).__init__()
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.max_seq_length = max_source_length + max_target_length + 1
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, i) -> dict:
        data_item = self.data[i]
        a_ids = self.tokenizer.encode(text=data_item['prompt'], add_special_tokens=True, truncation=True, max_length=self.max_source_length)
        b_ids = self.tokenizer.encode(text=data_item['response'], add_special_tokens=False, truncation=True, max_length=self.max_target_length)
        context_length = len(a_ids)
        input_ids = a_ids + b_ids + [self.tokenizer.eos_token_id]
        labels = [self.tokenizer.pad_token_id] * context_length + b_ids + [self.tokenizer.eos_token_id]
        pad_len = self.max_seq_length - len(input_ids)
        input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_len
        labels = labels + [self.tokenizer.pad_token_id] * pad_len
        labels = [(l if l != self.tokenizer.pad_token_id else -100) for l in labels]
        assert len(input_ids) == len(labels), f"length mismatch: {len(input_ids)} vs {len(labels)}"
        return {
            "input_ids": input_ids,
            "labels": labels
        }
```


定义trainer.py脚本文件中的PrefixTrainer类（用来保存训练的结果），例如微调权重、训练参数、分词器等，核心代码如下。

```python
class PrefixTrainer(Trainer):
    def __init__(self, *args, save_changed=False, **kwargs):
        self.save_changed = save_changed
        super().__init__(*args, **kwargs)
    def save(self, output_dir: Optional[str] = None, state_dict=None):
        # 如果输出目录不为空，那么在函数执行过程中，不再对该目录进行检查
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"Saving model checkpoint to {output_dir}")
        # 使用save_pretrained()然后可以使用from_pretrained()重加载它们
        # 保存训练好的模型和配置
        if not isinstance(self.model, PretrainedModel):
            if state_dict is None:
                state_dict = self.model.state_dict()
                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
            else:
                logger.info("Trainer.model is not a `PretrainedModel`, only saving its state dict.")
                if state_dict is None:
                    state_dict = self.model.state_dict()
                    torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
                else:
                    if self.save_changed:
                        print("Saving PrefixEncoder")
                        state_dict = self.model.state_dict()
                        filtered_state_dict = {}
                        for k, v in self.model.named_parameters():
                            if v.requires_grad:
                                filtered_state_dict[k] = state_dict[k]
                        self.model.save_pretrained(output_dir, state_dict=filtered_state_dict)
                    else:
                        print("Saving the whole model")
                        self.model.save_pretrained(output_dir, state_dict=state_dict)
            if self.tokenizer is not None:
                self.tokenizer.save_pretrained(output_dir)
```

ChatGLM3的官方代码提供了全量微调和P-Tuning两种微调方式。全量微调需要大量的算力资源，因此我们采用P-Tuning方式。在微调脚本中，需要正确设置ChatGLM3模型的路径以及数据集的路径，并根据读者机器的显存调整DEV_BATCH_SIZE和GRAD_ACCUMULARION_STEPS。

P-Tuning微调的代码放在finetune.py脚本文件中，关键代码如下。

1）加载配置文件。如果使用P-Tuning，则需要额外添加参数，pre_seq_len指虚拟令牌号（Virtual Token）长度，prefix_projection用来指定使用P-Tuning还是P-Tuning V2。P-Tuning是只在输入层添加虚拟令牌，P-Tuning V2是在模型的每一层都引入连续的可训练提示，而不仅限于输入层。

```python
# 加载配置文件
config = AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
# 使用P-Tuning需要额外添加以下参数
config.pre_seq_len = model_args.pre_seq_len
config.prefix_projection = model_args.prefix_projection
```
2）加载分词器和模型。
```python
# 加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
# 加载模型
if model_args.ptuning_checkpoint is not None:
    model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)
    prefix_state_dict = torch.load(os.path.join(model_args.ptuning_checkpoint, "pytorch_model.bin"))
    new_prefix_state_dict = {}
    for k, v in prefix_state_dict.items():
        if k.startswith("transformer.prefix_encoder."):
            new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
    model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
else:
    model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)
```


3）初始化P-Tuning。

```python
# 初始化P-Tuning
if model_args.pre_seq_len is not None:
    # P-Tuning V2
    model = model.half()
    model.transformer.prefix_encoder.float()
else:
    # Finetune
    model = model.float()
```


4）初始化训练器并开始训练。

```python
# 初始化trainer
trainer = PrefixTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    save_changed=model_args.pre_seq_len is not None
)
checkpoint = None
if training_args.resume_from_checkpoint is not None:
    checkpoint = training_args.resume_from_checkpoint
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
trainer.train(resume_from_checkpoint=checkpoint)
trainer.save_model()
trainer.save_state()
```



5）撰写微调训练脚本，设定一些需要用到的参数，参数的具体解释已在finetune_pt.sh脚本文件中注释，请参见本书前提及的网站查找并下载。

```bash
# P-Tuning的软提示长度
PRE_SEQ_LEN=128
# 学习率
LR=2e-2
# 使用的GPU数量
NUM_GPUS=1
# 输入文本的最大长度
MAX_SOURCE_LEN=64
# 输出文本的最大长度
MAX_TARGET_LEN=256
# batch size
DEV_BATCH_SIZE=1
# 梯度累积
GRAD_ACCUMULARION_STEPS=16
# 训练步数
MAX_STEP=1000
# 每500步保存一个模型
SAVE_INTERVAL=500
# 时间戳
DATESTR=`date +%Y%m%d-%H%M%S`
# 任务名称（可自定义）
RUN_NAME=instruction_finetune
# ChatGLM3的模型路径
BASE_MODEL_PATH=/root/llm_test/chatglm3-6b
# 数据集的路径
DATASET_PATH=./data/processed/instruction_data_train.json
# 模型输出路径
OUTPUT_DIR=output/${RUN_NAME}-${DATESTR}-${PRE_SEQ_LEN}-${LR}
```
执行脚本后，完成模型微调，在输出目录中生成权重、分词和训练参数等文件。

![image](https://github.com/user-attachments/assets/47704cd7-4d48-4a79-9c2b-4c9fc0fb25f0)


至此，我们完成了微调模型工具的编写，完成了模型微调工作。

#### 8.3.3 法律大模型指令问答评估

训练完成后，我们在inference.py脚本文件中分别加载ChatGLM3的基座模型和经过微调后的模型，用相同的问题提问，对比两个模型回答的表现。

由于P-Tuning微调结束后，会将训练得到的参数保存到一个新的模型中。因此，需要进行以下操作。

第1步，需要同时加载基座模型和新增参数模型，并进行手动合并。需要注意的是，在加载config时，需要传入参数pre_seq_len，也就是在P-Tuning训练时设定的前缀（prefix）长度。

第2步，加载P-Tuning的权重，权重文件是微调输出文件中的pytorch_model.bin，最后将预训练模型与微调后的模型参数相结合，代码如下所示。
```python
# 加载预训练模型及微调后的模型参数
if args.pt_checkpoint:
    # 载入tokenizer，trust_remote_code=True表示信任远程代码
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, trust_remote_code=True)
    # 从预训练的模型路径model_path中加载配置文件，pre_seq_len=128表示设置输入序列的最大长度为128
    config = AutoConfig.from_pretrained(args.model, trust_remote_code=True, pre_seq_len=128)
    # 根据配置文件和模型路径，加载预训练模型
    model = AutoModel.from_pretrained(args.model, config=config, trust_remote_code=True)
    # 加载微调后的权重文件pytorch_model.bin的模型参数，使用torch.load函数将参数存储在prefix_state_dict中
    prefix_state_dict = torch.load(os.path.join(args.pt_checkpoint, "pytorch_model.bin"))
    # 提取微调后的模型参数中的前缀编码器的相关部分，存储在new_prefix_state_dict中
    new_prefix_state_dict = {}
    for k, v in prefix_state_dict.items():
        if k.startswith("transformer.prefix_encoder."):
            new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
    model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
else:
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, trust_remote_code=True)
    model = AutoModel.from_pretrained(args.model, trust_remote_code=True)
```
第3步，读取测试集数据，并用大模型生成结果，将生成结果和对应的真实标签（truth_label）写入result.json文件。
```python
import json
# 读取测试集
with open(args.test_path,'r',encoding='utf-8') as f:
    test_data=json.load(f)
f.close()
query=[]
truth_result=[]
# 预测测试集的内容
for data in test_data:
    query.append(data["prompt"])
    truth_result.append(data["response"])
inputs = tokenizer(query,return_tensors="pt",max_length=512,padding=True,truncation=True)
inputs = inputs.to(args.device)
response = model.generate(input_ids=inputs["input_ids"], max_length=inputs["input_ids"].shape[-1] + args.max_new_tokens)
response = response[:, inputs["input_ids"].shape[-1]:]
pred_result=[tokenizer.decode(rsp, skip_special_tokens=True) for rsp in response]
result=[{"truth_label":truth_result[i],"pred_label":pred_result[i]} for i in range(len(pred_result))]
# 将预测结果写入文件
with open(args.output+"/result.json",'w',encoding='utf-8') as f:
    json.dump(result,f,ensure_ascii=False,indent=2)
f.close()
```
第4步，使用ROUGE指标来衡量预测文本和真实文本之间的N-Gram共现率。
```python
import json
import jieba
from rouge_chinese import Rouge
import numpy as np
rouge=Rouge()
scores=[]
for i in range(len(pred_result)):
    # 预测文本
    pred=list(jieba.cut(pred_result[i]))
    # 真实文本
    truth=list(jieba.cut(truth_result[i]))
    try:
        scores.append(rouge.get_scores(' '.join(pred), ' '.join(truth))[0])
    except:
        None
rouge_1=[]
rouge_2=[]
rouge_l=[]
for sc in scores:
    rouge_1.append(sc['rouge-1']['f'])
    rouge_2.append(sc['rouge-2']['f'])
    rouge_l.append(sc['rouge-l']['f'])
print('Rouge-1:',sum(rouge_1)/len(rouge_1))
print('Rouge-2:',sum(rouge_2)/len(rouge_2))
print('Rouge-l:',sum(rouge_l)/len(rouge_l))
```
第5步，编写推理脚本，执行inference.py文件，加载P-Tuning权重路径、模型路径、分词器路径、测试集路径及结果文件路径。
```bash
python .../inference.py \
    --pt-checkpoint ../output/instruction_finetune-20231118-171645-128-2e-2 \
    --model /data/law/LLM/llm_store/chatglm3-6b \
    --tokenizer /data/law/LLM/llm_store/chatglm3-6b \
    --test_path ../data/processed/instruction_data_test.json \
    --output ../
```
部分参数的说明如下表所示。


|参数|说明|参数|说明|
| ---- | ---- | ---- | ---- |
|pt-checkpoint|P-Tuning权重路径|test_path|测试集路径|
|model|模型路径|output|结果文件路径|
|tokenizer|分词器路径| | |

第6步，完成推理后查看结果文件，对比真实文本（truth_label）和预测文本（pred_label）结果。


![image](https://github.com/user-attachments/assets/cfa62fcc-6441-40c9-9657-38f8e09219c0)


![image](https://github.com/user-attachments/assets/7887622b-5e20-4699-bd1e-e55611a5a985)


查看打印出来的指标结果，微调前预测文本基于召回率的一元词序列重叠度指标Rouge-1的分数为0.47，微调前预测文本基于召回率的二元词序列重叠度指标Rouge-2的分数为0.35，微调后预测文本Rouge-1的分数为0.42，说明经过微调后，模型输出的内容已经拟合了一半的训练数据，一元词序列重叠度指标已经下降，达到了较好的效果。但是查看数据后发现，拟合的数据只是生成文本的前半部分，后半部分拟合效果不好，还需要根据数据继续优化。

至此，我们完成了法律大模型微调及基于指令问答的评估。

#### 8.3.4 微调前后的对话问答对比
指令微调后，我们对微调前后大模型回答的差异进行具体分析。


1）首先在predict.py脚本文件中加载基座模型、配置文件和分词器。
```python
from transformers import AutoTokenizer, AutoModel, AutoConfig
import torch
model_path = '' # 填写基础模型路径
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)
model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)
model.eval()
model.cuda()
```


2）加载基座模型并输入问题让它回答。
```python
gainian_query='查询以下罪名的概念：交通肇事罪'
tezheng_query='查询以下罪名的特征：交通肇事罪'
jieshi_query='查询以下罪名的司法解释：交通肇事罪'
response,history=model.chat(tokenizer,gainian_query,history=[])
print('基座模型:\n'+gainian_query+'\n'+response+'\n')
response,history=model.chat(tokenizer,tezheng_query,history=[])
print('基座模型:\n'+tezheng_query+'\n'+response+'\n')
response,history=model.chat(tokenizer,jieshi_query,history=[])
print('基座模型:\n'+jieshi_query+'\n'+response+'\n')
```


3）最后加载微调后的模型，需要加载8.3.2节训练得到的权重，然后和基座模型相结合。最后提问与第2步相同的问题，并查看大模型的输出。
```python
# 将P-Tuning微调得到的参数拼接到ChatGLM3中
prefix_state_dict = torch.load('output/instruction_finetune-20231118-171645-128-2e-2/pytorch_model.bin')
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
def get_response(model,tokenizer,query):
    inputs=tokenizer(query,return_tensors='pt')
    inputs=inputs.to('cuda')
    response = model.generate(input_ids=inputs["input_ids"], max_length=128)
    response = response[:, inputs["input_ids"].shape[-1]:]
    return tokenizer.decode(response, skip_special_tokens=True)
response = get_response(model,tokenizer,gainian_query)
print('微调模型:\n'+gainian_query+'\n'+response+'\n')
response = get_response(model,tokenizer,tezheng_query)
print('微调模型:\n'+tezheng_query+'\n'+response+'\n')
response = get_response(model,tokenizer,jieshi_query)
print('微调模型:\n'+jieshi_query+'\n'+response+'\n')
```
我们以“交通肇事罪”为例，询问大模型“交通肇事罪”的概念、特征和司法解释，让基座模型和微调后的模型分别输出它的概念、特征和司法解释。 
