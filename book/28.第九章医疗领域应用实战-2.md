### 9.3 增量预训练与微调
构建完数据集之后，可以基于数据集进行增量预训练、有监督微调，以及直接偏好优化。

#### 9.3.1 增量预训练

完成了增量预训练数据集预处理，还需要编写增量预训练代码。接下来采用GLM的对话版本GLM-4-9B-Chat作为基座模型进行预训练。增量预训练代码文件为pretraining.py，训练Shell脚本为run_pt.sh，训练数据使用增量预训练数据集，增量预训练代码文件的执行逻辑如下。

1. **导入依赖包**

```python
import math
import os
from dataclasses import dataclass, field
from glob import glob
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
import numpy as np
import torch
from datasets import load_dataset
from loguru import logger
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training
from sklearn.metrics import accuracy_score
from transformers import (
    AutoConfig,
    BloomForCausalLM,
    AutoModelForCausalLM,
    AutoModel,
    LlamaForCausalLM,
    BloomTokenizerFast,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    Seq2SeqTrainingArguments,
    is_torch_tpu_available,
    set_seed,
    BitsAndBytesConfig,
)
from transformers.trainer import TRAINING_ARGS_NAME
from transformers.utils.versions import require_version
```


2. **设置模型预训练相关的参数**
```python
@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """
    model_type: str = field(
        default=None,
        metadata={"help": "Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys())}
    )
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
        },
    )
```



3. **定义各函数并加载训练集**
```python
def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    # 标签的特殊处理
    # 确保张量是以正确的类型创建的
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if type(first["label_ids"][0]) is int else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
```


4. **加载模型和分词器**
```python
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=block_size,
    )
    # 将input_ids复制到标签上进行语言建模，这既适用于掩码语言建模（如BERT），也适用于因果语言建模（如GPT）
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

def tokenize_wo_pad_function(examples):
    return tokenizer(examples["text"])

# 主数据处理函数，将连接数据集中的所有文本，并生成block_size大小的块
def group_text_function(examples):
    # 将所有文本连接起来
    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # 删除了少量剩余块，如果模型支持，你可以根据需要自定义此部分，进行添加填充，而不是删除
    if total_length >= block_size:
        total_length = (total_length // block_size) * block_size
    # 按max_len分割块
    result = {
        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```


5. **模型训练及评估**
```python
if script_args.use_peft:
    logger.info("微调模型：LoRA(PEFT)")
    if script_args.peft_path is not None:
        logger.info(f"Load pre - trained model: {script_args.peft_path}")
        model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)
    else:
        logger.info("Init new peft model")
        if load_in_8bit or load_in_4bit:
            model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)
        target_modules = script_args.target_modules.split(',') if script_args.target_modules else None
        if target_modules and all:
            target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)
        modules_to_save = script_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        # 调整嵌入层的大小以匹配新的分词器
        embedding_size = model.get_input_embeddings().weight.shape[0]
        if len(tokenizer) > embedding_size:
            model.resize_token_embeddings(len(tokenizer))
        logger.info(f"Peft target modules: {target_modules}")
        logger.info(f"Peft lora_rank: {script_args.lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=script_args.lora_rank,
            lora_alpha=script_args.lora_alpha,
            lora_dropout=script_args.lora_dropout,
            modules_to_save=modules_to_save
        )
        model = get_peft_model(model, peft_config)
    for param in filter(lambda p: p.requires_grad, model.parameters()):
        param.data = param.data.to(torch.float32)
    model.print_trainable_parameters()
```


6. **查看训练结果**
```python
if training_args.do_train:
    logger.info("*** Train ***")
    logger.debug(f"Train dataloader example: {next(iter(trainer.get_train_dataloader()))}")

checkpoint = training_args.resume_from_checkpoint
if training_args.resume_from_checkpoint:
    checkpoint = trainer.train(resume_from_checkpoint=checkpoint)
train_result = trainer.train()
metrics = train_result.metrics
metrics["train_samples"] = max_train_samples
trainer.log_metrics("train", metrics)
trainer.save_metrics("train", metrics)
trainer.save_state()
model.config.use_cache = True # enable cache after training
tokenizer.padding_side = "left" # restore padding side
tokenizer.init_kwargs["padding_side"] = "left"

if trainer.is_world_process_zero():
    logger.debug(f"Training metrics: {metrics}")
    logger.info(f"Saving model checkpoint to {training_args.output_dir}")
    if is_zero3_enabled():
        save_model_zero3(model, tokenizer, training_args, trainer)
    else:
        save_model(model, tokenizer, training_args)
```


编写训练用的Shell脚本。显存参数可以根据GPU的实际情况进行修改，当前参数采用的基础模型是GLM4，显卡为A40，已配置总显存为96GB。
```bash
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 pretraining.py \
    --model_type chatglm \
    --model_name_or_path /data/testuser/pretrained_model/glm-4-9b-chat \
    --train_file_dir ./data/pretrain \
    --validation_file_dir ./data/pretrain \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --do_train \
    --do_eval \
    --use_peft True \
    --seed 42 \
    --max_train_samples 10000 \
    --max_eval_samples 10 \
    --num_train_epochs 0.5 \
    --learning_rate 2e-4 \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 50 \
    --evaluation_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 13 \
    --gradient_accumulation_steps 1 \
    --preprocessing_num_workers 10 \
    --block_size 512 \
    --group_by_length True \
    --output_dir glm4-pt-v1 \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --target_modules all \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --torch_dtype bfloat16 \
    --bf16 \
    --device_map auto \
    --report_to tensorboard \
    --ddp_find_unused_parameters False \
    --gradient_checkpointing True \
    --cache_dir ./cache
```
训练过程控制台显示如图9-7所示，包括损失函数值loss、梯度值grad_norm、学习率learning_rate、训练轮次epoch等信息。

![image](https://github.com/user-attachments/assets/108721fa-3397-4d34-9a7c-ffef03ddf809)


```
{'loss': 5.4375, 'grad_norm': 4.047325134277344, 'learning_rate': 2.222222222222223e-05, 'epoch': 0.0}
{'loss': 4.4991, 'grad_norm': 4.23513355267334, 'learning_rate': 0.000198642105263159, 'epoch': 0.03}
{'loss': 3.3523, 'grad_norm': 6.901773452758769, 'learning_rate': 0.0001855263157894737, 'epoch': 0.06}
{'loss': 2.9566, 'grad_norm': 3.8719933697946674, 'learning_rate': 0.0001972538210526316, 'epoch': 0.09}
{'loss': 2.8664, 'grad_norm': 2.26759672164917, 'learning_rate': 0.00014605263157894737, 'epoch': 0.16}
{'loss': 2.7839, 'grad_norm': 2.348264083862305, 'learning_rate': 0.00011973684210526317, 'epoch': 0.19}
{'eval_loss': 2.875, 'eval_accuracy': 0.4882583176254403, 'eval_runtime': 0.9347, 'eval_samples_per_second': 10.7, 'eval_steps_per_second': 5.35}
{'loss': 2.6297, 'grad_norm': 2.3694567688358887, 'learning_rate': 0.00013289473684210528, 'epoch': 0.22}
{'loss': 2.6797, 'grad_norm': 3.215446949805127, 'learning_rate': 0.00010657894736842107, 'epoch': 0.25}
{'loss': 2.6555, 'grad_norm': 2.334383726119995, 'learning_rate': 0.00010657894736842107, 'epoch': 0.28}
{'loss': 2.5672, 'grad_norm': 2.4750844345855713, 'learning_rate': 9.34210526315789e-05, 'epoch': 0.31}
{'loss': 2.5438, 'grad_norm': 2.29602805978125, 'learning_rate': 8.026315789473685e-05, 'epoch': 0.31}
```

在训练过程中，还会保存模型的检查点，根据距离矩阵评估算法性能（Eval Metrics），控制台显示的具体信息如图9-8所示。

模型默认使用LoRA训练模型，LoRA权重保存在adapter_model.bin文件，LoRA配置文件是adapter_config.json，合并到基础模型的方法参见merge_peft_adapter.py。

日志保存在glm4-pt-v1/runs目录下，可以使用Tensorboard查看，启动Tensorboard的方式如下：tensorboard --logdir glm4-pt-v1/runs --host 0.0.0.0 --port 8009，如图9-9所示。

Tensorboard按照设置的地址和端口启动服务，单击打开服务网址（如http://0.0.0.0:8009），Tensorboard网页显示信息如图9-10所示。

将LoRA模型权重合并到GLM-4-9B-Chat，将合并后的模型glm4-pt-merged保存到--output_dir指定的目录下，合并方法如下。

```bash
python merge_peft_adapter.py \
    --model_type chatglm \
    --base_model /data/testuser/pretrained_model/glm-4-9b-chat \
    --lora_model glm4-pt-v1 \
    --output_dir glm4-pt-merged/
```
合并过程中的控制台显示信息如图9-11所示。

![image](https://github.com/user-attachments/assets/6bfb54b7-6bfa-4f2a-b24e-a0534e8188c9)

![image](https://github.com/user-attachments/assets/117cd31e-07bf-43b1-ae71-c1476282fbba)


![image](https://github.com/user-attachments/assets/f91464c6-eb87-462b-8d2f-b4b295a55924)


```
Namespace(model_type='chatglm', base_model='/data/testuser/pretrained_model/glm-4-9b-chat', tokenizer_path=None, lora_model='glm4-pt-v1', resize_emb=False, output_dir='glm4-pt-merged/', hf_hub_model_id='', hf_hub_token=None)
Loading LoRA for causal language model/glm-4-9b-chat
Loading checkpoint shards: 10/10 [00:10<00:00,  1.02s/it]
Special tokens have been added to the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Merging with merge_and_unload...
Saving to Hugging Face format...
Done! model saved to glm4-pt-merged/
```
至此，完成了以GLM-4-9B-Chat作为基座模型的预训练，并生成了增量预训练模型glm4-pt-merged。

#### 9.3.2 有监督微调

完成了有监督微调数据集预处理之后，还需要编写有监督微调代码。本节涉及的文件和工作如下。

主要涉及的文件如下。有监督微调代码文件为supervised_finetuning.py，训练用的Shell脚本为run_sft.sh，训练数据使用已构建的有监督微调指令对话数据集，生成模型使用增量预训练阶段训练好的模型glm4-pt-merged，有监督微调代码文件的执行逻辑与增量预训练的执行逻辑一致。

主要涉及的工作包括：①导入依赖包；②设置有关有监督微调的参数；③定义各函数并加载训练集；④加载模型和分词器；⑤模型训练及评估；⑥查看训练结果。可参见9.3.1节的增量训练代码，不再赘述。

编写训练用的Shell脚本。显存参数可以根据GPU的实际情况进行修改，当前参数采用的基础模型是glm4-pt-merged，显卡为A40，已配置总显存为96GB。

```bash
CUDA_VISIBLE_DEVICES
