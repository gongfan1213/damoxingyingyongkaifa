### 附录 大模型的发展阶段
大模型不断发展至今，主要经历了统计语言模型（Statistical Language Model，SLM）、神经网络语言模型（Nerual Language Model，NLM）、预训练语言模型以及大语言模型等发展阶段。


![image](https://github.com/user-attachments/assets/64c89622-e590-4939-8400-9a90ae03781f)


![image](https://github.com/user-attachments/assets/48cb281d-777d-499b-b761-3c8daa3dfd76)


![image](https://github.com/user-attachments/assets/c403f8c2-d186-4acd-b924-a57cbf13317c)



![image](https://github.com/user-attachments/assets/6eae9c14-eff4-4039-9382-ad3849d6fe38)



![image](https://github.com/user-attachments/assets/c56f07e5-fc84-41bf-9b91-0cce85fd8aa1)


![image](https://github.com/user-attachments/assets/50d02aca-5b00-4da6-afe0-6a21095c7096)



![image](https://github.com/user-attachments/assets/c84a8049-63cb-400d-95a8-ef73eefe4a32)


![image](https://github.com/user-attachments/assets/dd3bec1e-f37e-473a-9bbb-a65f054a90ae)


![image](https://github.com/user-attachments/assets/97437106-70e6-47e6-8865-48962b9d7715)



![image](https://github.com/user-attachments/assets/e62ef971-ab35-47bf-8f69-58a6fc756e50)



![image](https://github.com/user-attachments/assets/3cfb7844-01f7-4391-bdbc-162b62a67675)



![image](https://github.com/user-attachments/assets/45c6910a-279d-468c-8c15-ca77e22488cc)


#### A.1 统计语言模型
统计语言模型是一种早期的自然语言处理的方法，其核心在于表示和计算一个句子或一段话出现的可能性。这种模型以统计学理论为基础，通过对大量语言数据的学习来计算一个句子中词的顺序组合概率。这种组合的可能性反映了句子成立的可能性。概率越高，意味着这句话越符合语言规则，自然性越强。统计语言模型在机器翻译、搜索引擎等应用场景中取得了显著成功。

尽管如此，统计语言模型也存在一些局限性。由于每个词语出现的概率都是基于统计计算出来的，因此它们无法对未收录词语进行计算，容易出现数据稀疏问题。

统计语言模型的主要目标是对任意文本序列在语言交流中出现的概率进行建模，概率越大表示该段文本序列越符合语言习惯和会话逻辑。对于一条长为m的文本序列s = w₁, w₂, …, wₘ ，其出现概率可由链式法则表示为以下形式：
\[ P(s)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots P(w_m|w_1,\cdots,w_{m - 1})=\prod_{i = 1}^{m}P(w_i|w_1,\cdots,w_{i - 1}) \]
其中 \( P(w_i|w_1,\cdots,w_{i - 1}) \) 表示给定前 \( i - 1 \) 个词的情况下第 \( i \) 个词出现的概率。然而，这种建模方法所需的参数量是巨大的，假设一种语言的词汇量为 \( V \)，则长为 \( m \) 的文本序列共有 \( V^m \) 种组合，这意味着模型的参数量随文本序列长度呈指数级增长。

N - Gram模型是统计语言模型中的一个典型代表，它将文本内容按照字节进行大小为 \( N \) 的滑动窗口操作，形成长度为 \( N \) 的字节片段序列，每一个字节片段称为Gram。通过对所有Gram的出现频度进行统计，并按照事先设定好的阈值进行过滤，可以形成关键Gram列表。这个列表构成了文本的向量特征空间，其中每一种Gram代表一个特征向量维度。若引入马尔可夫假设（假设当前词出现的概率仅依赖前 \( n - 1 \) 个词），则上述文本序列 \( s \) 的出现概率可近似地表示为：
\[ P(s)=\prod_{i = 1}^{m}P(w_i|w_1,\cdots,w_{i - 1})\approx\prod_{i = 1}^{m}P(w_i|w_{i - n + 1},\cdots,w_{i - 1}) \]
特别是，当 \( n = 1、2、3 \) 时，对应的语言模型分别称为一元语言模型（Uni - Gram）、二元语言模型（Bi - Gram）和三元语言模型（Tri - Gram）。尽管N - Gram语言模型进行了简化，但其所需的参数量仍为 \( V^n \) 量级，因此在实际应用中一般取 \( n < 4 \)。

N - Gram语言模型的核心在于对条件概率 \( P(w_i|w_{i - n + 1},\cdots,w_{i - 1}) \) 的计算。在给定训练语料的情况下，可以采用最大似然估计（Maximum Likelihood Estimation，MLE）的方式进行统计，即：
\[ P(w_i|w_{i - n + 1},\cdots,w_{i - 1})=\frac{\text{count}(w_{i - n + 1},\cdots,w_{i - 1},w_i)}{\text{count}(w_{i - n + 1},\cdots,w_{i - 1})} \]


其中，\( \text{count}(w_{i - n + 1},\cdots,w_{i - 1}) \) 表示给定的N - Gram文本序列 \( w_{i - n + 1},\cdots,w_{i - 1} \) 在语料中出现的频次，而 \( \text{count}(w_{i - n + 1},\cdots,w_{i - 1},w_i) \) 表示第 \( i \) 个词在给定的N - Gram文本序列 \( w_{i - n + 1},\cdots,w_{i - 1},w_i \) 中出现的频次。然而这种计算方法可能会由于数据稀疏导致零概率问题，即有的N - Gram在训练语料中没有出现，导致这些序列的最大似然估计结果为0，进而导致整个文本序列出现的概率变为0。为解决这一问题，常见的做法是采用数据平滑（Data Smoothing）的方法对未出现N - Gram的频率进行调整，例如拉普拉斯（Laplace）平滑（也称加一平滑）将任意N - Gram出现的频次都加1，即：
\[ P(w_i|w_{i - n + 1},\cdots,w_{i - 1})=\frac{\text{count}(w_{i - n + 1},\cdots,w_{i - 1},w_i)+1}{\text{count}(w_{i - n + 1},\cdots,w_{i - 1})+|V|} \]


其中，\( |V| \) 是指平滑因子 \( 1\times \) 特征数量（词汇量） \( V \) 的值，从而避免了零概率问题。其他常见的平滑方法还有Lidstone平滑、Backoff平滑、差值平滑等。

总的来说，N - Gram模型是一种简单而有效的统计语言模型，常用的N - Gram模型有二元语言模型和三元语言模型。在对话系统、机器翻译等实际任务中表现良好。然而由于N - Gram所基于的马尔可夫假设只建模了前 \( n - 1 \) 个词对当前词的影响，这使得它难以处理文本中的长距离依赖关系。而当 \( n \) 增大时，参数空间又呈指数级增长。随着深度学习方法的兴起，基于神经网络的语言模型逐渐成为主流。

#### A.2 神经网络语言模型
随着神经网络在机器学习领域应用的推广，进一步运用了NLP来估计单词的分布。神经网络语言模型（Nerual Language Model，NLM）作为一种无监督学习技术，可以直接从原始文本数据中学习到有用的文本特征表示。此外，神经网络语言模型通过词向量的距离衡量单词之间的相似度，因此，对统计语言模型无法计算的未收录词语，也可以通过相似词进行估计，从而避免出现数据稀疏问题。

神经网络语言模型需要学习语言中的规则和模式，进而能很好地完成预测文本序列的下一个词的任务，这涉及对大量的词汇和语句结构进行学习，以及对词义和语法等知识的表示。


经典的神经网络语言模型框架包括以下几个步骤。

1. 输入单词被编码成一个高维空间中的“词向量”（也称为“词嵌入”或“稠密向量”）。

2. 将词向量送入神经网络进行处理。

3. 神经网络的输出被解码成预测下一个单词的概率分布。


常见的神经网络语言模型有RNN、LSTM、GRU等，这些模型具有循环连接，可以捕捉文本序列中的上下文信息，从而实现对长文本的理解和生成。在任务方面，除了预测下一个单词之外，神经网络语言模型也可用于其他NLP任务，如文本分类、情感分析、命名实体识别等。


在自然语言中，单词构成了表达含义的基本单元。为了使计算机能够理解和处理人类的自然语言，需要将每个单词映射为一个数值向量，以便后续进行计算操作，这种数值向量称为词向量。词向量的表示主要有独热编码和分布式表示两种方式。

1. **独热编码**

独热编码是一种最简单、直观的词向量表示方法。对一种词汇量为 \( V \) 的语言，每个词都与 \( 0\sim (V - 1) \) 之间的一个编号相对应。因此，每个词都可以通过一个长度为 \( V \) 的0 - 1向量来表示，其中只有该单词对应编号位置的元素为1，其余均为0。然而，这种表示方法存在两个缺陷。

（1）随着词汇表的不断增大，向量维度也随之呈线性增长，但每个词向量中仅有一个元素为1。最终，所有词向量构成了一个高度稀疏的庞大矩阵，给后续的存储和使用都造成了不便。

（2）任意两个词向量之间的点积（或余弦相似度）均为0，无法体现词与词之间的语义关系。


为了解决上述离散表示所存在的缺陷，技术人员提出了Word2Vec、GloVe等分布式词向量表示方法。这些方法将每个单词的语义信息编码为一个低维稠密向量，在方便使用的同时捕获单词之间的语义关系，使得具有相似语义的单词在向量空间中距离较近，而语义上差异较大的单词则距离较远。这些词向量表示方法大幅提升了计算机对语言的理解能力，使其能够更好地处理文本分类、语义相似度计算、机器翻译等NLP任务。

2. **分布式词向量表示**

Word2Vec是一种分布式词向量表示方法。相比离散的独热编码，该方法将单词转化为低维的稠密向量，能够更好地建模单词之间的语义关系，因而在各种NLP任务中得到广泛应用。具体来说，Word2Vec包含了跳字模型和连续词袋模型两种。

（1）**跳字模型**

跳字模型的目标是基于给定目标词（Target Word）预测其一定范围内的上下文词（Context Word），即尽量使这两个词的词向量接近。

假定词汇表 \( V \) 中的每个词 \( w_i \) 都对应两个可学习的 \( N \) 维向量表示 \( v_i \) 和 \( u_i \)，分别作为 \( w_i \) 中心词向量和上下文词向量，其中 \( i\in V = \{0,1,\cdots,|V| - 1\} \) 表示单词在词汇表中的索引编号。假设上下文各单词在给定中心词的情况下是独立生成的，因此基于中心词 \( w_c \) 生成任意上下文词 \( w_o \) 的条件概率可建模为：

\[ P(w_o|w_c)=\frac{\exp(u_o^Tv_c)}{\sum_{i\in V}\exp(u_i^Tv_c)} \]

其中，\( v_c \) 为中心词 \( w_c \) 的词向量，\( u_o \) 为任意上下文词 \( w_o \) 的词向量。

跳字模型采用浅层的神经网络，以目标词作为输入，以上下文词作为输出，网络结构仅有一层隐藏层（投影层）。


![image](https://github.com/user-attachments/assets/182e7394-caf5-4705-b074-534cdc2ab11a)


![image](https://github.com/user-attachments/assets/b82ded76-e76f-4b8a-bfa5-fb7876ad1cf7)


给定长度为 \( T \) 的文本序列，记其中第 \( t \) 个词为 \( w^{(t)} \)，上下文窗口长度为 \( j \)，窗口大小为 \( m \)（一般选为2），则跳字模型对应的似然函数为：
\[ \prod_{t = 1}^{T}\prod_{-m\leq j\leq m,j\neq0}P(w^{(t + j)}|w^{(t)}) \]

为了学习模型参数，可采用最大化似然函数的方法来进行模型训练，其等价于如下的最小化损失函数：
\[ \mathcal{L}_{SG}=-\sum_{t = 1}^{T}\sum_{-m\leq j\leq m,j\neq0}\log P(w^{(t + j)}|w^{(t)}) \]

通常采用随机梯度下降算法来进行模型优化。在模型迭代过程中，会调整词向量，使得目标词的词向量与其上下文词的词向量在向量空间中尽可能地接近。

（2）**连续词袋模型**


![image](https://github.com/user-attachments/assets/23392bde-8b40-43d2-97c4-4f4e81044a72)


![image](https://github.com/user-attachments/assets/162ad750-8818-4385-8862-f7c9828b0412)



连续词袋模型的目标与跳字模型恰好相反，是基于上下文预测中心词。与跳字模型类似，词典中的每个词 \( w_i \) 都对应两个可学习的 \( N \) 维向量表示 \( v_i \) 和 \( u_i \)，但不同的是，它们分别对应 \( w_i \) 作为上下文词和中心词时的词向量。连续词袋模型假设中心词是基于其周围的上下文词生成的，因此基于上下文词 \( w_{o_1},\cdots,w_{o_{2m}} \) 生成任意中心词 \( w_c \) 的条件概率可建模为：

\[ P(w_c|w_{o_1},\cdots,w_{o_{2m}})=\frac{\exp\left(\frac{1}{2m}u_c^T(v_{o_1}+\cdots+v_{o_{2m}})\right)}{\sum_{i\in V}\exp\left(\frac{1}{2m}u_i^T(v_{o_1}+\cdots+v_{o_{2m}})\right)} \]

其中，\( v_{o_1}+\cdots+v_{o_{2m}} \) 为上下文词 \( w_{o_1},\cdots,w_{o_{2m}} \) 的词向量，\( u_c \) 为中心词 \( w_c \) 的词向量。

连续词袋模型是一种使用词向量来训练神经网络的方法，上下文由给定目标词的多个单词表示作为输入层，神经网络作为隐藏层（投影层），预测词作为输出层。

给定长度为 \( T \) 的文本序列，其中第 \( t \) 个词表示为 \( w^{(t)} \)，上下文窗口大小为 \( m \)，则连续词袋模型对应的似然函数为：
\[ \prod_{t = 1}^{T}P(w^{(t)}|w^{(t - m)},\cdots,w^{(t - 1)},w^{(t + 1)},\cdots,w^{(t + m)}) \]

最大化似然函数等价于如下的最小化损失函数：
\[ \mathcal{L}_{CBOW}=-\sum_{t = 1}^{T}\log P(w^{(t)}|w^{(t - m)},\cdots,w^{(t - 1)},w^{(t + 1)},\cdots,w^{(t + m)}) \]

因为一个中心词会有多个上下文词，而且每个上下文词都会计算得到一个 \( 1*N \) 向量，将这些 \( 1*N \) 的向量相加取平均，得到中间层（隐藏层）的向量，这个向量也是 \( 1*N \)，之后这个向量需要乘以一个 \( N*V \) 的矩阵，最终得到的输出层维度为 \( 1*V \)。然后将 \( 1*V \) 的向量经Softmax处理得到新的 \( 1*V \) 向量，在 \( V \) 个取值中概率最大的数字对应的位置所表示的词就是预测结果。

在上述两种模型的训练过程中都会涉及在整个词表上的Softmax操作，当词表较大时（通常为几十万或上百万），训练的计算开销非常大。为此，技术人员进一步提出了负样本采样和层级Softmax两种优化方法，在保持词向量质量的同时大幅提升了Word2Vec的训练速度。

尽管Word2Vec在许多任务中表现出色，但其限制在于生成的词向量是上下无关的，即对一个单词，无论它的上下文是什么，它对应的词向量都是固定不变的。然而事实上是，同一个词在不同上下文中所表达的含义往往是有差异的，例如“活动”这个词既可以做名词，也可以做动词，既可以做主语，也可以做谓语，使用固定的词向量难以反映这种细粒度的差异。此外，上下文无关词向量也难以处理一词多义的问题，例如“苹果”这个词在“苹果真好吃”和“最近推出了新款苹果手机”这两个句子中所表达的含义是完全不同的。随着深度学习的发展，技术人员提出了各种基于神经网络的上下文词向量表示方法，这些方法能够根据单词所在的上下文动态生成相应的词向量，从而更好地应对语义变化和一词多义的问题，在各种NLP任务中展现出更为优越的性能。

常见的神经网络语言模型有RNN、LSTM、GRU等。


![image](https://github.com/user-attachments/assets/bc0f25c8-28bb-41e8-b28d-0ff652704a11)


![image](https://github.com/user-attachments/assets/563a06fa-2a40-40b2-868d-da6674d7d794)


#### A.3 预训练语言模型
在认知智能领域，随着诸如词向量、Seq - to - Seq模型及注意力机制等深度学习及应用技术的快速发展，以及预训练语言模型在NLP领域大放异彩，逐渐成为多种NLP任务的标准范式。

NLP技术人员研究发现，通过扩展预训练语言模型（通常扩展模型大小或数据大小）能提高下游任务的模型容量（可以理解为模型预测能力的上限）。因此，许多研究通过训练越来越大的预训练语言模型来探索性能的极限。

预训练语言模型的基本思想是在大量未标记的文本数据上先训练一个具有广泛语言知识的通用模型，然后在特定的下游任务上进行微调，如文本分类、情感分析、问答系统和机器翻译等。

预训练语言模型的训练分为两个阶段。

1. **预训练阶段**：模型通过在大规模的文本语料库上训练来学习语言的通用模式和知识。人们通过预训练模型学习各自领域的专业能力。常见的预训练模型有基于BERT（Bidirectional Encoder Representations from Transformers）的掩码语言模型、基于GPT的自回归语言模型等。

2. **微调阶段**：完成模型预训练之后，接下来将在特定的下游任务上进行模型微调。在微调过程中，模型的所有参数都可以进行更新，但由于模型已经具备了广泛的语言知识，微调所需的数据通常比从零开始训练要少得多，训练时间也相应缩短。除了通用预训练任务外，也有一些针对具体应用需求设计的特殊类型的预训练任务。

预训练语言模型主要包括BERT、GPT、XLNet、RoBERTa、T5等，其中预训练模型BERT和GPT模型区别如下表所示。

|对比项|BERT|GPT|
| ---- | ---- | ---- |
|模型结构|Transformer编码器|Transformer编码器|
|训练策略|掩码语言
