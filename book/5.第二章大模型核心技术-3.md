
### 第2章 大模型核心技术

信息，并且对于不同的Token，它们的注意力焦点也会有所不同，从而实现对序列中不同元素的差异化关注。

5）前馈神经网络对多头自注意力层的输出进行非线性变换和映射，以提供更丰富的特征表示。这一过程通常由一个简单的MLP （多层感知机）实现，该感知机包含两个线性变换和一个非线性激活函数（例如ReLU或GeLU）。值得注意的是，前馈神经网络对每个位置的处理是相同的，但其参数未在不同的位置间共享，这种设计有助于模型捕捉与位置相关的特定特征。

6）在多头自注意力和前馈神经网络的每一步处理之后，都会应用一个残差连接，紧接着是规范化层的处理，即层归一化。残差连接有助于缓解梯度消失问题，并且使得深层网络的训练变得更加可行。层归一化则用于稳定训练过程，确保网络在训练过程中的输入分布的稳定性，从而提高模型的训练效率和性能。

以上描述的步骤在多个相同的层中重复。每一层的输出都成为下一层的输入。

（2）**Transformer解码器的工作流程**

Transformer解码器的具体工作流程如下。

1）掩码多头自注意力层负责对目标语言进行初步处理，并实现Token的序列生成。在此过程中，输入序列被分解为一系列Token，每个Token代表输入文本中的一个词或字符。这些Token随后被映射到模型预训练好的向量矩阵所定义的高维向量空间中的特定点，从而实现将Token转换成对应的固定维度的向量表示，以捕捉各自的语义和上下文信息。之后，通过加入位置编码来融合Token序列的顺序信息，并利用掩码多头自注意力机制来确保在序列生成过程中仅引用已生成的Token。

2）掩码多头自注意力层处理解码器每步的输入，并确保在生成序列的每个步骤中，模型只能看到之前的词而不能看到之后的词。通过在多头自注意力的基础上增加掩码，在训练阶段模拟实际预测场景，解码器一次只能生成一个词。解码器在生成第n个词时，只能看到真实的前n - 1个词。模型在每个时间步都能并行地处理整个需求，加速训练并提高效率。在预测阶段，如在机器翻译或文本生成等实际应用中，当生成序列的某个部分时，目标序列是未知的。模型在生成第n个词时，只能依赖于它之前生成的n - 1个词。这种方式确保了生成的每个新词都是基于所有先前生成的词的上下文，从而维持了语言的连贯性和逻辑性。通过在注意力计算中对未来位置应用一个非常大的负数（或负无穷），使得这些位置的Softmax输出接近于零。即在生成当前词时，只能利用之前的词。同解码器中的每一层一样，掩码多头自注意力层后方也连接有残差连接和规范化层，这有助于避免深层网络中的梯度消失问题，并保持训练的稳定性。 

3）多头自注意力层承接了上一层的输出，并引入了来自编码器的信息。在这个阶段，解码器通过注意力机制关注编码器输出的相关部分，将编码器捕捉的上下文信息融入正在生成的序列中。这一过程是解码器构建输出序列时不可或缺的一步，确保了生成的目标语言既能与原始输入相关联，又能在语法和语义上保持连贯。解码器接收编码器输出的结构如图2-7所示。


![image](https://github.com/user-attachments/assets/b5888730-0f60-4af3-991c-718ca6b6516c)


**图2-7 解码器接收编码器输出的结构**

输入经编码器处理，掩码自注意力输出i用于产生查询q，编码器输出的k和v与q一起计算注意力，得出结果。



具体来说，在这个多头自注意力层中，查询（Query，即q）来源于解码器中掩码多头自注意力的输出Token，而键（Key，即k）和值（Value，即v）则来源于编码器的输出Token。掩码多头自注意力层的输出结果i仅仅用于产生查询q，而键和值则是由编码器的各个输出ai分别成对产生的，其中每一对（ki,vi）都会与q一起用于计算注意力，最终得出i与基于编码器全部输出内容ai的计算结果v。

这种结构使得解码器在生成每个目标语言Token时，每一步都要考虑编码器捕获的整个输入序列的信息（即输入句子中所有相关的Token），从而实现深度的信息融合。这确保了生成的序列不仅在语法和语义上与之前生成的词连贯，而且与原始输入序列紧密关联。与其他层一样，多头自注意力层的输出也需要经过残差连接和规范化处理，以提高模型的稳定性和训练效率。 

4）前馈神经网络层与输出。前馈神经网络层对多头自注意力层的每个Token进行非线性加工和细化处理，并为生成精确且连贯的输出序列提供所需的数据转换。之后将处理结果通过线性层和归一化指数函数层（Softmax）加工，加工后的Token转换为最终的输出。具体来说，线性层的任务是将前馈神经网络层的输出转换为更大的维度空间，通常对应于词汇表的大小（模型能够理解和生成的不同词汇的数量）。归一化指数函数层将线性层的输出结果转换为概率分布，即计算每个词在当前位置出现的概率，并确保所有词的概率总和为1。通过概率分布预测下一个词，即选择概率最高的词作为输出，如图2-8所示。


![image](https://github.com/user-attachments/assets/2b2fc02c-0da2-44f1-911c-f1ebf01ccfd8)


**图2-8 线性层和归一化指数函数层**

左侧经线性层到归一化指数函数层，右侧词汇表及对应概率。


在整个解码过程中，线性层和归一化指数函数层是生成最终序列的关键。它们将解码器中的复杂表示转换为实际的词的输出，使得模型能够产生连贯且有意义的文本序列。这种设计确保了解码器不仅能够处理和理解输入序列，还能有效地生成目标序列。

接下来将句子“我是一名工程师”翻译成英文“I am an engineer”。对于原始输入“我是一名工程师”，它并不会被解码器直接利用，而是会在编码器中进行编码，再将其输出结果传递给解码器。而解码器将会严格地从前到后逐步生成目标翻译结果，每次生成一个词，然后将它作为下一步输入的一部分，具体流程如图2-9所示。


![image](https://github.com/user-attachments/assets/7bd4f1d9-56c5-4931-b448-34a4bda9276b)


**图2-9 机器翻译的解码流程**

步骤1：[begin] 经解码器生成I；步骤2：[begin]I经解码器生成am；步骤3：[begin]I am经解码器生成an；步骤4：[begin]I am an经解码器生成engineer；步骤5：[begin]I am an engineer经解码器生成[end]；结果：[begin]I am an engineer[end] 。



流程描述如下。

①因为解码器的首轮将不会有任何信息输入，所以设定一个起始符[begin]，首先由起始符经过解码器模型生成I。

②再将I添至起始符后方作为新的输入[begin]I，经过解码器模型生成am。

③随后将am添至I的后方作为新的输入[begin]I am，经过解码器模型生成an。

④再将an添至I am的后方作为新的输入[begin]I am an，经过解码器模型生成engineer。

⑤此时，目标翻译序列已经产生完毕，需要“告知”解码器模型生成结束，人为设定一个终止符[end]，此时的输入将会是“[begin]I am an engineer”，经过解码器之后将会产生终止符[end]。

⑥至此，翻译完成。



2. **因果解码器**

因果解码器（Causal Decoder）架构采用单向注意力掩码，确保每个输入Token只能关注过去的Token和自身。输入和输出Token在解码器中以相同的方式进行处理。GPT系列模型就是基于因果解码器架构开发的代表性语言模型，特别是GPT-3和GPT-4，由于其惊人的能力，在大众中引起了广泛关注。

因果解码器架构广泛应用在各种主流的大模型的基础结构中，例如OPT、BLOOM和Gopher。



3. **前缀解码器**

前缀解码器（Prefix Decoder）修正了因果解码器的掩码机制，从而能够对前缀Token执行双向注意力处理，并仅对生成的Token采用单向注意力处理。也就是说，前缀解码器能够双向编码前缀序列并自回归地逐个预测输出。

因为编码和解码过程中会共享相同的参数，所以建议不要从头开始进行预训练，而是继续训练因果解码器，然后将它转换为前缀解码器以加速收敛。采用前缀解码器的代表性大模型包括GLM-130B和U-PaLM。

注意，因果解码器和前缀解码器都属于仅解码器（Decoder-only）架构。（本书提到仅解码器架构时，主要是指因果解码器架构。）



### 2.2.3 注意力机制

注意力机制的提出最初是受到人类感知过程的启发。2014年，Google Mind团队发表的研究“Recurrent Models of Visual Attention”将该机制引入视觉领域。人类倾向于选择性地关注场景中的关键部分，而非一次性地处理全部信息。这一机制被应用到模型中，使模型能够专注于输入数据中的重要部分。

在NLP领域，注意力机制起初被用于解决序列标注问题，例如词性标注。词性标注任务要求为文本中的每个单词分配正确的词性标签，如名词、动词、形容词等。自然语言往往是复杂多义的，具有词义多义性。注意力机制使得模型能够从整个序列中筛选出重要的信息，并更好地处理词义多义性和上下文依赖，提高了模型处理序列数据的灵活性和效率。

注意力机制通过调整信息的权重来平衡其重要性，这意味着即使是被赋予较低权重的信息，也会在一定程度上影响最终的输出结果。这种机制增强了模型处理序列数据的能力，特别是在涉及理解上下文或处理长距离依赖的任务中，如机器翻译、文本摘要和问答系统。

在注意力机制出现之前，虽然CNN和RNN等模型已经尝试解决上下文理解问题，但由于计算能力和模型复杂性的限制，效果有限。注意力机制能够有效处理大量信息中的关键部分，减少冗余信息的影响，提高模型效率。



**提示**：CNN是一种深度学习架构，特别擅长处理具有网格状拓扑结构的数据，如图像。其工作原理主要涉及以下几个关键要素。

1）卷积层：通过滤波器（或称为卷积核）在输入数据上滑动，提取局部特征。每个滤波器负责识别一种特定的特征，如边缘、角点或纹理。

2）激活函数：通常使用ReLU（Rectified Linear Unit，修正线性单元），它将卷积层的输出转换为非线性，增加模型的复杂性和表达能力。 

3）池化层：进行下采样，减少数据的空间尺寸，同时增加对小范围位移变化的不变性，有助于减少计算量和防止过拟合。 

4）全连接层：将前面层的特征映射到最终的输出，如分类标签的概率。

CNN的特性包括以下几点。

1）特征自动提取：无须手动设计特征提取器，网络会自动学习数据中的特征。

2）多尺度和多方向识别：通过不同大小和方向的滤波器，能够识别不同尺度和方向的特征。 

3）空间不变性：通过池化层，网络对输入数据的小的位移变化具有不变性，增强了模型的泛化能力。 

4）减少全连接层参数：通过卷积和池化，网络在进入全连接层之前显著减少了参数的数量，降低了过拟合的风险。

这些特性使得CNN在图像和视频识别、自然语言处理、医学图像分析等领域表现出色。



Transformer模型通过注意力机制构建了特殊的注意力层，使得模型能够重点关注句子中的关键词。接下来将重点区分讲解自注意力、缩放点积注意力和多头自注意力。



1. **自注意力**

自注意力作为注意力机制的一个关键分支，专门用于在深度学习模型中处理序列数据，尤其在像Transformer这样的架构中发挥核心作用。其创新之处在于，能够让模型中的每个元素不仅关注其直接邻居，还能审视整个输入序列，以捕捉和探索元素间的复杂依赖关系。

在自注意力机制中，每个输入元素被转化为三个关键向量：查询、键和值。查询向量定义了元素所需匹配的信息类型；键向量代表了序列中各个位置可以提供的信息；值向量则对应位置的具体数据。通过点积操作来量化查询与所有键向量之间的相似度，模型能够计算出当前元素对各值向量的注意力权重，反映它们对当前元素的相关性和重要性。

基于计算出的权重，值向量被加权求和，生成针对当前元素的上下文向量。这个过程在序列中的所有元素上重复执行，最终构建出富含全局上下文信息的新序列表示。

自注意力在并行化计算、捕捉长距离依赖、动态适应性等方面展现了显著优势。

1）并行化计算：与RNN的序列化处理不同，自注意力机制支持并行计算，极大地加速了训练过程。 


