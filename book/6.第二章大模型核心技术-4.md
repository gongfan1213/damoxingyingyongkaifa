2）**长距离依赖捕捉**：自注意力能够有效识别并利用序列中远距离元素间的关系，这对理解和解析语义及语法结构至关重要。

3）**动态适应性**：自注意力兼容不同长度的输入序列，并且可以通过堆叠多层注意力模块来增强模型的理解力。

尤其值得注意的是，自注意力机制的核心在于它能够聚焦于输入序列内部元素间的内在联系，将序列的不同位置通过计算注意力权重紧密相连，从而确保每个元素的表示能够根据整个输入序列进行动态调整。在将输入序列转化为输出序列的过程中，自注意力机制确保了每个元素的转换是基于对整个输入序列的全面考虑，同时避免了对所有输入元素进行平均的无重点处理。通过学习过程，模型会自动为每个输入向量分配权重，权重大小反映了对应“值”向量的重要性。权重较高的向量会获得模型更多的关注。因此，每个输出向量实际上整合了整个输入序列的信息，同时又突出其中的关键部分。这种机制有助于模型高效捕捉和利用输入数据中的深层特征。



2. **缩放点积注意力**

在Transformer模型中，缩放点积注意力函数的总体结构如图2-10所示。


![image](https://github.com/user-attachments/assets/d4374f19-8a7e-42d8-84bc-3cb8cf1010b7)


**图2-10 缩放点积注意力函数的总体结构**

Q、K、V经MatMul(Q,K)、Scale、Mask(可选)、Softmax、MatMul(Softmax,V) 得到最终结果。


![image](https://github.com/user-attachments/assets/23864b3d-b8c4-44ad-8ebd-660d0b8ad6ed)


缩放点积注意力的计算公式如下：

\[ Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]

其中，Q矩阵、K矩阵、V矩阵是指查询、键、值是在原始输入向量序列（经过位置编码之后）与三个不同的神经网络参数（\(W^q\)、\(W^k\)、\(W^v\)）相乘得到的向量序列（参见图2-11），这些向量序列是进行缩放点积注意力计算的关键，而\(d_k\)是指键向量维度。


**注意**：缩放点积注意力使用两个向量的点积来表示相似度，当Q矩阵（查询）和K矩阵（键）完全无关的时候，两个向量垂直，点积为0；当Q矩阵和K矩阵完全相关时，两个向量同向，那么点积为1。

在实际处理过程中，对输入向量序列的处理并非依次单独进行的，而是将其拼接成矩阵I，通过矩阵并行运算处理以追求效率。而自注意力的Q矩阵、K矩阵、V矩阵都是在原本的矩阵I的基础上，通过各自不同的矩阵变换产生，用于计算向量序列之间的各部分注意力，如图2-11所示。


![image](https://github.com/user-attachments/assets/fd278eaf-21f0-4944-aedc-99b6b9c282d0)


![image](https://github.com/user-attachments/assets/a7dc9807-dd24-4e1f-b2cc-e6ea4ec92976)


**图2-11 Q矩阵、K矩阵、V矩阵处理过程**

\(Q = I \cdot W^q\) ，\(K = I \cdot W^k\) ，\(V = I \cdot W^v\)  。

其中，\(W^q\)、\(W^k\)、\(W^v\)是神经网络参数，随机初始化后在模型训练时更新，与输入向量的拼接矩阵I点乘后得到Q矩阵、K矩阵、V矩阵。

**缩放点积注意力函数的具体计算过程如下**。

1）**MatMul(Q, K)**：将Q矩阵和K矩阵通过矩阵乘法（MatMul）操作进行点积计算（\(QK^T\)），计算出不同查询与所有键的相似度，得到相似度矩阵A。如图2-12所示，通过计算查询和每个键之间的点积，度量查询和每个键之间的方向相似性。如果查询和某个键的点积值较大，这表明查询和这个键在向量空间中的方向更接近，它们之间的相似性更高。


![image](https://github.com/user-attachments/assets/62f65c6f-0533-42d9-bc2a-bfdcee145a68)


![image](https://github.com/user-attachments/assets/3d16bc9e-484a-4bd7-b64f-a8b030acbd5f)


**图2-12 通过点积得到相似度矩阵A**

\(A = Q \cdot K^T\) 

通过点积计算得到的相似度分数会被用来为相应的值（V）分配权重，这样在计算加权和生成注意力的输出时，更相似的（即更重要的）值就会有更大的影响。

2）**Scale**：对得到的点积结果进行缩放，避免点积的数值非常大时导致Softmax函数的梯度消失。对于输入向量\(x\)中的每个元素\(x_i\)，Softmax的计算公式如下：

\[ Softmax(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]

其中，\(i\)，\(j\)是元素在输入向量中的位置编号，从1开始，不超过向量\(x\)的长度。

因为梯度是通过导数来计算的，而当Softmax函数的输出接近0或1时，Softmax函数的导数接近0，它的梯度可能会非常小。这意味着反向传播过程中几乎没有梯度信号被传递到网络，因此权重更新非常微小，几乎没有学习发生，就会造成“梯度消失” 问题。


通常通过除以键向量维度的平方根（即除以\(\sqrt{d_k}\)）来缩放点积结果，这样做可以减少点积计算后数值过大，导致梯度消失的问题，从而确保在计算Softmax函数时数值稳定，梯度正常传播，促进模型的学习和收敛。 

3）**Mask（可选）**：可选的掩码操作可以将不应被注意的位置设置为一个非常大的负数（在Softmax函数之前），这样经过Softmax函数处理后，这些位置的权重会接近于零，从而避免模型关注这些位置。这种操作可以防止在某些情况下（例如解码时）发生信息泄露。 

4）**Softmax**：对缩放（以及可能掩码）后的查询（Q）和键（K）之间的点积结果应用Softmax函数，将其转换为概率分布，这些概率代表了各个值（V）在注意力加权中的权重，表明查询对各个键的关注程度，也就是通过相似度来得到权重。

在应用Softmax函数之前，查询和键的点积（或者经过缩放的点积）给出了一个相似度分数。这个分数可能在任意范围变化，但它并不代表有效的概率。Softmax函数将任何实数向量转换为一个概率分布。具体来说，它对每个元素执行指数运算，然后将这些指数值归一化，使得它们的和为1。在计算最终的注意力加权输出时，每个值（V）都会被它相应的概率权重加权，最终得到权重矩阵\(A'\)。 

5）**MatMul(Softmax, V)**：将Softmax输出的权重矩阵\(A'\)，与值（V）进行矩阵乘法（MatMul）操作，即\(Attention(Q, K, V)\)具体操作的最后一步，得到注意力加权后的向量矩阵B，如图2-13所示。


![image](https://github.com/user-attachments/assets/ff0bf7c4-344d-494a-aada-ba62c315c9b3)


![image](https://github.com/user-attachments/assets/f7ab8bbc-df64-455b-b6ea-15909034aad3)


**图2-13 形成最终输出向量矩阵**

\(B = A' \cdot V\) 

缩放点积注意力是对自注意力机制的一种改进，优化了模型对原向量的理解和处理方式，将原向量转换为具有丰富上下文信息的新向量。该过程的输出是每个查询对各个值的加权和，这使得模型能够集中关注输入中的重要部分。这种机制在许多NLP任务中非常有用，尤其适用需要模型理解序列中不同部分之间复杂关系的场景。

3. **多头自注意力**

多头自注意力机制是注意力机制的一种扩展，也是Transformer模型中的关键组成部分。在该机制中，模型通过将注意力集中于输入序列的某些部分，创建多个注意力“头”。这些“头”是通过将原始的查询、键、值分别投影到多个不同的表示子空间中生成的，每个“头”独立地执行注意力操作，关注输入数据的不同方面。这样可以增加模型的复杂性和表达能力，同时捕捉不同类型的信息。最终，这些头的输出会在通道维度上连接，以增强模型的表示能力。

多头自注意力机制的公式如下：

\[ MultiHeadAttention(Q, K, V) = Concat(head_1, \cdots, head_h)W^O \]

其中，\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)表示第\(i\)（\(1 < i < h\)）个注意力头的输出，每个“头”（head）都对原输入Q、K、V矩阵进行各自独立的注意力操作，对不同的\(i\)进行不同权重矩阵的变换（即\(W_i\)各不相同的变换），计算方式与2.2.3节一致，最后得到向量矩阵\(B\)即为\(head_i\)的结果。多个注意力头的计算完成后，使用Concat进行连接操作，将所有注意力头的输出在通道维度上拼接起来。\(W^O\)是一个可学习的权重矩阵，用于对拼接后的多维输出进行进一步的变换。具体到每个注意力头的计算，通常使用点积注意力或缩放点积注意力机制。

多头自注意力允许模型同时进行多个注意力操作，通过并行运行多个注意力头来扩展注意力机制。这种方式使得Transformer模型能够高效地处理和整合来自序列不同部分的信息，特别是在处理长距离依赖关系和复杂数据结构时表现突出。多头自注意力不仅提高了计算效率，还增强了模型处理复杂关系的能力。


### 2.2.4 词向量

词向量是用来表示词的向量，把词汇转化为数值向量的技术也叫词嵌入（Word Embedding）。词向量可以捕捉词汇之间的语义和语法关系。

**词向量的关键特性如下**。

1）**分布式表示**：每个词被表示为一个实数向量，向量的维度远小于词汇表的大小。这种表示方式允许词向量捕捉词与词之间的细微差别和关系。

2）**语义相似性**：在词向量空间中，语义上相近的词往往彼此靠近。这意味着可以通过计算向量之间的距离或相似度来衡量词之间的语义相似性。

3）**线性代数操作**：词向量支持向量加法和减法，可以用来执行有意义的算术运算，例如著名的例子：“国王 - 男人 + 女人 = 女王”。 

4）**维度降低**：词向量通过降维的方式将高维的词汇空间映射到低维的连续向量空间，这有助于减少计算成本并提高模型效率。



**词向量的生成方法包括但不限于以下几种**。

1）**独热编码（One - Hot Encoding）**：这是一种简单的编码方式，但不是真正的词向量，因为它不能编码词与词之间的关系，只表示词存在与否。

2）**BoW（Bag of Words，词袋）**：同样不形成词向量，而是统计文档中每个词的频率，并且会忽略词序。 

3）**TF - IDF**：改进的BoW模型，通过词频 - 逆文档频率来反映词的重要性，但仍不捕捉词间关系。 

4）**Word2Vec**：由Google提出的一种流行算法，包括连续词袋（CBOW）和跳字（Skip - Gram）两种模型，能生成高质量的词向量。 

5）**GloVe（Global Vectors for Word Representation）**：另一种流行的词向量生成方法，可以结合全局矩阵因子分解和局部上下文窗口信息。 

6）**FastText**：Facebook AI Research提出的模型，能够处理子词信息，对于罕见词和拼写变体表 现更好。 

7）**BERT（Bidirectional Encoder Representations from Transformers）**：基于Transformer架构的深度学习模型，能够生成上下文敏感的词向量。

### 2.2.5 位置编码

注意力机制强调了“单元”的关键信息和上下文关联，但未直接关注序列中“单元”的位置信息。例如，在处理文本时，注意力机制本身不区分词汇的顺序，这可能导致模型忽略词序带来的语义差异。以翻译任务为例，“[我][吃][苹果]”和“[苹果][吃][我]”这两个句子在注意力机制看来是无差别的，但实际语义差异非常大。因此，在处理序列数据时位置信息至关重要。

在Transformer模型出现之前，RNN是处理序列数据的主流技术，它按序列顺序逐一处理每个元素，如图2-14所示。


![image](https://github.com/user-attachments/assets/d065aa65-65e3-4701-9ea2-a3865e5d6b12)


**图2-14 RNN按序列顺序处理每个“单元”**

输入层\(x_0,x_1,\cdots,x_t\) 经权重矩阵U到隐藏层\(h_0,h_1,\cdots,h_t\) ，再经权重矩阵V到输出层\(y_0,y_1,\cdots,y_t\)  。


![image](https://github.com/user-attachments/assets/564509e5-282f-4fec-8dc0-1659861991b5)


在RNN中，每个元素的前后关联信息是通过其隐藏状态进行传递的，这种机制自然而然地在处理过程中体现了序列的顺序性。具体来说，假设有一个输入序列\(x_0, x_1, \cdots, x_T\)，RNN会在每个时间步\(t\)接收当前的输入\(x_t\)和前一时间步的隐藏状态\(h_{t - 1}\)，然后计算出当前的隐藏状态\(h_t\)，输出的状态向量\(y_t\)，其中\(U\)是输入层到隐藏层的权重矩阵，\(V\)是隐藏层到输出层的权重矩阵，\(W\)是隐藏层的权重矩阵。

相比之下，Transformer架构颠覆了RNN的概念，不再依赖隐藏状态来建立序列内的顺序关系。相反，它引入了位置编码的概念，显式地编码每个元素在其所属序列中的位置信息。这样，Transformer在处理序列时能够同时考虑内容和位置，从而在无须依赖循环结构的情况下准确捕捉元素间的顺序依赖关系。

由于Transformer中的自注意力模块和位置前馈层是置换不变的，因此Transformer使用位置嵌入（Position Embedding，PE）来注入绝对或相对位置信息以建模序列。位置嵌入形式包括绝对位置嵌入、相对位置嵌入和旋转位置嵌入。


![image](https://github.com/user-attachments/assets/1ef571cb-361f-4ab9-8de4-2c581e79c1f4)


1. **绝对位置嵌入**

传统的Transformer中使用了绝对位置嵌入。在编码器和解码器的底部，将绝对位置嵌入添加到输入向量中，这包括正弦位置嵌入和学习位置嵌入两种变体。其中，正弦位置嵌入通过正弦和余弦函数生成位置编码，而学习位置嵌入则是通过模型训练来学习每个位置的嵌入，后者通常在现有的预训练语言模型中使用。

（1）**正弦位置嵌入**

正弦位置嵌入仅仅依赖于自己的索引位置，计算公式如下所示：

\[ PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right) \]

\[ PE(pos, 2i + 1) = \cos\left(\frac{pos}{10000^{2i/d}}\right) \]

其中，pos表示某个“单元”在序列中的位置，\(pos \in [0, n]\)，\(n\)是一个序列的最大长度，即位置序列的最大数量；\(i\)表示维度索引，\(d\)表示嵌入向量的维度，即每个“单元”的编码长度。上述公式生成的编码在不同位置和不同维度之间有不同的频率，从而使模型能够区分位置。这样，固定位置编码就可以为任何模型的任意位置提供一个编码信息。那么，\(PE(pos, j)\)就是pos位置单元的位置编码，其中\(j \in [0, d]\)，即：

\[ PE(pos) = [PE(pos, 0), PE(pos, 1), \cdots, PE(pos, d - 1)] \]

对于每个\(pos \in [0, n]\)的“单元”，都可以得到一个维度为\(d\)的向量\(PE(pos)\)，这个长度为\(d\)的向量长度同“单元”本身的嵌入向量尺寸完全一致。直接在嵌入向量的基础上加上位置编码，就可以得到最终的表示。



来看一个示例，生成一个长度为50、嵌入维度为512的位置信息，具体代码如下。

```python

import numpy as np

import torch


def get_positional_encoding(max_len, d):
    pe = np.zeros((max_len, d))
    position = np.arange(0, max_len).reshape(-1, 1)
    div_term = np.exp(np.arange(0, d, 2) * -(np.log(10000.0) / d))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return torch.tensor(pe, dtype=torch.float32)

# 示例：生成长度为50，嵌入维度为512的位置信息
pos_encoding = get_positional_encoding(50, 512)
print(pos_encoding)
```
（2）**学习位置嵌入**

学习位置嵌入是模型的一部分，它会随着训练过程发生更新。一种简单的可学习位置模块就是对每个位置\(pos \in [0, n]\)构造一个\(1 \times d\)的向量，那么每个位置的向量就可以构成一个\(n \times d\)的矩阵，该矩阵在模型训练过程中进行参数更新，即位置编码可在训练过程中能够隐式地学习到。在具体实现中，也可以先给定位置编码的初始值，然后在训练过程中进一步调整这些位置编码。

2. **相对位置嵌入**

与绝对位置嵌入不同，相对位置嵌入依赖于编码位置的相对距离。相对位置嵌入是根据键和查询之间的偏移量生成的。虽然没有一个通用且广泛接受的标准公式，但这些方法通常都考虑了每个“单元对”之间的位置关系。

例如，Transformer - XL（一种扩展的Transformer）中引入了一种流行的相对位置嵌入的变体。该变体对注意力分数的计算方式进行了修改，引入了与相对位置对应的可学习嵌入。T5模型又进一步简化了相对位置嵌入的方法，随后被Gopher采纳。具体而言，相对位置嵌入在注意力分数中添加了可学习的标量，这些标量




![image](https://github.com/user-attachments/assets/092d998f-e9c3-486b-8883-2a25df5a555f)
