不同领域之间的指令微调数据集可能存在着一些区别，同时，在这些领域中也存在着专门的框架对大模型进行微调。表2-4所示为8个特定领域下的指令微调数据集构建方案示例。

**表2-4 8个特定领域下的指令微调数据集构建方案示例**

|领域|解决方案|微调大模型|基础模型|参数量|训练集大小|
| ---- | ---- | ---- | ---- | ---- | ---- |
|对话|构建长序列对话指令集，扩大编码长度|InstructDial|T0|3B| - |
|意图分类|构建跨领域的意图分类和槽填充指令集|LINGUIST|AlexaTM|5B|1.3万|
|信息抽取|构建规范化信息抽取指令数据集|InstructUIE|FlanT5|11B|100万|
|情感分析|将其转换为问答式指令|IT-MTL|T5|220M| - |
|写作|构建具有多种风格的写作指令数据集|Writing-Alpaca-7B|LLaMA|7B| - |
|||CoEdIT|FlanT5|11B| - |
|||CoPoet|T5|11B| - |
|医学|在医学知识图谱上进行指令调优|Radiology-GPT|Alpaca|7B|12.2万|
|||ChatDoctor|LLaMA|7B|10万|
|||ChatGLM-Med|ChatGLM|6B| - |
|算术|构建多样化算术表达式指令数据集|Goat|LLaMA|7B|100万|
|代码|收集多语言代码生成指令样例|WizardCoder|StarCoder|15B|7.8万|


#### 2.3.2 调优策略

高效参数微调的精进不仅依赖于高品质的指令微调数据集，同时需要使用高效率的微调策略。当前，指令微调策略大致可归为三类：加法型策略、规范化策略以及重新参数化策略。

1）**加法型策略**通过添加模型原先未包含的可训练参数来实现，典型代表包括适配器调优（Adapter Tuning）、前缀调优（Prefix-Tuning）、提示词调优（Prompt-Tuning）和P调优（P-Tuning）等，它们允许模型在保留基础架构的同时，通过附加组件来适应特定领域任务。 

2）**规范化策略**则锁定模型的某些参数不变，专攻其余参数的优化，典型代表包括BitFit调优，它专门针对预训练模型中的偏置项（Bias Term）进行微调，确保模型的主干结构得以保存。 

3）**重新参数化策略**致力于将模型的权重转换至更经济的参数维度，核心理念在于将权重维度降低，从而缩减待优化参数的数量。典型代表为LoRA（Low-Rank Adaptive，低秩适应）微调，它通过调整模型的权重矩阵，使模型具有较低的秩（即较小的特征空间），从而减少其参数数量和计算复杂度。这种方法通常应用于大型神经网络，特别是在资源受限的设备上进行部署时，因此LoRA技术早在大模型微调前就在多种其他模型微调中被使用。例如，经典的文本图模型Stable Diffusion的多数用户都在训练时使用LoRA，以便更好地生成特定场景或人物的图片。此外，这类策略还包括本征提示微调（Intrinsic Prompt-Tuning，IPT），它能够识别并利用跨任务优化提示所共享的低维子空间，实现高效学习。


下面介绍在大模型微调中最常用的几种调优技术。

**1. 适配器调优**

大模型引入Adapter模块，通过适配器微调减少了微调过程中可训练参数的数量，通过AdapterFusion（适配器融合）两阶段学习算法融合不同任务中的Adapter参数，通过Adapter Drop动态且有效地移除部分Adapter参数，提升模型在训练和推理时的工作效率。


（1）**Adapter Tuning**

Adapter的概念最初在计算机视觉领域的论文“Learning multiple visual domains with residual adapters”中被提出，其核心思想是在网络结构中嵌入残差模块，并仅针对这些模块进行优化。由于残差模块的参数数量较少，因此微调的成本显著降低。Houlsby等人在“Parameter-Efficient Transfer Learning for NLP”一文中将这一概念引入NLP领域，提出在Transformer的注意力层和前馈神经网络层后附加全连接层。在微调过程中，仅对新增的Adapter结构和层归一化进行调整，以保证训练效率。当面临新的特定领域任务时，通过引入Adapter模块，以便捷地扩展模型，从而避免了全面微调可能导致的灾难性遗忘问题。

适配器微调技术通过将小型神经网络模块（Adapter）嵌入Transformer模型中，实现了模型的灵活调整。Adapter模块被整合到每一个Transformer层，通常在注意力层和前馈神经网络层这两个核心部分之后进行串行插入。此外，也可以在Transformer层中采用并行适配器的方式，将两个适配器模块与注意力层和前馈神经网络层相应地并行放置，以增强模型的适应性和效率。

在微调过程中，Adapter模块将根据特定领域任务目标进行优化，而原始语言模型的参数在这个过程中被冻结。通过这种方式，可以有效地减少微调过程中可训练参数的数量。

适配器微调的效率极高，通过调整少量（通常不到4%）的模型参数，便能达到与完全微调相媲美的性能水平。适配器微调模型架构如图2-15所示。


![image](https://github.com/user-attachments/assets/81e95d1c-6e4f-4142-a079-50d4919b379f)


- 图2-15a展示了在每个Transformer层中添加Adapter模块的位置，分别位于多头自注意力层以及两个前馈神经网络层之后；
- 图2-15b描述了Adapter模块的架构，包含两个前馈神经网络层和跳跃连接，该架构首先通过前馈降维（Feedforward Down-project，FFDown）将原始输入的高维特征向量d压缩为一个更小的低维特征r来限制Adapter模块的参数量。一般情况下，r远小于d。接着进行非线性转换，然后通过前馈升维（Feedforward Up-project，FFUp）将低维特征r恢复到原始的高维特征d，作为Adapter模块的输出。同时，通过跳跃连接将Adapter模块的输入再次加入最终的输出中（形成残差连接）。

**图2-15 适配器微调模型架构图**

a) Transformer层中Adapter模块的位置 ：Transformer层依次经过层归一化、Adapter、2×前馈神经网络、层归一化、Adapter、前馈神经网络、多头注意力。

b) Adapter架构 ：Adapter层包含d维前馈神经网络、层归一化、层归一化、r维层归一化、d维前馈神经网络，通过跳跃连接和前馈降维、升维操作构成。


（2）**AdapterFusion**

为了有效捕捉并融合多种任务中的知识，通常采用顺序式微调或并行的多任务学习策略。虽然顺序式微调方法操作简便，但它依赖于预设的任务顺序，且模型在学习新任务时容易丧失对旧知识的记忆。多任务学习则尝试同时处理多个任务，但这种方法常常面临任务间干扰和数据集规模不一的平衡难题，需要精心设计以确保各任务间的有效协作和保持较高的学习效率。

适配器微调技术通过向预训练模型中添加少量新参数，即Adapter参数，实现了对单一任务的高效学习，这些新增的Adapter参数成为特定领域任务知识的载体。在此基础上，AdapterFusion技术提出采用两阶段学习算法，巧妙地融合了不同任务中的Adapter参数，为利用跨任务知识开辟了一条全新的道路，进一步提升了大模型的适应性和学习效率。

AdapterFusion是一种融合多任务信息的Adapter的变体，在Adapter的基础上进行优化，通过将学习过程分为两阶段来提升特定领域任务的表现。

- **知识提取阶段**：在不同任务下引入各自的Adapter模块，用于学习特定领域任务的信息。

- **知识组合阶段**：将预训练模型参数与特定领域任务的Adapter参数固定，引入变体数AdapterFusion来学习组合多个Adapter中的知识，以提高模型在目标任务中的表现。

AdapterFusion的架构图如图2-16所示，其中图2-16a是在Transformer中使用AdapterFusion进行两阶段训练的过程。


![image](https://github.com/user-attachments/assets/ea426014-e0e9-4a4e-bd05-c0f9ae90ed06)


**图2-16 AdapterFusion架构**

a) AdapterFusion进行两阶段训练的过程 ：包含残差连接与规范化、AdapterFusion、Adapter、残差连接与规范化、前馈神经网络、残差连接与规范化、多头注意力。

b) AdapterFusion架构 ：包含残差连接与规范化、AdapterFusion（内有Softmax及Q、K、V矩阵运算相关结构）、Adapter（内有前馈升维、前馈降维等结构）、残差连接与规范化。


在第一阶段的训练中，采用了两种不同的方法。

- **单任务适配器（ST-A）**：在这种方法中，针对N个不同的任务，模型会对每个任务进行独立的优化，确保它们各自的训练过程互不干扰，互不影响。

- **多任务适配器（MT-A）**：在这个框架下，N个任务通过多任务学习的策略实现联合优化。


在第二阶段的组合中，为了防止引入特定领域任务参数可能导致的灾难性遗忘问题，采用了一种共享多任务信息的架构。对特定的任务m，AdapterFusion综合利用了第一阶段中训练得到的N个Adapter信息。在这个过程中，模型的参数以及N个适配器的参数保持不变，同时引入名为AdapterFusion的参数。目标函数的设计旨在学习特定领域任务m的AdapterFusion参数，以达到最优效果。

图2-16b展示了AdapterFusion架构的细节。AdapterFusion架构的核心在于一个基于多头注意力机制的结构，它在Transformer的每一层中都有所体现。具体而言，该架构由三个关键的矩阵参数组成，分别是Q、K和V。其中，Q是由Transformer中每个小模块处理完数据后得到的输出结果，而K和V则来源于N个不同任务中各自Adapter的输出。通过AdapterFusion机制，模型能够为不同任务的Adapter分配适当的权重，整合多任务的信息，从而为特定领域任务生成更加精准的输出。针对每一个Adapter，适配器微调的前馈降维将原始特征向量压缩到一个更小的维度，接着进行非线性转换，然后通过前馈升维恢复到原始维度，确保了信息的有效传递和处理。


（3）**Adapter Drop**

Adapter Drop是一种用于适配器微调的方法，其思路是通过适配器层的随机丢弃机制，实现动态的适配器选择和微调。

Adapter Drop的架构图如图2-17所示，Adapter Drop技术能够在不牺牲任务处理效果的前提下，动态且有效地移除部分Adapter，从而大幅减少模型的参数数量，显著提升模型在学习（训练）反向传播和预测（推理）正向传播时的工作效率。这一策略通过精简模型结构，优化了计算资源的分配，使得模型在保持高性能的同时，更加高效和轻量化。


![image](https://github.com/user-attachments/assets/490a2f21-30c7-4d77-a583-6d58fb5d3fd9)



**图2-17 Adapter Drop架构**

左侧为Adapter Drop架构，包含残差连接与规范化、前馈升维、前馈降维、残差连接与规范化、前馈神经网络、残差连接与规范化、多头注意力。右侧为两个对比结构，展示了注意力头、词嵌入等组件在正向传播和反向传播中的情况，体现了Adapter Drop对模型结构的优化。



实验结果表明，Adapter Drop技术对AdapterFusion中的Adapter进行了适当的裁剪。即使移除了AdapterFusion中大多数的Adapter，只保留两个，也能达到与原本配置了8个Adapter的完整模型的相似效果，并且推理速度还能得到大幅度提升。



**2. 前缀调优**

在前缀调优（Prefix-Tuning）技术出现之前，模板的构建通常依赖于手工设计或自动搜寻。在手工设计模板时，即使是对词汇的微小增减或位置调整，都可能对模型的最终效果产生显著影响，这要求设计者具备高度的专业性，能进行细致的调整。自动搜寻模板虽然能够省去烦琐的人工设计，但这一过程往往耗费大量的资源，且搜寻出的固定模板可能并非最优选择，限制了模型的灵活性和性能优化空间。

传统的调优方式是在预训练模型的基础上，针对不同的特定领域任务进行个别的微调，这种方式意味着每个任务都需要单独保存一套调优过的模型参数。这不仅耗费时间，还需要占用大量的存储空间，增加了模型管理和维护的复杂性。因此，寻找一种既能有效适应不同任务需求，又能减少资源消耗的调优策略，成为研究的重点。

针对这些问题，前缀调优提出了一个新的思路，固定预训练语言模型，并为其添加可训练、任务特定的前缀。这样就可以为不同的任务保存不同的前缀，大大减少了微调所需的成本。

传统的模型微调与前缀调优区别如图2-18所示。前缀调优的前缀实际上是连续可微的虚拟Token，与传统的离散Token相比，它们更容易进行优化，而且效果更佳。


![image](https://github.com/user-attachments/assets/9a8949eb-b874-4512-8b92-4540ed3e5704)


**图2-18 模型微调与前缀调优区别**

上方为模型微调，有多个Transformer（翻译、总结、表格转文本等）分别处理输入“ How are you today ? [SEP]”得到不同输出。下方为前缀调优，不同Prefix（翻译、总结、表格转文本等）结合Transformer（表格转文本）处理相同输入得到输出。



前缀调优是在语言模型的每个Transformer层前添加一系列前缀，这些前缀是一组可训练的连续向量。在微调模型的过程中，只更新训练前缀部分的参数，而预训练语言模型中的其他参数则保持不变，因此可以实现基于参数的高效模型优化。


这些前缀向量是任务特定的，可以被视为虚拟的Token嵌入。可以通过学习一个MLP（多层感知机）函数来优化前缀向量。该函数将一个较小的矩阵映射到前缀的参数矩阵，而不是直接优化前缀。事实证明，这种技巧对稳定训练是有用的。优化后，映射函数将被丢弃，只保留派生的前缀向量以增强任务特定的性能。

根据不同的模型结构，前缀调优需要设计不同形式的前缀，如图2-19所示。对自回归模型（Autoregressive Model）来说，会在句子的开头添加一个前缀，形成z=[Prefix; x; y]的结构。恰当的前缀可以在不改变语言模型本身的情况下，引导模型生成符合前缀意图或任务需求的下文（例如GPT的上下文学习能力）。对编码器 - 解码器结构的模型来说，编码器和解码器的部分都会添加前缀，形成z=[Prefix; x; Prefix'; y]的结构。在编码器端添加前缀的目的是指导输入部分的编码过程，在解码器端添加前缀则是为了引导后续Token的生成过程。

高效微调技术通过创建一系列包含具体指令及其预期结果的样本，采用监督学习的方法对预训练的大模型进行针对性调整。因为微调过程中使用指令集，因此也称为指令微调。这种方法能够使模型更好地理解和执行特定任务，提高大模型在实际应用中的准确性和效率。通过精心设计的指令样本，大模型能够学习到如何根据给定的指令生成预期的输出，从而在各种任务中展现出更优的性能。



**图2-19 前缀调优示例**

包括自回归模型和编码器 - 解码器模型示例，以及总结示例、表格转文本示例。自回归模型和编码器 - 解码器模型示例展示了Prefix层、激活索引等内容；总结示例为关于大脑感知与饮食失调关系的文章及总结；表格转文本示例包含表格信息和文本描述。 


![image](https://github.com/user-attachments/assets/8997c6ac-12ce-4bbd-8a1a-8584d06fdc4e)


