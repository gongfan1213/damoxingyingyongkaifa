相比人为制定、固定不变且参数无法更新的“显式”提示，前缀更像是一种可以通过学习不断进步的“隐式”提示。

### 3. 提示词调优
#### （1）提示词调优分类与适用场景
提示词调优（Prompt-tuning）是直接在输入前添加前缀提示。根据提示信息的类型和表达方式，可以将其分为硬提示（Hard Prompt）和软提示（Soft Prompt）两种。
- **硬提示**：是一种固定的提示模板，通过将特定的关键词或短语直接嵌入到文本中，引导模型生成符合要求的文本。这种提示方式简单直接，但缺乏灵活性，不能根据不同的任务和需求进行调整，因此其泛化能力相对较弱。
- **软提示**：通过给模型输入一个可参数化的提示模板，从而引导模型生成符合特定要求的文本。软提示通过调整参数来灵活地控制模型的输出，适应不同的任务和场景，具有较强的泛化能力。这种提示方式能够更好地适应多变的应用需求，提高模型的适应性和性能。

图2-20展示了硬提示和软提示的比较。针对输入文本“The Movie is fantastic.”，硬提示通过构建含有[MASK]标记的模板嵌入到文本“It is [MASK]”中，引导固定的文本模板（Fixed Text Template）生成符合要求的内容。而软提示通过可调嵌入（Tunable Embedding）调整参数，引导模板生成符合特定要求的文本。

![image](https://github.com/user-attachments/assets/0d246722-a199-4ddb-b307-46455b324401)


**图2-20 硬提示与软提示比较**

输入：This Movie is fantastic.

标签：positive

- **硬提示**：This Movie is fantastic. 下方连接输入；It is [MASK] 下方连接固定的文本模板。

- **软提示**：左侧为可调嵌入，右侧连接 This Movie is fantastic. 下方连接输入。

硬提示和软提示具有各自的优势和适用场景。总的来说，硬提示和软提示的主要区别在于提示方式的灵活性和泛化能力。硬提示固定且缺乏灵活性，而软提示则通过参数化设计，提供了更大的灵活性和更强的泛化能力，使得模型能够更好地适应各种任务和场景。选择硬提示还是软提示，需要根据具体的任务和需求进行考虑。对于一些固定不变的提示任务，硬提示具有更好的效果和性能；而对于需要灵活调整提示内容、适应不同场景的任务，软提示是一个更好的选择。此外，硬提示注重在实际应用中的效果和性能，而软提示在学术研究方面得到了广泛的关注和研究。

提示词调优可以看作前缀调优的简化版本，直接在输入前添加前缀提示。它给每个任务定义了自己的提示，然后拼接到数据上作为输入。但与前缀调优不同，提示词调优主要集中在输入层处加入可训练的提示向量，并且不需要加入MLP进行调整，以解决难训练的问题。

#### （2）提示词调优方法

提示词调优是基于离散提示方法，即通过包含一组软提示Token（以前缀或自由形式）扩充输入文本，然后通过增强的提示输入来解决特定领域的任务。在实现上，特定任务的提示嵌入与输入文本嵌入相结合，然后输入到语言模型中。由于该方法在输入层引入的可训练参数较少，其性能高度依赖于底层语言模型的模型容量。因此，选择或设计一个具有足够大参数量的底层语言模型，对于提升提示词调优的性能至关重要。

图2-21展示了传统模型调优（Model Tuning）与提示词调优的不同之处。

![image](https://github.com/user-attachments/assets/99981d80-2ae6-430b-a2d8-eefdbe3dbbc3)


**图2-21 传统模型调优与提示词调优比较**

- **传统模型调优**：预训练模型分别连接Task A模型（输入a1等参数）、Task B模型（输入b1等参数）、Task C模型（输入c1、c2等参数）。
- **提示词调优**：预训练模型连接混合任务批处理（包含A、B、C任务及a1、b1等参数）。


传统模型调优针对每个特定领域任务（如Task A、Task B、Task C），都需要进行一次专门的微调操作，并分别保存各自的预训练模型副本，推理时也只能单独对每个任务批量处理。这种方式不仅耗时，而且占用了大量的存储空间。

提示词调优则需要为各个任务分别保存一个小型的、与任务相关的提示，进行混合任务批处理（Mixed-task Batch），然后结合原始的预训练模型执行多任务混合推理。这种方法提高了处理效率，减少了存储需求，同时能够更好地利用预训练模型的通用能力。

更进一步，提示词调优还推出了一个提示集成（Prompt Ensembling，也称为“提示聚合”）的技巧，它允许在同一个批次中训练同一个任务的多个不同提示，即用多种不同的方式来提出同一个问题。这种方法实际上等同于同时训练了多个模型，但所需成本远低于传统的模型集成方式。提示聚合通过多样化的提示设计，增强了模型的鲁棒性和泛化能力。

### 4. P-Tuning

与前缀调优相比，P-Tuning（P调优）在输入层引入了可微分的虚拟Token，这些虚拟Token可以被灵活地插入到不同的位置，而非仅限定在前缀。这种灵活性使得P-Tuning能够更好地适应不同的任务和输入结构，提高了模型的适应性和性能。

为了更好地理解前缀调优与P-Tuning，有必要先了解一下PET（Pattern-Exploiting Training，模板开发训练，即人工构建模板）。PET的主要思想是借助由自然语言构成的模板（也称Pattern或Prompt），将特定领域任务转化为一个完形填空任务，这样就可以用预训练语言模型的能力来进行预测了。例如，通过特定模板将新闻分类转换为执行MLM任务，即利用先验知识人工定义模板，将目标分类任务转化为完形填空，然后微调MLM任务的参数。这种转化使得模型能够利用预训练语言模型的能力，更好地理解和处理特定任务。

前缀调优和P-Tuning都是在PET的基础上发展起来的技术，它们通过引入可微分的虚拟Token，进一步增强了模型的灵活性和适应性。前缀调优将虚拟Token固定为前缀，而P-Tuning则允许这些Token插入到输入序列的任意位置，从而提供了更多的优化空间和可能性。这两种技术都旨在通过调整输入表示，提升模型在特定任务上的表现，是自然语言处理领域中重要的微调策略。图2-22是一个通过PET将新闻分类转换为MLM任务的示例。

![image](https://github.com/user-attachments/assets/baea38d3-ce3d-4f06-968e-9b2b372cddb5)



**图2-22 通过PET将新闻分类转换为MLM任务的示例**

原始文本“中国女排再夺冠！”，PET模板“下面是 [MASK] [MASK] 新闻”，分类标签假定为“体育/财经/时政/军事”。输入中的[u1]到[u6]，实际上指的是Tokenizer词汇表中的[unused1]到[unused6]，用来拼凑出一个模板。接着，通过已有的标注数据来确定这个模板的具体内容。P-Tuning算法通过优化特定的Token嵌入来提升模型性能。在例子中，这些Token的范围从[unused1]到[unused6]，在标注数据稀缺时仅对这些固定的Token进行调整，而固定模型的其他权重。



这种方法的优势在于限制了需要学习的参数数量，减少了过拟合风险并加快了训练速度。然而，当有足够的标注数据时，仅优化这些Token可能不足以捕捉数据中的复杂关系，导致欠拟合。此时，模型会解锁所有权重进行全面的微调，以保持任务和预训练任务之间的一致性。在选择目标Token，如选择“体育”时，在数据受限的场景下，手动选择目标Token往往更为有效。而在数据丰富的环境下，引入[unused*]Token可以提供更广阔的优化空间，进而提升性能。直接随机初始化虚拟Token可能会使模型陷入局部最优，因为这些Token应当在自然语言中相互关联。为此，技术人员采用了LSTM和MLP组合来编码这些虚拟Token，以促进模型更快收敛并提高效果。其他优化策略包括在训练特定领域任务时不仅预测目标Token（比如“体育”），还预测其他Token。无论是通过MLM随机遮蔽Token，还是通过语言模型预测整个序列，这样可以使得重构的序列更加符合自然语言的特性。

P-Tuning能够自动创建模板。模板是由自然语言构成的前缀或后缀，比如“The capital of Britain is [MASK]”这种形式，也可以是“Britain's capital is [MASK]”或者“[MASK] is the capital of Britain”等。通过这些模板才能使得特定领域任务与预训练任务一致，才能更加充分地利用原始预训练模型，从而在零样本或少样本的学习场景中发挥更出色的作用。

P-Tuning并不限制模板的具体形式，以及它们是否由自然语言构成，重点在于模型最终的性能。换句话说，P-Tuning不关心模板长什么样、由什么构成，只关注模板由哪些Token组成、该插入到哪里、插入后能不能完成特定领域任务、输出的候选空间是什么。

因此P-Tuning的做法是用上下文x（Britain）、目标y（[MASK]）去组合成一个提示模板t（「The capital of … is …」）。图2-23展示了离散提示搜索“The capital of Britain is [MASK]”与P-Tuning处理示例。

![image](https://github.com/user-attachments/assets/81222d3d-8570-4433-93a8-d7676a5bda10)



**图2-23 离散提示搜索与P-Tuning处理示例**

- **离散提示搜索**：输入向量（The capital of Britain is [MASK] 经词嵌入e处理后的向量）连接提示生成器，提示生成器输出伪提示[P0…Pl]，接收离散奖励，再连接预训练语言模型。

- **P-Tuning**：输入向量（capital、Britain、[MASK] 经词嵌入e处理后的向量h及其他向量）连接提示编码器，提示编码器输出[Pm…Pn]，经反向传播，再连接预训练语言模型。



在这个例子中，灰色区域代表上下文Britain，粗实线标记的区域表示目标[MASK]，加粗实线的区域则是想生成的模板。在图2-23左侧部分，提示生成器仅接收离散奖励，这种离散化搜索的做法不一定是最优的；而图2-23右侧的P-Tuning，通过预训练的嵌入层将一组离散输入Token（即伪提示）映射到向量，然后结合（capital,Britain）生成得到结果，再优化生成的解码器部分。这种方法可以找到更好的连续提示，并通过下游损失函数对连续提示进行优化。这些连续提示可以通过反向传播给提示编码器进行学习来改进提示效果和提高提示的稳定性。

P-Tuning V1是在P-Tuning基础上改进的参数高效微调技术。P-Tuning首次提出了用连续空间搜索的向量做提示。这种方法通过在连续空间中搜索合适的向量来构建提示，使得提示形式更加灵活和高效。P-Tuning V2是基于P-Tuning V1的改进技术，将逐层提示向量融入Transformer架构中，专门用于NLU任务中，同时利用多任务学习，将逐层提示向量融入Transformer架构中，专门用于NLU任务中，同时利用多任务学习共同优化共享提示。这种融入方式使得提示向量能够更好地与Transformer的各层结构相结合，提高了提示的有效性和模型的整体性能。P-Tuning V2能够改善不同参数规模在NLU任务上的性能。通过多任务学习的共同优化，P-Tuning V2不仅提升了单个任务的表现，还增强了模型在多个任务间的泛化能力。这种优化策略使得模型在处理NLU任务时更加高效和准确，是NLP领域中重要的技术进步。



### 5. 低秩适应微调
LoRA为每个密集层的更新矩阵施加低秩约束，以减少适应特定领域任务的可训练参数。这种方法通过限制更新矩阵的秩，有效地减少了模型参数的数量，同时保持了模型的性能。AdaLoRA对LoRA进行了改良，根据每个权重矩阵的重要性来动态调配参数。这种动态调配策略使得模型能够更加智能地分配资源，将更多的参数分配给重要的权重矩阵，从而提高了模型的适应性和效率。QLoRA会转换预训练模型格式，加入一组可学习的低秩适配器权重，并通过对量化权重的反向传播梯度来进行微调。这种方法结合了量化权重和低秩适配器，进一步减少了模型的参数数量，同时保持了模型的性能。通过反向传播梯度的微调，QLoRA能够更好地适应特定领域任务，提高了模型的泛化能力和效率。

#### （1）LoRA

在大模型完成预训练并达到收敛状态后，模型中包含众多执行矩阵乘法的稠密层，这些层的权重矩阵通常具有满秩特性。然而，经过针对特定任务的微调，矩阵乘法中的权重变化呈现出低秩特征。这一现象揭示了，即使将权重参数随机映射至一个维度较小的子空间，模型依然能够进行有效的学习。这进一步说明，针对特定任务的权重矩阵无须维持其满秩状态，即可实现高效的学习和任务适应。

LoRA技术通过在模型的现有权重参数矩阵上叠加额外的低秩矩阵，实现了在不显著增加参数量的前提下的模型微调功能。具体而言，LoRA对每个稠密层的更新矩阵施加低秩约束，以此来减少适应特定领域任务所需的可训练参数数量。如图2-24所示，LoRA技术对网络层的权重矩阵进行低秩学习，从而优化模型在特定任务上的表现。

![image](https://github.com/user-attachments/assets/6274ad05-482e-4061-94d3-4e81b624f213)



**图2-24 LoRA对网络层的权重矩阵进行低秩学习**

左侧为多层Transformer结构，包含前馈神经网络、自注意力及多个线性层（Wq、Wk、Wv等） 。右侧为预训练权重W ，通过添加低秩矩阵A（A~N(0,σ²) ）和B（B=0 ）进行低秩学习。


其中，Wd、Wq、Wk、Wv为线性变换矩阵，用于将输入向量转换为输出向量。d是指前馈神经网络的维度，Wd是前馈神经网络的线性变换矩阵。Wq、Wk、Wv是神经网络参数，随机初始化后在模型训练时更新。预训练权重通过学习得到，其计算公式为：A = N(0,σ²)，其中N()表示正态分布，σ²是方差。

LoRA方法的精髓在于用低秩分解的方式来近似模型参数的变化，这样就能用很少的参数训练庞大的模型。再来看一下涉及矩阵乘法的处理：在原有的预训练语言模型结构中加入一个额外的路径。这条新路径由低秩矩阵A和B构成，A矩阵用来降低维度，B矩阵则用来提升维度，它们的乘积能够模拟出模型参数的“本征秩”（Intrinsic Rank）。而在这个过程中，中间层的维度被设置为r。假设一个线性层的原始权重矩阵为W，修改权重矩阵过程可以写成一般形式：W^ ← W + ΔW。即冻结原始矩阵W ∈ R^mn ，其中m、n表示矩阵R的行和列，W^ 是修改后的权重矩阵。通过低秩分解矩阵来近似参数更新ΔW，即ΔW = BA，其中可训练参数B ∈ R^mr 和A ∈ R^rn 是低秩矩阵，r ≪ min(m,n)是中间层秩的维度。这种设计使得模型能够在保持原有规模的同时，以更高效率的方式进行调整和学习。 

LoRA技术的主要优势在于能够显著节省内存和存储资源（例如VRAM）。通过维持一个大模型的副本，并结合一些任务特定的低秩分解矩阵，LoRA使得大模型能够灵活适应不同的特定领域任务，同时保持高效的资源利用率。这种方法不仅优化了模型的部署和运行，还增强了模型在多样化任务中的适应性和性能。

![image](https://github.com/user-attachments/assets/65f58d9d-fde7-498a-9adb-bcd03371ed92)


#### （2）AdaLoRA

在对大型预训练语言模型进行微调时，需要采用一种更为灵活的策略来处理权重矩阵。不应僵化地设定矩阵的秩，而应根据不同层次和模块中权重矩阵的实际重要性进行动态调整。这要求我们识别出关键的权重矩阵并为其提供更多的参数支持，同时减少对非关键矩阵的参数分配。这一策略的益处显而易见：一方面能提升模型的整体性能；另一方面能节约计算资源，降低模型性能退化的风险。为突破现有方法的局限，有人提出了一种名为AdaLoRA（自适应LoRA）的新技术。AdaLoRA通过评估权重矩阵的重要性得分，从而在不同权重矩阵间智能地调配参数预算。其核心优势在于自适应性，确保模型在应对不同任务时，能灵活分配计算资源，从而获得更优的性能。

AdaLoRA在LoRA的基础上进行了改良，根据每个权重矩阵的重要性来动态调配参数。该技术的核心在于如何分配增量矩阵的参数。对于那些至关重要的增量矩阵，AdaLoRA会赋予它们较高的秩，以便能够捕捉更细腻和特定于任务的信息。而对于那些相对不那么重要的矩阵，降低秩可以防止模型过拟合，并节省计算资源。

此外，AdaLoRA采用了奇异值分解（SVD）的方式来对这些增量更新进行参数化。通过设置重要性指标，它可以屏蔽掉那些不重要的奇异值，只保留那些重要的奇异值。由于对大型矩阵进行完整的奇异值分解是一项计算成本极高的操作，AdaLoRA通过削减不必要的参数，加快了计算速度，同时为将来可能的恢复保留了余地，也确保了训练的稳定性。

为了进一步提升训练过程的稳定性，AdaLoRA还在训练用的损失函数中引入了额外的惩罚项。这有助于维持奇异矩阵P和Q的正交性，从而避免了复杂的奇异值分解计算。 
